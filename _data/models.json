[
  {
    "name": "BERT",
    "provider": "Google",
    "providerClass": "google",
    "description": "Bidirectional Encoder Representations from Transformers, designed for deep contextual understanding of text. It revolutionized NLP by enabling bidirectional context in pre-training, excelling in tasks like question answering and text classification.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "October 2018",
      "version": "1.0",
      "size": "340M parameters"
    },
    "features": [
      "Bidirectional Context",
      "Pre-training",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.5/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "ALBERT",
    "provider": "Google",
    "providerClass": "google",
    "description": "A Lite BERT with reduced parameters for efficiency while maintaining performance. It uses factorized embedding parameterization and cross-layer parameter sharing, ideal for resource-constrained environments.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "September 2019",
      "version": "1.0",
      "size": "12M parameters"
    },
    "features": [
      "Parameter Efficiency",
      "Scalable",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "7.8/10",
      "accuracy": "8.3/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "MobileBERT",
    "provider": "Google",
    "providerClass": "google",
    "description": "A compact BERT variant optimized for mobile and edge devices. It balances performance and resource usage, enabling efficient NLP on low-power hardware.",
    "badges": ["LLM", "Mobile"],
    "metadata": {
      "released": "April 2020",
      "version": "1.0",
      "size": "25M parameters"
    },
    "features": [
      "Mobile Optimization",
      "Low Latency",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "7.5/10",
      "accuracy": "8.0/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "MuRIL",
    "provider": "Google",
    "providerClass": "google",
    "description": "Multilingual Representation for Indian Languages, a BERT-based model tailored for Indian languages. It supports 17 Indian languages, enhancing NLP tasks like sentiment analysis and text classification.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "March 2021",
      "version": "1.0",
      "size": "237M parameters"
    },
    "features": [
      "Multilingual",
      "Indian Languages",
      "Pre-training",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "7.9/10",
      "accuracy": "8.2/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "ELECTRA",
    "provider": "Google",
    "providerClass": "google",
    "description": "Efficient pre-training model using a generator-discriminator framework. It achieves high performance with less compute by replacing masked language modeling with token detection.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "March 2020",
      "version": "1.0",
      "size": "335M parameters"
    },
    "features": [
      "Efficient Pre-training",
      "Token Detection",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.2/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "T5",
    "provider": "Google",
    "providerClass": "google",
    "description": "Text-to-Text Transfer Transformer, a unified framework for NLP tasks. It converts all tasks into a text-to-text format, enabling versatile applications like translation and summarization.",
    "badges": ["LLM", "Versatile"],
    "metadata": {
      "released": "October 2019",
      "version": "1.0",
      "size": "11B parameters"
    },
    "features": ["Text-to-Text", "Pre-training", "Fine-tuning", "Multitask"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "mT5",
    "provider": "Google",
    "providerClass": "google",
    "description": "Multilingual T5, supporting 101 languages for global NLP applications. It extends T5’s text-to-text framework to low-resource languages, improving cross-lingual performance.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "October 2020",
      "version": "1.0",
      "size": "13B parameters"
    },
    "features": ["Multilingual", "Text-to-Text", "Pre-training", "Fine-tuning"],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "ByT5",
    "provider": "Google",
    "providerClass": "google",
    "description": "Byte-level T5 model for character-based text processing. It operates directly on UTF-8 bytes, improving performance on noisy text and low-resource languages.",
    "badges": ["LLM", "Character-based"],
    "metadata": {
      "released": "May 2021",
      "version": "1.0",
      "size": "12B parameters"
    },
    "features": [
      "Byte-level Processing",
      "Multilingual",
      "Text-to-Text",
      "Robust"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.6/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Flan-T5",
    "provider": "Google",
    "providerClass": "google",
    "description": "Instruction-tuned T5 for zero-shot task generalization. It improves T5’s performance on unseen tasks by fine-tuning with diverse instruction datasets.",
    "badges": ["LLM", "Instruction-tuned"],
    "metadata": {
      "released": "October 2022",
      "version": "1.0",
      "size": "11B parameters"
    },
    "features": [
      "Zero-shot Learning",
      "Text-to-Text",
      "Fine-tuning",
      "Instruction Tuning"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "9.0/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "PaLM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Pathways Language Model, a 540B-parameter model for advanced reasoning and multilingual tasks. It excels in complex tasks like mathematical reasoning and code generation.",
    "badges": ["LLM", "Large-scale"],
    "metadata": {
      "released": "April 2022",
      "version": "1.0",
      "size": "540B parameters"
    },
    "features": [
      "Advanced Reasoning",
      "Multilingual",
      "Code Generation",
      "Scalable"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Commercial"
    }
  },
  {
    "name": "PaLM 2",
    "provider": "Google",
    "providerClass": "google",
    "description": "Enhanced version of PaLM with improved efficiency and performance. It offers better reasoning, multilingual capabilities, and optimized training for diverse tasks.",
    "badges": ["LLM", "Advanced"],
    "metadata": {
      "released": "May 2023",
      "version": "2.0",
      "size": "340B parameters"
    },
    "features": [
      "Advanced Reasoning",
      "Multilingual",
      "Efficient",
      "Code Generation"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.3/10",
      "license": "Commercial"
    }
  },
  {
    "name": "PaLM-E",
    "provider": "Google",
    "providerClass": "google",
    "description": "Embodied PaLM for robotics and multimodal tasks, integrating language with sensory inputs. It enables language-guided control in physical environments like robotic navigation.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "March 2023",
      "version": "1.0",
      "size": "562B parameters"
    },
    "features": ["Multimodal", "Robotics", "Advanced Reasoning", "Embodied AI"],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Commercial"
    }
  },
  {
    "name": "U-PaLM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Continually trained PaLM with Unified Language Learning (UL2) objectives. It enhances generalization across tasks, improving performance in reasoning and multilingual settings.",
    "badges": ["LLM", "Advanced"],
    "metadata": {
      "released": "June 2023",
      "version": "1.0",
      "size": "540B parameters"
    },
    "features": [
      "Continual Learning",
      "Advanced Reasoning",
      "Multilingual",
      "Scalable"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.2/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Flan-PaLM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Instruction-tuned PaLM for improved task generalization. It leverages diverse instruction datasets to excel in zero-shot and few-shot scenarios across tasks.",
    "badges": ["LLM", "Instruction-tuned"],
    "metadata": {
      "released": "August 2023",
      "version": "1.0",
      "size": "540B parameters"
    },
    "features": [
      "Zero-shot Learning",
      "Instruction Tuning",
      "Advanced Reasoning",
      "Multilingual"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.3/10",
      "license": "Commercial"
    }
  },
  {
    "name": "LaMDA",
    "provider": "Google",
    "providerClass": "google",
    "description": "Language Model for Dialogue Applications, optimized for conversational tasks. It generates coherent and contextually relevant responses for natural dialogue.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "January 2022",
      "version": "1.0",
      "size": "137B parameters"
    },
    "features": [
      "Conversational",
      "Contextual Understanding",
      "Dialogue",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Commercial"
    }
  },
  {
    "name": "MUM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Multitask Unified Model, a multimodal model for search combining text and images. It enhances search relevance by understanding complex, multimodal queries.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "May 2021",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Search Optimization",
      "Text and Image",
      "Scalable"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Commercial"
    }
  },
  {
    "name": "XLNet",
    "provider": "Google",
    "providerClass": "google",
    "description": "Generalized autoregressive model developed with CMU, outperforming BERT in certain tasks. It uses permutation-based training for better context modeling.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "June 2019",
      "version": "1.0",
      "size": "340M parameters"
    },
    "features": [
      "Autoregressive",
      "Permutation Training",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.1/10",
      "accuracy": "8.6/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "CANINE",
    "provider": "Google",
    "providerClass": "google",
    "description": "Character-based model for multilingual text processing without word tokenization. It excels in low-resource languages and noisy text environments.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "June 2021",
      "version": "1.0",
      "size": "370M parameters"
    },
    "features": ["Character-based", "Multilingual", "Robust", "Pre-training"],
    "stats": {
      "performance": "7.8/10",
      "accuracy": "8.1/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Switch Transformer",
    "provider": "Google",
    "providerClass": "google",
    "description": "Mixture-of-experts model for scaling to trillions of parameters efficiently. It dynamically selects expert subnetworks, reducing compute costs for large-scale tasks.",
    "badges": ["LLM", "Scalable"],
    "metadata": {
      "released": "January 2021",
      "version": "1.0",
      "size": "1.6T parameters"
    },
    "features": ["Mixture-of-Experts", "Scalable", "Efficient", "Pre-training"],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Commercial"
    }
  },
  {
    "name": "GShard",
    "provider": "Google",
    "providerClass": "google",
    "description": "Sharding-based Mixture-of-Experts model optimized for translation tasks. It enables efficient scaling for multilingual applications with reduced computational costs.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "June 2020",
      "version": "1.0",
      "size": "600B parameters"
    },
    "features": [
      "Mixture-of-Experts",
      "Multilingual",
      "Translation",
      "Scalable"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "GLaM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Generalist Language Model, a 1.2T-parameter Mixture-of-Experts model. It achieves high performance with lower energy consumption for NLP tasks.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "December 2021",
      "version": "1.0",
      "size": "1.2T parameters"
    },
    "features": ["Mixture-of-Experts", "Efficient", "Scalable", "NLP Tasks"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Commercial"
    }
  },
  {
    "name": "ETC",
    "provider": "Google",
    "providerClass": "google",
    "description": "Extended Transformer Construction with hierarchical attention for long-context processing. It handles extended sequences efficiently for tasks like document understanding.",
    "badges": ["LLM", "Long-context"],
    "metadata": {
      "released": "August 2020",
      "version": "1.0",
      "size": "400M parameters"
    },
    "features": [
      "Long-context",
      "Hierarchical Attention",
      "Pre-training",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "7.9/10",
      "accuracy": "8.2/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Flan",
    "provider": "Google",
    "providerClass": "google",
    "description": "Instruction-tuned model family for zero-shot performance across tasks. It leverages fine-tuning on diverse datasets to improve generalization.",
    "badges": ["LLM", "Instruction-tuned"],
    "metadata": {
      "released": "December 2021",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Zero-shot Learning",
      "Instruction Tuning",
      "Multitask",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.9/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Minerva",
    "provider": "Google",
    "providerClass": "google",
    "description": "PaLM-based model optimized for quantitative reasoning tasks. It excels in solving mathematical and scientific problems with high accuracy.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "June 2022",
      "version": "1.0",
      "size": "540B parameters"
    },
    "features": [
      "Quantitative Reasoning",
      "Advanced Reasoning",
      "Math-focused",
      "Scalable"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Commercial"
    }
  },
  {
    "name": "PEGASUS",
    "provider": "Google",
    "providerClass": "google",
    "description": "Pre-training with Extracted Gap-sentences for Abstractive Summarization. It is optimized for generating concise and accurate text summaries.",
    "badges": ["LLM", "Summarization"],
    "metadata": {
      "released": "February 2020",
      "version": "1.0",
      "size": "570M parameters"
    },
    "features": [
      "Summarization",
      "Pre-training",
      "Fine-tuning",
      "Text Generation"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "BigBird",
    "provider": "Google",
    "providerClass": "google",
    "description": "Transformer with sparse attention for processing long sequences. It reduces memory usage while maintaining performance on tasks like document classification.",
    "badges": ["LLM", "Long-context"],
    "metadata": {
      "released": "July 2020",
      "version": "1.0",
      "size": "400M parameters"
    },
    "features": [
      "Sparse Attention",
      "Long-context",
      "Pre-training",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.4/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "UniT",
    "provider": "Google",
    "providerClass": "google",
    "description": "Unified Transformer for vision and language tasks. It handles multimodal inputs for applications like image captioning and visual question answering.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "June 2021",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Vision and Language",
      "Pre-training",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "ViT-BERT",
    "provider": "Google",
    "providerClass": "google",
    "description": "Hybrid vision-language model combining Vision Transformer and BERT. It excels in tasks requiring joint understanding of images and text.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "April 2021",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Vision and Language",
      "Pre-training",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.9/10",
      "license": "Commercial"
    }
  },
  {
    "name": "TAPAS",
    "provider": "Google",
    "providerClass": "google",
    "description": "Table-based question answering and parsing model. It processes structured data in tables, enabling natural language queries over tabular content.",
    "badges": ["LLM", "Structured Data"],
    "metadata": {
      "released": "May 2020",
      "version": "1.0",
      "size": "340M parameters"
    },
    "features": [
      "Table Parsing",
      "Question Answering",
      "Structured Data",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.2/10",
      "accuracy": "8.6/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "DocT5query",
    "provider": "Google",
    "providerClass": "google",
    "description": "T5-based model for document ranking and query generation. It improves search relevance by generating queries for document indexing.",
    "badges": ["LLM", "Search"],
    "metadata": {
      "released": "August 2020",
      "version": "1.0",
      "size": "11B parameters"
    },
    "features": [
      "Document Ranking",
      "Query Generation",
      "Text-to-Text",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "UL2",
    "provider": "Google",
    "providerClass": "google",
    "description": "Unified Language Learning model with a mixture-of-denoisers approach. It supports diverse tasks by combining multiple pre-training objectives for flexibility.",
    "badges": ["LLM", "Versatile"],
    "metadata": {
      "released": "May 2022",
      "version": "1.0",
      "size": "20B parameters"
    },
    "features": [
      "Mixture-of-Denoisers",
      "Pre-training",
      "Multitask",
      "Scalable"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "MedPaLM",
    "provider": "Google",
    "providerClass": "google",
    "description": "Medical-domain PaLM for clinical tasks like medical question answering. It is fine-tuned with medical data for high accuracy in healthcare applications.",
    "badges": ["LLM", "Medical"],
    "metadata": {
      "released": "July 2023",
      "version": "1.0",
      "size": "540B parameters"
    },
    "features": [
      "Medical QA",
      "Clinical Tasks",
      "Advanced Reasoning",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Meena",
    "provider": "Google",
    "providerClass": "google",
    "description": "Early conversational model, a precursor to LaMDA. It focuses on generating human-like dialogue with improved coherence and context awareness.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "January 2020",
      "version": "1.0",
      "size": "2.6B parameters"
    },
    "features": [
      "Conversational",
      "Contextual Understanding",
      "Dialogue",
      "Pre-training"
    ],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.3/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Longformer",
    "provider": "Google",
    "providerClass": "google",
    "description": "Transformer with efficient attention for long-document processing. It reduces computational complexity while handling extended sequences for tasks like summarization.",
    "badges": ["LLM", "Long-context"],
    "metadata": {
      "released": "April 2020",
      "version": "1.0",
      "size": "435M parameters"
    },
    "features": [
      "Long-context",
      "Efficient Attention",
      "Pre-training",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.1/10",
      "accuracy": "8.5/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Reformer",
    "provider": "Google",
    "providerClass": "google",
    "description": "Memory-efficient Transformer using locality-sensitive hashing. It reduces memory usage for processing long sequences, suitable for resource-constrained environments.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "January 2020",
      "version": "1.0",
      "size": "360M parameters"
    },
    "features": [
      "Memory-efficient",
      "Long-context",
      "Pre-training",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "7.9/10",
      "accuracy": "8.2/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "T0",
    "provider": "Google",
    "providerClass": "google",
    "description": "T5-based model for zero-shot task generalization. It leverages multitask prompting to perform well on unseen tasks without additional fine-tuning.",
    "badges": ["LLM", "Zero-shot"],
    "metadata": {
      "released": "October 2021",
      "version": "1.0",
      "size": "11B parameters"
    },
    "features": [
      "Zero-shot Learning",
      "Text-to-Text",
      "Multitask",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Luminous",
    "provider": "Google",
    "providerClass": "google",
    "description": "Multilingual text generation model with limited public details. It focuses on high-quality text generation for diverse languages and applications.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "March 2022",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multilingual",
      "Text Generation",
      "Pre-training",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.7/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini Ultra",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Google's largest multimodal model for text, vision, and reasoning tasks. It excels in complex problem-solving across diverse data types like code, images, and text.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "December 2023",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Advanced Reasoning",
      "Code Generation",
      "Vision"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini Pro",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Mid-tier multimodal Gemini model for text, vision, and reasoning. It offers a balance of performance and efficiency for various tasks.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "December 2023",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["Multimodal", "Reasoning", "Code Generation", "Vision"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini Nano",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Lightweight Gemini model optimized for on-device tasks. It supports text and vision processing with low resource requirements.",
    "badges": ["LLM", "On-device"],
    "metadata": {
      "released": "December 2023",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["On-device", "Multimodal", "Efficient", "Vision"],
    "stats": {
      "performance": "7.8/10",
      "accuracy": "8.2/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini Flash",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Fast and efficient Gemini variant for rapid task processing. It prioritizes low latency while maintaining multimodal capabilities.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "May 2024",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["Multimodal", "Low Latency", "Efficient", "Vision"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini 1.5 Pro",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Enhanced multimodal Gemini model with improved performance. It excels in complex reasoning, text, and vision tasks with greater efficiency.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "February 2024",
      "version": "1.5",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Advanced Reasoning",
      "Code Generation",
      "Vision"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.4/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemini 1.5 Flash",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Lightweight 1.5 Gemini variant for fast, on-device multimodal tasks. It combines efficiency with strong text and vision capabilities.",
    "badges": ["LLM", "On-device"],
    "metadata": {
      "released": "May 2024",
      "version": "1.5",
      "size": "Unknown parameters"
    },
    "features": ["On-device", "Multimodal", "Low Latency", "Vision"],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.4/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gopher",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "280B-parameter model focused on reasoning and language tasks. It competes with large-scale models in NLP benchmarks and research applications.",
    "badges": ["LLM", "Large-scale"],
    "metadata": {
      "released": "December 2021",
      "version": "1.0",
      "size": "280B parameters"
    },
    "features": ["Advanced Reasoning", "Scalable", "NLP Tasks", "Research"],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Chinchilla",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "70B-parameter compute-optimal model for efficient performance. It outperforms larger models in NLP tasks with less computational cost.",
    "badges": ["LLM", "Efficient"],
    "metadata": {
      "released": "March 2022",
      "version": "1.0",
      "size": "70B parameters"
    },
    "features": ["Compute-optimal", "Efficient", "NLP Tasks", "Scalable"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Commercial"
    }
  },
  {
    "name": "RETRO",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Retrieval-augmented Transformer for enhanced language modeling. It uses external memory to improve performance on knowledge-intensive tasks.",
    "badges": ["LLM", "Retrieval-augmented"],
    "metadata": {
      "released": "January 2022",
      "version": "1.0",
      "size": "7B parameters"
    },
    "features": [
      "Retrieval-augmented",
      "Knowledge-intensive",
      "NLP Tasks",
      "Scalable"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Flamingo",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Multimodal vision-language model for tasks like image captioning. It combines visual and textual understanding for versatile applications.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "April 2022",
      "version": "1.0",
      "size": "80B parameters"
    },
    "features": [
      "Multimodal",
      "Vision and Language",
      "Image Captioning",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gato",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Generalist agent for text, vision, and robotics tasks. It performs well across diverse domains, from language to physical control.",
    "badges": ["LLM", "Generalist"],
    "metadata": {
      "released": "May 2022",
      "version": "1.0",
      "size": "1.2B parameters"
    },
    "features": ["Multimodal", "Robotics", "Generalist", "Scalable"],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "AlphaCode",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Model for competitive programming and code generation. It solves complex algorithmic problems with high accuracy and efficiency.",
    "badges": ["LLM", "Code"],
    "metadata": {
      "released": "February 2022",
      "version": "1.0",
      "size": "41B parameters"
    },
    "features": [
      "Code Generation",
      "Competitive Programming",
      "Reasoning",
      "Scalable"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Commercial"
    }
  },
  {
    "name": "SayCan",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Language-guided robotic control model for task execution. It combines language understanding with physical actions for robotic applications.",
    "badges": ["LLM", "Robotics"],
    "metadata": {
      "released": "August 2022",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["Robotics", "Language-guided", "Task Execution", "Multimodal"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Ithaca",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Model for historical text restoration, specializing in ancient Greek texts. It reconstructs missing text with high contextual accuracy.",
    "badges": ["LLM", "Specialized"],
    "metadata": {
      "released": "March 2022",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Text Restoration",
      "Historical Texts",
      "Contextual Understanding",
      "Specialized"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.6/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Dramatron",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Model for scriptwriting and creative writing assistance. It generates coherent narratives and dialogue for storytelling applications.",
    "badges": ["LLM", "Creative"],
    "metadata": {
      "released": "December 2022",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Creative Writing",
      "Scriptwriting",
      "Text Generation",
      "Narrative"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Sparrow",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Dialogue model with a focus on safety and ethical responses. It aims to reduce harmful outputs while maintaining conversational quality.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "September 2022",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["Conversational", "Safety-focused", "Dialogue", "Ethical"],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Med-PaLM 2",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Advanced medical-domain model for clinical tasks and question answering. It improves on MedPaLM with enhanced accuracy for healthcare applications.",
    "badges": ["LLM", "Medical"],
    "metadata": {
      "released": "August 2023",
      "version": "2.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Medical QA",
      "Clinical Tasks",
      "Advanced Reasoning",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.4/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Gemma",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Open-weight models (2B and 7B) for research and development. They offer efficient performance for text generation and NLP tasks.",
    "badges": ["LLM", "Open-weight"],
    "metadata": {
      "released": "February 2024",
      "version": "1.0",
      "size": "7B parameters"
    },
    "features": ["Open-weight", "Text Generation", "Efficient", "Research"],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Tram",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Model for structured data processing and analysis. It excels in tasks involving tabular data and complex data structures.",
    "badges": ["LLM", "Structured Data"],
    "metadata": {
      "released": "October 2023",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Structured Data",
      "Data Analysis",
      "Table Processing",
      "Scalable"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Flamingo-C",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Compact version of Flamingo for multimodal vision-language tasks. It maintains strong performance with reduced resource requirements.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "June 2023",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": [
      "Multimodal",
      "Vision and Language",
      "Efficient",
      "Image Captioning"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.9/10",
      "license": "Commercial"
    }
  },
  {
    "name": "Perceiver",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "General-purpose architecture for text and multimodal tasks. It uses cross-attention to handle diverse data types efficiently.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "July 2021",
      "version": "1.0",
      "size": "Unknown parameters"
    },
    "features": ["Multimodal", "Cross-attention", "Efficient", "Scalable"],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.6/10",
      "license": "Commercial"
    }
  },
  {
    "name": "CodeGen",
    "provider": "Google DeepMind",
    "providerClass": "google",
    "description": "Code-generation model complementing AlphaCode for programming tasks. It generates high-quality code for various languages and applications.",
    "badges": ["LLM", "Code"],
    "metadata": {
      "released": "March 2022",
      "version": "1.0",
      "size": "16B parameters"
    },
    "features": ["Code Generation", "Programming", "Reasoning", "Scalable"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Commercial"
    }
  },
  {
    "name": "LLaMA",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "A family of language models designed for research purposes, known for efficiency in natural language tasks. LLaMA models excel in text generation and understanding with optimized architectures.",
    "badges": ["LLM", "Research"],
    "metadata": {
      "released": "February 2023",
      "version": "1.0",
      "size": "13B parameters"
    },
    "features": [
      "Text Generation",
      "Efficient",
      "Research-focused",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.8/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "LLaMA 2",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "An improved version of LLaMA, offering enhanced performance and safety for research applications. It supports a wide range of NLP tasks with better generalization and efficiency.",
    "badges": ["LLM", "Research"],
    "metadata": {
      "released": "July 2023",
      "version": "2.0",
      "size": "70B parameters"
    },
    "features": [
      "Text Generation",
      "Safety-focused",
      "Efficient",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "LLaMA 3",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "The latest iteration of LLaMA, optimized for advanced reasoning and text generation. It provides state-of-the-art performance for research with improved efficiency and scalability.",
    "badges": ["LLM", "Advanced"],
    "metadata": {
      "released": "April 2024",
      "version": "3.0",
      "size": "70B parameters"
    },
    "features": [
      "Advanced Reasoning",
      "Text Generation",
      "Efficient",
      "Scalable"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "Code Llama",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "A specialized LLaMA variant for code generation and programming tasks. It supports multiple programming languages and excels in generating accurate, context-aware code.",
    "badges": ["LLM", "Code"],
    "metadata": {
      "released": "August 2023",
      "version": "1.0",
      "size": "34B parameters"
    },
    "features": [
      "Code Generation",
      "Programming",
      "Context-aware",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "RoBERTa",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "Robustly optimized BERT approach for enhanced NLP performance. It improves on BERT with dynamic masking and larger pre-training data for tasks like text classification.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "July 2019",
      "version": "1.0",
      "size": "355M parameters"
    },
    "features": [
      "Bidirectional Context",
      "Pre-training",
      "Fine-tuning",
      "NLP Tasks"
    ],
    "stats": {
      "performance": "8.2/10",
      "accuracy": "8.6/10",
      "license": "MIT"
    }
  },
  {
    "name": "XLM-R",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "Cross-lingual Language Model-RoBERTa for multilingual NLP tasks. It supports 100 languages, enabling robust performance in translation and text classification across languages.",
    "badges": ["LLM", "Multilingual"],
    "metadata": {
      "released": "November 2019",
      "version": "1.0",
      "size": "550M parameters"
    },
    "features": [
      "Multilingual",
      "Pre-training",
      "Fine-tuning",
      "Cross-lingual"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.7/10",
      "license": "MIT"
    }
  },
  {
    "name": "BART",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "Bidirectional and Auto-Regressive Transformer for text generation and comprehension. It excels in tasks like summarization and translation with a denoising pre-training objective.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "October 2019",
      "version": "1.0",
      "size": "400M parameters"
    },
    "features": [
      "Text Generation",
      "Summarization",
      "Translation",
      "Fine-tuning"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.8/10",
      "license": "MIT"
    }
  },
  {
    "name": "OPT",
    "provider": "Meta AI",
    "providerClass": "meta",
    "description": "Open Pre-trained Transformer models for research, offering efficient large-scale language modeling. It provides performance comparable to GPT-3 with open access for academic use.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "May 2022",
      "version": "1.0",
      "size": "175B parameters"
    },
    "features": [
      "Text Generation",
      "Efficient",
      "Research-focused",
      "Scalable"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.9/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "GPT-1",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "The first in the GPT series, a transformer-based model focused on unsupervised learning for natural language tasks. It laid the foundation for future models with generative pre-training.",
    "badges": ["LLM", "Research"],
    "metadata": {
      "released": "June 2018",
      "version": "1.0",
      "size": "117M parameters"
    },
    "features": ["Text Generation", "Unsupervised Learning", "Pre-training"],
    "stats": {
      "performance": "6.5/10",
      "accuracy": "6.8/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-2",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "An improved transformer model with better coherence in text generation. Initially partially released due to misuse concerns, it showed strong zero-shot task performance.",
    "badges": ["LLM", "Open-weight"],
    "metadata": {
      "released": "February 2019",
      "version": "1.0",
      "size": "1.5B parameters"
    },
    "features": ["Text Generation", "Zero-shot Learning", "Coherent Output"],
    "stats": {
      "performance": "7.8/10",
      "accuracy": "8.0/10",
      "license": "Partially Open"
    }
  },
  {
    "name": "GPT-3",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A massive model excelling in diverse NLP tasks, from text generation to question answering. It introduced few-shot learning capabilities and powered early API applications.",
    "badges": ["LLM", "Commercial"],
    "metadata": {
      "released": "June 2020",
      "version": "1.0",
      "size": "175B parameters"
    },
    "features": [
      "Text Generation",
      "Few-shot Learning",
      "Question Answering",
      "Translation"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-3.5",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "An optimized version of GPT-3 with fewer parameters, fine-tuned using reinforcement learning for conversational tasks. It powers the initial ChatGPT release.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "November 2022",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Conversational AI", "Text Generation", "Fine-tuned", "RLHF"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-3.5 Turbo",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A highly capable variant of GPT-3.5, optimized for speed and efficiency. It supports ChatGPT and was integrated into platforms like Bing before GPT-4.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Conversational AI", "High Efficiency", "Text Generation"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-4",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A multimodal model with enhanced text and image processing capabilities. It achieves human-level performance on academic benchmarks and supports complex tasks.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "March 2023",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text Generation",
      "Image Processing",
      "Advanced Reasoning",
      "Multimodal"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-4 Turbo",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "An upgraded GPT-4 with improved performance in coding and non-English languages. It addresses issues like UTF-8 handling and task completion.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "April 2024",
      "version": "2024-04-09",
      "size": "Not disclosed"
    },
    "features": [
      "Text Generation",
      "Code Generation",
      "Multilingual",
      "Multimodal"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-4o",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A multimodal model with advanced text, image, and audio processing. It offers superior performance in multilingual tasks and real-time interactions.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "May 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text Generation",
      "Image Processing",
      "Audio Processing",
      "Real-time Interaction"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.4/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-4o mini",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A smaller, cost-efficient version of GPT-4o, designed for enterprises and developers. It replaces GPT-3.5 Turbo in ChatGPT with lower API costs.",
    "badges": ["LLM", "Multimodal"],
    "metadata": {
      "released": "July 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text Generation",
      "Image Processing",
      "Cost-efficient",
      "API-focused"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "GPT-4.5",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "Codenamed Orion, this large model reduces hallucinations compared to GPT-4o and o1. It’s designed for conversational tasks and broad knowledge applications.",
    "badges": ["LLM", "Conversational"],
    "metadata": {
      "released": "February 2025",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Text Generation", "Low Hallucination", "Conversational AI"],
    "stats": {
      "performance": "9.3/10",
      "accuracy": "9.5/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "o1",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A reasoning-focused model excelling in STEM tasks like math, coding, and science. It uses chain-of-thought techniques for step-by-step problem-solving.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "September 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Advanced Reasoning",
      "Chain-of-thought",
      "STEM-focused",
      "Error Correction"
    ],
    "stats": {
      "performance": "9.4/10",
      "accuracy": "9.6/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "o1-mini",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A smaller, cost-efficient reasoning model without vision capabilities. It’s optimized for fast STEM task performance with adjustable effort levels.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "September 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Reasoning", "Cost-efficient", "STEM-focused"],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "o3",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A large reasoning model designed for complex tasks, offering intelligent tool usage like Python and image analysis. It builds on o1’s capabilities.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "December 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Advanced Reasoning", "Tool Usage", "Complex Task Handling"],
    "stats": {
      "performance": "9.5/10",
      "accuracy": "9.7/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "o3-mini",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A smaller, faster version of o3, optimized for cost-efficiency and reasoning tasks. It supports multiple effort levels for flexible performance.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "December 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Reasoning", "Cost-efficient", "Flexible Effort Levels"],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Codex",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A specialized model for code generation and editing, powering tools like GitHub Copilot. It excels in understanding and generating programming languages.",
    "badges": ["LLM", "Code"],
    "metadata": {
      "released": "August 2021",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Code Generation", "Code Editing", "Programming Support"],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "CLIP",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A vision-language model that connects text and images for tasks like image classification and captioning. It’s open-source and widely used in research.",
    "badges": ["Vision", "Open Source"],
    "metadata": {
      "released": "January 2021",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Image Classification", "Text-Image Mapping", "Captioning"],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "MIT"
    }
  },
  {
    "name": "Whisper",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "An automatic speech recognition model for transcribing and translating audio. It supports multilingual speech processing with high accuracy.",
    "badges": ["Speech", "Open Source"],
    "metadata": {
      "released": "September 2022",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Speech Recognition", "Transcription", "Translation"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "MIT"
    }
  },
  {
    "name": "DALL-E",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A text-to-image model generating creative images from textual prompts. It combines GPT-like architectures with diffusion models.",
    "badges": ["Multimodal", "Image Generation"],
    "metadata": {
      "released": "January 2021",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Image Generation", "Text-to-Image", "Creative Output"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "DALL-E 2",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "An enhanced version of DALL-E with improved image quality and editing capabilities. It supports higher resolution and more precise outputs.",
    "badges": ["Multimodal", "Image Generation"],
    "metadata": {
      "released": "April 2022",
      "version": "2.0",
      "size": "Not disclosed"
    },
    "features": ["Image Generation", "Image Editing", "High Resolution"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "DALL-E 3",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "The latest DALL-E model with advanced image generation and integration into ChatGPT. It offers improved detail and prompt adherence.",
    "badges": ["Multimodal", "Image Generation"],
    "metadata": {
      "released": "October 2023",
      "version": "3.0",
      "size": "Not disclosed"
    },
    "features": ["Image Generation", "Prompt Adherence", "ChatGPT Integration"],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Sora",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A text-to-video model generating high-quality short videos from text prompts. It’s designed for creative and professional video production.",
    "badges": ["Multimodal", "Video Generation"],
    "metadata": {
      "released": "February 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Video Generation", "Text-to-Video", "Creative Output"],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Jukebox",
    "provider": "OpenAI",
    "providerClass": "openai",
    "description": "A model for generating music from text prompts, supporting various genres and styles. It’s an experimental open-source project.",
    "badges": ["Audio", "Open Source"],
    "metadata": {
      "released": "April 2020",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": ["Music Generation", "Text-to-Audio", "Genre Support"],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.2/10",
      "license": "Non-commercial"
    }
  },
  {
    "name": "Stable Diffusion",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A pioneering open-source text-to-image model that generates high-quality, photorealistic images from textual prompts, leveraging latent diffusion techniques. Widely adopted for its flexibility and ability to run on consumer hardware, it supports creative applications in art, design, and media production.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "August 2022",
      "version": "1.0",
      "size": "4B parameters"
    },
    "features": [
      "Text-to-Image",
      "High Resolution",
      "Artistic Quality",
      "Open Source"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "8.9/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Diffusion 2.0",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An enhanced version of Stable Diffusion, introducing inpainting, outpainting, and depth-guided image generation for improved creative control. It maintains high-quality outputs while addressing ethical concerns through filtered training data and permissive licensing for diverse applications.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "November 2022",
      "version": "2.0",
      "size": "4B parameters"
    },
    "features": [
      "Inpainting",
      "Outpainting",
      "Depth-guided Generation",
      "Text-to-Image"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Diffusion XL (SDXL)",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A powerful evolution of Stable Diffusion, SDXL delivers superior image quality and prompt adherence at higher resolutions, ideal for professional use cases. It incorporates advanced training methods and supports diverse styles like 3D, photography, and painting, with optimized performance on consumer hardware.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "July 2023",
      "version": "1.0",
      "size": "3.5B parameters"
    },
    "features": [
      "High-Resolution Images",
      "Prompt Adherence",
      "Diverse Styles",
      "Consumer Hardware"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.1/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Diffusion 3",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A highly advanced text-to-image model with improved multi-subject prompt handling, image quality, and spelling accuracy, available in early preview as of February 2024. Combining diffusion transformer architecture and flow matching, it offers scalable options from 800M to 8B parameters for diverse creative needs.",
    "badges": ["LLM", "Image Generation", "Research"],
    "metadata": {
      "released": "February 2024",
      "version": "3.0",
      "size": "800M-8B parameters"
    },
    "features": [
      "Multi-subject Prompts",
      "High-Quality Images",
      "Scalable",
      "Transformer Architecture"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.2/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Diffusion 3.5 Large",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "The flagship model in the Stable Diffusion 3.5 series, boasting 8 billion parameters and 1-megapixel resolution for professional-grade image generation. Released in October 2024, it excels in prompt adherence and diverse outputs, available on platforms like AWS and Azure for enterprise applications.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "October 2024",
      "version": "3.5",
      "size": "8B parameters"
    },
    "features": [
      "High-Resolution Images",
      "Prompt Adherence",
      "Enterprise-ready",
      "Diverse Outputs"
    ],
    "stats": {
      "performance": "9.3/10",
      "accuracy": "9.4/10",
      "license": "Stability AI Community License"
    }
  },
  {
    "name": "Stable Diffusion 3.5 Large Turbo",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A distilled variant of Stable Diffusion 3.5 Large, optimized for speed by generating high-quality images in just four steps, launched in October 2024. It sacrifices minimal quality for faster processing, making it ideal for time-sensitive applications while retaining strong prompt adherence.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "October 2024",
      "version": "3.5",
      "size": "8B parameters"
    },
    "features": [
      "Fast Generation",
      "High-Quality Images",
      "Prompt Adherence",
      "Enterprise-ready"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.3/10",
      "license": "Stability AI Community License"
    }
  },
  {
    "name": "Stable Diffusion 3.5 Medium",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A balanced model in the 3.5 series with 2.5 billion parameters, released in October 2024, optimized for consumer hardware and supporting 0.25 to 2-megapixel resolutions. It offers a cost-effective solution for hobbyists and developers, delivering robust image quality with enhanced customization options.",
    "badges": ["LLM", "Image Generation", "Open Source"],
    "metadata": {
      "released": "October 2024",
      "version": "3.5",
      "size": "2.5B parameters"
    },
    "features": [
      "Consumer Hardware",
      "Flexible Resolution",
      "Customizable",
      "High-Quality Images"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.1/10",
      "license": "Stability AI Community License"
    }
  },
  {
    "name": "Stable Image Ultra",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A premium text-to-image model available on Amazon Bedrock, designed for ultra-high-quality outputs with exceptional detail and realism. It caters to professional creators needing top-tier visual fidelity for applications in advertising, design, and entertainment.",
    "badges": ["LLM", "Image Generation", "Commercial"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Ultra-High Quality",
      "Text-to-Image",
      "Professional Use",
      "High Fidelity"
    ],
    "stats": {
      "performance": "9.4/10",
      "accuracy": "9.5/10",
      "license": "Commercial License"
    }
  },
  {
    "name": "Stable Image Core",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An enhanced version of SDXL available on Azure AI Foundry, offering high-quality image generation with improved speed and efficiency for enterprise use. Launched in 2024, it supports scalable applications in marketing, e-commerce, and content creation with robust integration options.",
    "badges": ["LLM", "Image Generation", "Commercial"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "High-Quality Images",
      "Fast Generation",
      "Enterprise Integration",
      "Scalable"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.2/10",
      "license": "Commercial License"
    }
  },
  {
    "name": "Stable Cascade",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A research-preview text-to-image model introduced in February 2024, featuring a three-stage architecture for superior quality, flexibility, and fine-tuning efficiency. It aims to reduce hardware barriers, enabling broader access to high-performance image generation for developers and creators.",
    "badges": ["LLM", "Image Generation", "Research"],
    "metadata": {
      "released": "February 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Three-stage Architecture",
      "Fine-tuning",
      "High-Quality Images",
      "Accessible"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Audio Open",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An open-source text-to-audio model released in April 2024, optimized for generating short audio samples, sound effects, and production elements. Partnered with Arm for mobile device compatibility, it supports creative audio design for music, film, and gaming applications.",
    "badges": ["Audio", "Open Source"],
    "metadata": {
      "released": "April 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text-to-Audio",
      "Sound Effects",
      "Mobile Optimization",
      "Creative Audio"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.8/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Audio 2.0",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An advanced text-to-audio model launched in March 2024, capable of generating high-quality, full tracks up to three minutes long at 44.1 kHz stereo. It excels in producing coherent musical structures from natural language prompts, ideal for professional audio production.",
    "badges": ["Audio", "Open Source"],
    "metadata": {
      "released": "March 2024",
      "version": "2.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text-to-Audio",
      "Full Tracks",
      "High-Quality Audio",
      "Musical Structure"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Video 4D",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An innovative video-to-4D model released in July 2024, generating dynamic novel-view videos from a single input video with eight new angles. It enhances creative versatility for filmmaking, gaming, and virtual reality by delivering immersive, multi-perspective content.",
    "badges": ["Video", "4D Generation"],
    "metadata": {
      "released": "July 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Video-to-4D",
      "Novel Views",
      "Immersive Content",
      "Multi-angle Generation"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.1/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Fast 3D",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A breakthrough 3D asset generation model launched in July 2024, transforming a single input image into a detailed 3D object in seconds. It sets new standards for speed and quality, serving industries like gaming, entertainment, and industrial design.",
    "badges": ["3D", "Asset Generation"],
    "metadata": {
      "released": "July 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Image-to-3D",
      "Fast Generation",
      "High-Quality Assets",
      "Industry Applications"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.2/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Point Aware 3D (SPAR3D)",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A cutting-edge 3D generation model introduced in January 2025, enabling real-time editing and complete structure generation from a single image in under a second. It supports rapid prototyping for gaming, architecture, and entertainment with high precision and efficiency.",
    "badges": ["3D", "Real-time"],
    "metadata": {
      "released": "January 2025",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Real-time Editing",
      "Image-to-3D",
      "High Precision",
      "Rapid Prototyping"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.3/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable Virtual Camera",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "A research-preview multi-view diffusion model launched in March 2025, transforming 2D images into immersive 3D videos with realistic depth and perspective. It eliminates the need for complex reconstruction, making it ideal for virtual reality and cinematic applications.",
    "badges": ["3D", "Video", "Research"],
    "metadata": {
      "released": "March 2025",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "2D-to-3D Video",
      "Immersive Depth",
      "No Reconstruction",
      "Cinematic Use"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.1/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Stable LM",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An open-source language model suite launched in April 2023, with 3B to 7B parameter models designed for efficient text and code generation on personal devices. Trained on a massive dataset three times larger than The Pile, it offers high performance for conversational and coding tasks.",
    "badges": ["LLM", "Language", "Open Source"],
    "metadata": {
      "released": "April 2023",
      "version": "Alpha",
      "size": "3B-7B parameters"
    },
    "features": [
      "Text Generation",
      "Code Generation",
      "Efficient",
      "Consumer Devices"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.7/10",
      "license": "CC BY-SA-4.0"
    }
  },
  {
    "name": "Stable LM 2",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An advanced language model series released in April 2024, featuring a 12B parameter base model and an instruction-tuned variant trained on 2 trillion tokens in seven languages. It balances performance, efficiency, and multilingual capabilities for text generation and conversational tasks.",
    "badges": ["LLM", "Language", "Multilingual"],
    "metadata": {
      "released": "April 2024",
      "version": "2.0",
      "size": "12B parameters"
    },
    "features": [
      "Multilingual",
      "Text Generation",
      "Instruction-tuned",
      "Efficient"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "8.9/10",
      "license": "CC BY-SA-4.0"
    }
  },
  {
    "name": "Stable Code Instruct 3B",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "An instruction-tuned language model built on Stable Code 3B, launched in March 2024, optimized for code completion and natural language interactions. It enhances programming efficiency by generating accurate, context-aware code, supporting developers in software development tasks.",
    "badges": ["LLM", "Code", "Open Source"],
    "metadata": {
      "released": "March 2024",
      "version": "3B",
      "size": "3B parameters"
    },
    "features": [
      "Code Completion",
      "Natural Language",
      "Programming Support",
      "Context-aware"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.8/10",
      "license": "CC BY-SA-4.0"
    }
  },
  {
    "name": "TripoSR",
    "provider": "Stability AI",
    "providerClass": "stability",
    "description": "Developed in partnership with Tripo AI in February 2024, this fast 3D object reconstruction model generates detailed 3D objects from a single image, inspired by large reconstruction models. It caters to entertainment, gaming, and design industries with rapid, high-quality outputs for visualization.",
    "badges": ["3D", "Reconstruction"],
    "metadata": {
      "released": "February 2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Image-to-3D",
      "Fast Reconstruction",
      "High-Quality Outputs",
      "Industry Applications"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "CreativeML Open RAIL-M"
    }
  },
  {
    "name": "Phi-1",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A small language model (SLM) with 1.3 billion parameters, designed for efficient text generation and basic reasoning tasks, particularly in research settings. It achieves strong performance on benchmarks like HumanEval, focusing on lightweight, cost-effective AI solutions for developers.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "June 2023",
      "version": "1.0",
      "size": "1.3B parameters"
    },
    "features": [
      "Text Generation",
      "Efficient",
      "Research-focused",
      "Lightweight"
    ],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.2/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-1.5",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "An enhanced version of Phi-1 with improved reasoning and text generation capabilities, maintaining a compact 1.3 billion parameter size. It excels in tasks like coding and math, offering a balance of efficiency and performance for local deployment.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "September 2023",
      "version": "1.5",
      "size": "1.3B parameters"
    },
    "features": ["Text Generation", "Reasoning", "Coding", "Local Deployment"],
    "stats": {
      "performance": "8.2/10",
      "accuracy": "8.4/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-2",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A 2.7 billion parameter SLM optimized for reasoning, coding, and math, delivering near state-of-the-art performance for its size. It’s designed for low-resource environments, making it ideal for on-device applications and research experimentation.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "December 2023",
      "version": "2.0",
      "size": "2.7B parameters"
    },
    "features": ["Reasoning", "Coding", "Math", "On-device"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-3",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A family of SLMs (3.8B to 14B parameters) launched in April 2024, offering high performance in reasoning, coding, and language tasks at a lower cost. It’s optimized for enterprise use cases, available on Azure AI Foundry and Hugging Face, with strong responsible AI safeguards.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "April 2024",
      "version": "3.0",
      "size": "3.8B-14B parameters"
    },
    "features": ["Reasoning", "Coding", "Enterprise-ready", "Responsible AI"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-3.5",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "An advanced iteration of Phi-3, released in August 2024, with models like Phi-3.5-mini optimized for Windows Copilot+ PCs, excelling in math and coding. It leverages high-quality datasets and post-training innovations for superior performance in compact, on-device applications.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "August 2024",
      "version": "3.5",
      "size": "3.8B parameters"
    },
    "features": ["Math", "Coding", "On-device", "Responsible AI"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-4",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A 14 billion parameter SLM introduced in December 2024, excelling in complex reasoning, particularly math, and outperforming larger models on competition benchmarks. Available on Azure AI Foundry and Hugging Face, it pushes the boundaries of efficiency and quality for enterprise and research use.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "December 2024",
      "version": "4.0",
      "size": "14B parameters"
    },
    "features": ["Complex Reasoning", "Math", "Enterprise-ready", "Efficient"],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-4-mini",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A compact 3.8 billion parameter model from the Phi-4 family, launched in 2024, offering 30% faster inference and a 128K-token context window for coding and math tasks. It’s tailored for retail and enterprise applications, providing high performance with minimal computational resources.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "2024",
      "version": "4.0",
      "size": "3.8B parameters"
    },
    "features": ["Fast Inference", "Coding", "Math", "Enterprise-ready"],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "MIT"
    }
  },
  {
    "name": "Phi-4-multimodal",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A multimodal SLM launched in 2024, unifying text, speech, and vision for context-aware interactions, such as diagnosing product issues via camera and voice inputs. It’s optimized for enterprise applications like retail kiosks, offering efficient, scalable performance on Azure AI Foundry.",
    "badges": ["SLM", "Multimodal"],
    "metadata": {
      "released": "2024",
      "version": "4.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text Processing",
      "Speech Processing",
      "Vision Processing",
      "Context-aware"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "MIT"
    }
  },
  {
    "name": "MAI-1",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A large-scale AI model with approximately 500 billion parameters, developed in 2024 under Mustafa Suleyman’s leadership, designed to rival top models like GPT-4. It’s being tested for integration into Copilot products, aiming to reduce Microsoft’s reliance on OpenAI with competitive performance.",
    "badges": ["LLM", "Reasoning"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "500B parameters"
    },
    "features": [
      "Text Generation",
      "Reasoning",
      "Complex Problem Solving",
      "Copilot Integration"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "BitNet b1.58 2B4T",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A 1-bit AI model with 2 billion parameters, released in April 2025, designed for hyper-efficient performance on CPUs, including Apple’s M2, using a custom bitnet.cpp framework. It achieves double the speed and lower memory usage compared to traditional models, ideal for resource-constrained devices.",
    "badges": ["SLM", "Open Source"],
    "metadata": {
      "released": "April 2025",
      "version": "1.0",
      "size": "2B parameters"
    },
    "features": ["Efficient", "CPU-optimized", "Low Memory", "Fast Inference"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "8.9/10",
      "license": "MIT"
    }
  },
  {
    "name": "Magma",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A multimodal AI model introduced in February 2025, combining visual and language processing to control software interfaces and robotic systems, enabling agentic AI for autonomous task execution. It features Set-of-Mark and Trace-of-Mark for spatial intelligence, with public code released on GitHub.",
    "badges": ["Multimodal", "Agentic AI"],
    "metadata": {
      "released": "February 2025",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Visual Processing",
      "Language Processing",
      "Robotic Control",
      "Spatial Intelligence"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.1/10",
      "license": "MIT"
    }
  },
  {
    "name": "Muse (WHAM)",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A generative AI model for video game visuals and controller actions, released in February 2025, developed with Ninja Theory and published in Nature. It supports gameplay ideation through the WHAM Demonstrator, with open-source weights and sample data available on Azure AI Foundry.",
    "badges": ["Generative AI", "Open Source"],
    "metadata": {
      "released": "February 2025",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Game Visuals",
      "Controller Actions",
      "Gameplay Ideation",
      "Interactive Interface"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.0/10",
      "license": "MIT"
    }
  },
  {
    "name": "Turing",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "A family of language models developed by Microsoft Research, used across products like Bing and Azure for tasks like search and text generation. It leverages advanced techniques for efficiency and performance, contributing to Microsoft’s AI infrastructure.",
    "badges": ["LLM", "Proprietary"],
    "metadata": {
      "released": "Not disclosed",
      "version": "Not disclosed",
      "size": "Not disclosed"
    },
    "features": ["Text Generation", "Search", "Efficient", "Scalable"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "E.L.Y. Crop Protection",
    "provider": "Microsoft (with Bayer)",
    "providerClass": "microsoft",
    "description": "A specialized SLM developed with Bayer, available in Azure AI Foundry, designed for sustainable crop protection in agriculture. Trained on thousands of real-world questions from Bayer’s crop protection labels, it supports compliance and knowledge for agricultural entities.",
    "badges": ["SLM", "Industry-specific"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Crop Protection",
      "Compliance",
      "Agricultural Intelligence",
      "Sustainable"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Healthcare AI Models",
    "provider": "Microsoft (with Providence, Paige.ai)",
    "providerClass": "microsoft",
    "description": "Multimodal medical imaging models launched in 2024 in Azure AI Studio, enabling analysis of data in specialties like ophthalmology, pathology, radiology, and cardiology. They integrate diverse data types to enhance healthcare diagnostics and decision-making for professionals.",
    "badges": ["Multimodal", "Healthcare"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Medical Imaging",
      "Multimodal Analysis",
      "Diagnostics",
      "Healthcare Integration"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "DeBERTa",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "DeBERTa is a family of transformer-based language models (including base, large, V2, V3, and multilingual variants) that enhances BERT with disentangled attention and ELECTRA-style pre-training, achieving top performance on benchmarks like SuperGLUE and SQuAD. With sizes ranging from 22M to 1.5B parameters, it supports tasks like text classification, question answering, and cross-lingual transfer, powering Microsoft’s Turing NLRv4 for Bing and Azure.",
    "badges": ["LLM", "Open Source", "Multilingual"],
    "metadata": {
      "released": "June 2020",
      "version": "3.0 (latest)",
      "size": "22M-1.5B parameters"
    },
    "features": [
      "Disentangled Attention",
      "ELECTRA Pre-training",
      "Cross-lingual Transfer",
      "Question Answering"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "MIT"
    }
  },
  {
    "name": "CodeBERT",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "CodeBERT is a bimodal pre-trained model for programming and natural language, leveraging a large corpus of code and comments to excel in tasks like code search and documentation generation. It supports multiple programming languages and is widely used in tools for software development and AI-driven code analysis.",
    "badges": ["LLM", "Open Source", "Code"],
    "metadata": {
      "released": "May 2020",
      "version": "1.0",
      "size": "125M parameters"
    },
    "features": [
      "Code Search",
      "Documentation Generation",
      "Programming Languages",
      "Bimodal Pre-training"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.6/10",
      "license": "MIT"
    }
  },
  {
    "name": "MT-DNN",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "Multi-Task Deep Neural Network (MT-DNN) combines multi-task learning with pre-trained language models like BERT to achieve robust performance across diverse NLP tasks like sentiment analysis and text classification. Its knowledge distillation techniques enable efficient fine-tuning, making it a versatile choice for enterprise NLP applications.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "August 2019",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Multi-task Learning",
      "Knowledge Distillation",
      "Text Classification",
      "Sentiment Analysis"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "MIT"
    }
  },
  {
    "name": "UniLM",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "Unified Language Model (UniLM) is a pre-trained model supporting both natural language understanding and generation tasks, such as summarization and dialogue, through a shared transformer architecture. Its bidirectional, unidirectional, and sequence-to-sequence pre-training objectives make it highly flexible for applications in Azure Cognitive Services.",
    "badges": ["LLM", "Open Source"],
    "metadata": {
      "released": "July 2019",
      "version": "1.0",
      "size": "Not disclosed"
    },
    "features": [
      "Text Generation",
      "Summarization",
      "Dialogue",
      "Flexible Pre-training"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "MIT"
    }
  },
  {
    "name": "DialoGPT",
    "provider": "Microsoft",
    "providerClass": "microsoft",
    "description": "DialoGPT is a conversational model trained on Reddit dialogues, designed to generate human-like responses for interactive chat applications. Its GPT-2-based architecture and large-scale dialogue data enable coherent and contextually relevant conversations, influencing later models like BlenderBot.",
    "badges": ["LLM", "Open Source", "Conversational"],
    "metadata": {
      "released": "November 2019",
      "version": "1.0",
      "size": "345M parameters"
    },
    "features": [
      "Conversational AI",
      "Dialogue Generation",
      "Context-aware",
      "Human-like Responses"
    ],
    "stats": {
      "performance": "8.2/10",
      "accuracy": "8.4/10",
      "license": "MIT"
    }
  },
  {
    "name": "DeepSeek Coder",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek Coder is an open-source model optimized for programming tasks, enabling code generation and completion with high accuracy. It laid the foundation for DeepSeek's later coding-focused models.",
    "badges": ["LLM", "Open Source", "Coding"],
    "metadata": {
      "released": "November 2023",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["Code Generation", "Code Completion", "Programming Support"],
    "stats": {
      "performance": "8.0/10",
      "accuracy": "8.2/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek LLM",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek LLM is a general-purpose language model available in Base and Chat variants, trained on 2 trillion tokens of English and Chinese text. It excels in text generation and conversational tasks.",
    "badges": ["LLM", "Open Source", "General Purpose"],
    "metadata": {
      "released": "November 2023",
      "version": "1.0",
      "size": "7B and 67B parameters"
    },
    "features": [
      "Text Generation",
      "Conversational AI",
      "Multilingual Support",
      "Context-aware"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-MoE",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-MoE is a Mixture-of-Experts model designed for efficient computation, balancing performance and resource usage with shared and routed experts.",
    "badges": ["LLM", "Open Source", "Efficient"],
    "metadata": {
      "released": "January 2024",
      "version": "1.0",
      "size": "16B parameters (2.7B activated)"
    },
    "features": [
      "Efficient Processing",
      "Text Generation",
      "Conversational AI"
    ],
    "stats": {
      "performance": "8.1/10",
      "accuracy": "8.3/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-Math",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-Math is a specialized model for mathematical reasoning, available in Base, Instruct, and RL variants, designed to solve complex mathematical problems.",
    "badges": ["LLM", "Open Source", "Mathematical"],
    "metadata": {
      "released": "April 2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Mathematical Reasoning",
      "Problem Solving",
      "Instruction Following"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.6/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-V2",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-V2 is a cost-efficient general-purpose LLM with improved performance over its predecessor, triggering a price war in China's AI market.",
    "badges": ["LLM", "Open Source", "Cost-efficient"],
    "metadata": {
      "released": "May 2024",
      "version": "2.0",
      "size": "Not specified"
    },
    "features": [
      "Text Generation",
      "Conversational AI",
      "Cost-efficient Inference"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-Coder-V2",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-Coder-V2 is an advanced coding model with a large context window, designed for complex programming challenges and extensive codebases.",
    "badges": ["LLM", "Open Source", "Coding"],
    "metadata": {
      "released": "July 2024",
      "version": "2.0",
      "size": "236B parameters"
    },
    "features": [
      "Code Generation",
      "Code Completion",
      "Large Context Window",
      "Programming Support"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-V2.5",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-V2.5 is an incremental update to V2, offering refined performance for general-purpose tasks while maintaining cost-efficiency.",
    "badges": ["LLM", "Open Source", "General Purpose"],
    "metadata": {
      "released": "September 2024",
      "version": "2.5",
      "size": "Not specified"
    },
    "features": [
      "Text Generation",
      "Conversational AI",
      "Cost-efficient Inference"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "DeepSeek License"
    }
  },
  {
    "name": "DeepSeek-R1-Lite",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-R1-Lite is a lightweight preview model focused on reasoning, available for early access testing via API and chat.",
    "badges": ["LLM", "Open Source", "Reasoning"],
    "metadata": {
      "released": "November 2024",
      "version": "Preview",
      "size": "Not specified"
    },
    "features": ["Reasoning", "Conversational AI", "Lightweight"],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "Not specified"
    }
  },
  {
    "name": "DeepSeek-V3",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-V3 is a Mixture-of-Experts model excelling in math and coding, trained on 14.8 trillion tokens with advanced efficiency techniques like MLA and MTP.",
    "badges": ["LLM", "Open Source", "MoE"],
    "metadata": {
      "released": "December 2024",
      "version": "3.0",
      "size": "671B parameters (37B activated)"
    },
    "features": [
      "Mathematical Reasoning",
      "Code Generation",
      "Efficient Processing",
      "Large Context Window"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "MIT"
    }
  },
  {
    "name": "DeepSeek-R1",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-R1 is a reasoning-focused model fine-tuned from V3, competing with top models like OpenAI’s o1 in math, coding, and reasoning tasks.",
    "badges": ["LLM", "Open Source", "Reasoning"],
    "metadata": {
      "released": "January 2025",
      "version": "1.0",
      "size": "671B parameters (37B activated)"
    },
    "features": [
      "Chain-of-Thought Reasoning",
      "Mathematical Reasoning",
      "Code Generation",
      "Self-correction"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.4/10",
      "license": "MIT"
    }
  },
  {
    "name": "Janus-Pro-7B",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "Janus-Pro-7B is a multimodal vision model for image understanding and generation, outperforming models like DALL-E 3 on key benchmarks.",
    "badges": ["Multimodal", "Open Source", "Vision"],
    "metadata": {
      "released": "January 2025",
      "version": "1.0",
      "size": "7B parameters"
    },
    "features": [
      "Image Understanding",
      "Image Generation",
      "Multimodal Processing"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "MIT"
    }
  },
  {
    "name": "DeepSeek-V3-0324",
    "provider": "DeepSeek",
    "providerClass": "deepseek",
    "description": "DeepSeek-V3-0324 is an upgraded V3 model with enhanced reasoning, coding, and tool-use capabilities, outperforming GPT-4.5 in math and coding.",
    "badges": ["LLM", "Open Source", "MoE"],
    "metadata": {
      "released": "March 2025",
      "version": "3.0324",
      "size": "671B parameters (37B activated)"
    },
    "features": [
      "Mathematical Reasoning",
      "Code Generation",
      "Tool Use",
      "Large Context Window"
    ],
    "stats": {
      "performance": "9.3/10",
      "accuracy": "9.5/10",
      "license": "MIT"
    }
  },
  {
    "name": "PeopleNet",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "PeopleNet is a computer vision model developed using NVIDIA TAO for real-time pedestrian detection and tracking in urban environments, optimized for smart cities and autonomous vehicles.",
    "badges": ["Computer Vision", "Pretrained", "Real-time"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Pedestrian Detection",
      "Object Tracking",
      "Real-time Processing"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "Bi3D",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Bi3D is a binary depth classification network for classifying object depth, ideal for collision avoidance in autonomous mobile robots.",
    "badges": ["Computer Vision", "Pretrained", "Depth Estimation"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Depth Classification",
      "Collision Avoidance",
      "Efficient Processing"
    ],
    "stats": {
      "performance": "8.3/10",
      "accuracy": "8.5/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "BioBERT",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "BioBERT is a BERT-based model fine-tuned on biomedical datasets for text mining and NLP tasks, optimized for identifying chemical and protein entities.",
    "badges": ["NLP", "Pretrained", "Biomedical"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Biomedical Text Mining",
      "Entity Recognition",
      "Context-aware Processing"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "Spleen Segmentation",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Spleen Segmentation is a pretrained model for volumetric 3D segmentation of the spleen from CT images, using advanced medical segmentation techniques.",
    "badges": ["Computer Vision", "Pretrained", "Medical"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["3D Segmentation", "Medical Imaging", "High Accuracy"],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "Conformer",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Conformer is a convolution-augmented transformer model for automatic speech recognition, supporting over 10 languages for applications like live captioning and voice assistants.",
    "badges": ["Speech Recognition", "Pretrained", "Multilingual"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["Speech Recognition", "Multilingual Support", "High Accuracy"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "ECAPA-TDNN",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "ECAPA-TDNN is a time delay neural network-based model for speaker identification and verification, providing robust speaker embeddings for applications like medical conversation analysis.",
    "badges": ["Speech AI", "Pretrained", "Speaker Identification"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Speaker Identification",
      "Speaker Verification",
      "Robust Embeddings"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "Megatron 530B",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Megatron 530B is a transformer-based language model using ELECTRA pretraining, optimized for NLP tasks like chatbots and virtual assistants with smaller size and faster training.",
    "badges": ["LLM", "Pretrained", "Conversational"],
    "metadata": {
      "released": "2022",
      "version": "1.0",
      "size": "530B parameters"
    },
    "features": ["Text Generation", "Conversational AI", "Efficient Training"],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "NVIDIA License"
    }
  },
  {
    "name": "NVLM 1.0",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "NVLM 1.0 is a family of multimodal large language models, led by NVLM-D-72B, excelling in vision-language tasks and rivaling proprietary models like GPT-4o.",
    "badges": ["LLM", "Open Source", "Multimodal"],
    "metadata": {
      "released": "October 2024",
      "version": "1.0",
      "size": "72B parameters"
    },
    "features": [
      "Vision-Language Processing",
      "Text Generation",
      "High Accuracy",
      "Open Source"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.4/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Llama-3.1-Nemotron-70B-Instruct",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Llama-3.1-Nemotron-70B-Instruct is a high-performance language model optimized by NVIDIA, surpassing GPT-4o and Claude 3.5 in benchmarks for reasoning and instruction tasks.",
    "badges": ["LLM", "Open Source", "Reasoning"],
    "metadata": {
      "released": "October 2024",
      "version": "1.0",
      "size": "70B parameters"
    },
    "features": [
      "Instruction Following",
      "Reasoning",
      "Text Generation",
      "High Accuracy"
    ],
    "stats": {
      "performance": "9.3/10",
      "accuracy": "9.5/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Cosmos Nemotron",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "Cosmos Nemotron is an open reasoning model for physical AI development, offering customizable world generation for robotics and simulation.",
    "badges": ["LLM", "Open Source", "Physical AI"],
    "metadata": {
      "released": "March 2025",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "World Generation",
      "Physical AI Reasoning",
      "Customizable Simulation"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.3/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "GR00T N1",
    "provider": "NVIDIA",
    "providerClass": "nvidia",
    "description": "GR00T N1 is an open, customizable foundation model for humanoid robot reasoning, enabling advanced perception and action in robotics.",
    "badges": ["Robotics", "Open Source", "Humanoid AI"],
    "metadata": {
      "released": "March 2025",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["Humanoid Robot Reasoning", "Perception", "Action Planning"],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "OpenELM",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "OpenELM (Open-source Efficient Language Models) is a family of small language models designed for on-device text processing, optimized for efficiency and accuracy using a layer-wise scaling strategy.",
    "badges": ["LLM", "Open Source", "On-device"],
    "metadata": {
      "released": "April 2024",
      "version": "1.0",
      "size": "270M to 3B parameters"
    },
    "features": [
      "Text Generation",
      "On-device Processing",
      "Efficient Inference",
      "Instruction Tuning"
    ],
    "stats": {
      "performance": "8.4/10",
      "accuracy": "8.6/10",
      "license": "Apple Sample Code License"
    }
  },
  {
    "name": "MM1",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "MM1 (MultiModal 1) is a multimodal AI model capable of processing text and images, designed for tasks like answering questions about photos and general knowledge queries.",
    "badges": ["Multimodal", "Research", "Generative"],
    "metadata": {
      "released": "March 2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Image Analysis",
      "Text Generation",
      "Question Answering",
      "Multimodal Processing"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Not specified"
    }
  },
  {
    "name": "MGIE",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "MGIE (MLLM-Guided Image Editing) is an open-source AI model for instruction-based image editing, using multimodal large language models to interpret and execute natural language editing commands.",
    "badges": ["Multimodal", "Open Source", "Image Editing"],
    "metadata": {
      "released": "February 2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Image Editing",
      "Natural Language Processing",
      "Multimodal Processing",
      "Pixel-level Manipulation"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Ferret-UI",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "Ferret-UI is a multimodal model designed to understand and navigate user interfaces on mobile devices, enabling contextual interaction with smartphone screens.",
    "badges": ["Multimodal", "Research", "UI Interaction"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "UI Navigation",
      "Contextual Understanding",
      "Multimodal Processing"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "Not specified"
    }
  },
  {
    "name": "ReALM",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "ReALM (Reference Resolution As Language Modeling) is a language model focused on resolving contextual references, enhancing Siri’s ability to understand follow-up queries based on on-device data.",
    "badges": ["LLM", "Research", "Contextual"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Reference Resolution",
      "Contextual Understanding",
      "On-device Processing"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Not specified"
    }
  },
  {
    "name": "Keyframer",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "Keyframer is an AI tool leveraging large language models to animate static images based on text prompts, enabling creative animation design.",
    "badges": ["Generative", "Research", "Animation"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["Image Animation", "Text-based Animation", "Creative Design"],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.7/10",
      "license": "Not specified"
    }
  },
  {
    "name": "Apple Intelligence On-Device Model",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "A compact language model integrated into Apple Intelligence for on-device tasks like text refinement, notification summarization, and Siri enhancements, optimized for privacy and efficiency.",
    "badges": ["LLM", "On-device", "Privacy-focused"],
    "metadata": {
      "released": "October 2024",
      "version": "1.0",
      "size": "~3B parameters"
    },
    "features": [
      "Text Refinement",
      "Notification Summarization",
      "Siri Enhancements",
      "On-device Processing"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "9.1/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Apple Intelligence Server Model",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "A larger language model running on Apple’s Private Cloud Compute platform, designed for complex tasks requiring more computational power while maintaining end-to-end encryption.",
    "badges": ["LLM", "Cloud-based", "Privacy-focused"],
    "metadata": {
      "released": "October 2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Complex Task Processing",
      "Text Generation",
      "Privacy-focused Computing",
      "Cloud Integration"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Apple Intelligence Coding Model",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "A specialized model for Xcode integration, designed to assist developers with code generation and intelligence within Apple’s development environment.",
    "badges": ["LLM", "Coding", "Developer-focused"],
    "metadata": {
      "released": "2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": ["Code Generation", "Code Assistance", "Xcode Integration"],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "9.0/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Apple Intelligence Diffusion Model",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "A diffusion-based model for generating playful images, such as emojis and sketches, integrated into apps like Messages for creative expression.",
    "badges": ["Generative", "Diffusion", "Visual"],
    "metadata": {
      "released": "October 2024",
      "version": "1.0",
      "size": "Not specified"
    },
    "features": [
      "Image Generation",
      "Emoji Creation",
      "Sketch Generation",
      "On-device Processing"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.9/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "CoreML Models",
    "provider": "Apple",
    "providerClass": "apple",
    "description": "A collection of 20 open-source machine learning models optimized for on-device AI, supporting tasks like image classification, semantic segmentation, and audio-to-text conversion.",
    "badges": ["ML", "Open Source", "On-device"],
    "metadata": {
      "released": "June 2024",
      "version": "1.0",
      "size": "Varies (e.g., DETR-Resnet50, FastViT)"
    },
    "features": [
      "Image Classification",
      "Semantic Segmentation",
      "Audio-to-Text",
      "On-device Processing"
    ],
    "stats": {
      "performance": "8.6/10",
      "accuracy": "8.8/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen1.5 Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "An improved version of the Qwen large language model series, available in various sizes including a Mixture-of-Experts (MoE) model.",
    "badges": ["LLM", "Open Source", "Multilingual"],
    "metadata": {
      "released": "November 2024",
      "version": "1.5",
      "sizes": [
        "0.5B",
        "1.8B",
        "4B",
        "7B",
        "14B",
        "32B",
        "72B",
        "110B",
        "MoE-A2.7B"
      ]
    },
    "features": [
      "Enhanced Language Understanding",
      "Instruction Following",
      "Multilingual Support"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen2 Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "A series of base and instruction-tuned models with sizes ranging from 0.5B to 72B, including a Mixture-of-Experts model.",
    "badges": ["LLM", "Instruction-Tuned", "Open Source"],
    "metadata": {
      "released": "June 2024",
      "version": "2.0",
      "sizes": ["0.5B", "1.5B", "7B", "57B-A14B-MoE", "72B"]
    },
    "features": [
      "Instruction Following",
      "Multilingual Support",
      "Long Context Understanding"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "8.8/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen2.5 Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "The latest series of decoder-only language models, available in various sizes and optimized for instruction following and structured output generation.",
    "badges": ["LLM", "Instruction-Tuned", "Open Source"],
    "metadata": {
      "released": "January 2025",
      "version": "2.5",
      "sizes": ["0.5B", "1.5B", "3B", "7B", "14B", "32B", "72B"]
    },
    "features": [
      "Instruction Following",
      "Structured Output Generation",
      "Multilingual Support"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.0/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "CodeQwen1.5-7B",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "A code-specific version of Qwen1.5, supporting 92 programming languages and optimized for code generation tasks.",
    "badges": ["Code Generation", "Open Source", "Multilingual"],
    "metadata": {
      "released": "December 2024",
      "version": "1.5",
      "size": "7B"
    },
    "features": [
      "Code Generation",
      "Long Context Understanding (64K tokens)",
      "Text-to-SQL",
      "Bug Fixing"
    ],
    "stats": {
      "performance": "9.1/10",
      "accuracy": "9.0/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen2.5-Coder Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "A series of code-specific models optimized for code generation, reasoning, and fixing, available in multiple sizes.",
    "badges": ["Code Generation", "Open Source", "Multilingual"],
    "metadata": {
      "released": "March 2025",
      "version": "2.5",
      "sizes": ["1.5B", "7B", "32B"]
    },
    "features": [
      "Code Generation",
      "Code Reasoning",
      "Code Fixing",
      "Supports 92 Programming Languages"
    ],
    "stats": {
      "performance": "9.3/10",
      "accuracy": "9.1/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen2-Math Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "A series of math-specific language models built upon Qwen2, designed for complex mathematical problem-solving.",
    "badges": ["Math", "Open Source", "Multilingual"],
    "metadata": {
      "released": "August 2024",
      "version": "2.0",
      "sizes": ["1.5B", "7B", "72B"]
    },
    "features": [
      "Advanced Mathematical Reasoning",
      "Multi-Step Problem Solving",
      "Chain-of-Thought",
      "Tool-Integrated Reasoning"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "9.2/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen2.5-Math Series",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "An advanced math-specific model series extending Qwen2.5 capabilities with high performance in mathematical reasoning tasks.",
    "badges": ["Math", "Open Source", "Multilingual"],
    "metadata": {
      "released": "March 2025",
      "version": "2.5",
      "sizes": ["1.5B", "7B", "72B"]
    },
    "features": [
      "Math Word Problems",
      "Multi-Hop Reasoning",
      "Symbolic Math",
      "MathQA Tasks"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.3/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Qwen-VL-7B",
    "provider": "Alibaba",
    "providerClass": "alibaba",
    "description": "A vision-language model capable of understanding and generating content from images, supporting multi-round question answering.",
    "badges": ["Multimodal", "Vision-Language", "Open Source"],
    "metadata": {
      "released": "September 2023",
      "version": "1.0",
      "size": "7B"
    },
    "features": [
      "Image Understanding",
      "Multi-Round QA",
      "Creative Capabilities",
      "Multilingual Support"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "8.7/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Grok-1",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "The inaugural open-source large language model by xAI, emphasizing truth-seeking and mathematical reasoning.",
    "badges": ["LLM", "Open Source", "Multilingual"],
    "metadata": {
      "released": "March 2024",
      "version": "1.0",
      "size": "Unknown"
    },
    "features": [
      "General Language Understanding",
      "Mathematical Reasoning",
      "Truth-Seeking Responses"
    ],
    "stats": {
      "performance": "8.5/10",
      "accuracy": "8.3/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Grok-1.5",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "An enhanced version of Grok-1 with improved reasoning capabilities and extended context length.",
    "badges": ["LLM", "Open Source", "Multilingual"],
    "metadata": {
      "released": "March 2024",
      "version": "1.5",
      "size": "Unknown"
    },
    "features": [
      "Improved Reasoning",
      "Extended Context Length (128K tokens)",
      "Enhanced Truth-Seeking"
    ],
    "stats": {
      "performance": "8.7/10",
      "accuracy": "8.5/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Grok-1.5 Vision",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "A multimodal model extending Grok-1.5 with vision capabilities for image understanding.",
    "badges": ["Multimodal", "Vision-Language", "Open Source"],
    "metadata": {
      "released": "April 2024",
      "version": "1.5V",
      "size": "Unknown"
    },
    "features": [
      "Image Understanding",
      "Visual Question Answering",
      "Multimodal Reasoning"
    ],
    "stats": {
      "performance": "8.8/10",
      "accuracy": "8.6/10",
      "license": "Apache 2.0"
    }
  },
  {
    "name": "Grok-2",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "The second iteration of the Grok series, introducing image generation capabilities.",
    "badges": ["LLM", "Image Generation", "Multilingual"],
    "metadata": {
      "released": "August 2024",
      "version": "2.0",
      "size": "Unknown"
    },
    "features": [
      "Text-to-Image Generation",
      "Enhanced Language Understanding",
      "Multilingual Support"
    ],
    "stats": {
      "performance": "9.0/10",
      "accuracy": "8.8/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Aurora",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "A text-to-image generation model integrated into the Grok series for creative visual outputs.",
    "badges": ["Image Generation", "Creative AI", "Proprietary"],
    "metadata": {
      "released": "December 2024",
      "version": "1.0",
      "size": "Unknown"
    },
    "features": [
      "Text-to-Image Generation",
      "Creative Visual Outputs",
      "Integration with Grok"
    ],
    "stats": {
      "performance": "8.9/10",
      "accuracy": "8.7/10",
      "license": "Proprietary"
    }
  },
  {
    "name": "Grok-3",
    "provider": "xAI",
    "providerClass": "xai",
    "description": "The latest Grok model featuring reflection capabilities and advanced web search integration.",
    "badges": ["LLM", "Web Search", "Advanced Reasoning"],
    "metadata": {
      "released": "February 2025",
      "version": "3.0",
      "size": "Unknown"
    },
    "features": [
      "Reflection Capabilities",
      "DeepSearch Integration",
      "Advanced Reasoning"
    ],
    "stats": {
      "performance": "9.2/10",
      "accuracy": "9.0/10",
      "license": "Proprietary"
    }
  }
]
