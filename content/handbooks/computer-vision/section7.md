---
layout: default
---

Section VII: Vision Transformers and Large-Scale Models
Chapter 35: Foundations of Vision Transformers
(ViT, DeiT, patch embeddings, self-attention for images, training challenges)
Chapter 36: Hierarchical Vision Transformers
(Swin Transformer, Twins, PVT, Nested ViT, hierarchical design principles)
Chapter 37: Vision Transformers for Object Detection
(DETR, Deformable DETR, DINO, YOLOS, ViTDet)
Chapter 38: Vision Transformers for Segmentation
(SegFormer, Mask2Former, SETR, Swin-Unet, Segmenter)
Chapter 39: Vision Transformers for Video and Temporal Tasks
(Video Swin Transformer, TimeSformer, ViViT, MViT)
Chapter 40: Hybrid CNN-Transformer Architectures
(ConvNeXt, CoAtNet, LeViT, CvT, BoTNet)
Chapter 41: Vision Large Language Models (vLLMs)
(Flamingo, BLIP, LLaVA, CLIP-ViT, GIT, visual reasoning, image-text alignment)
Chapter 42: Scaling and Optimizing Vision Transformers
(Efficient ViTs, Sparse Transformers, Long-Range ViTs, FlashAttention for ViTs)
Chapter 43: Task-Specific ViT Innovations
(ViTPose, TransReID, ViTGAN, ViT-based OCR)
