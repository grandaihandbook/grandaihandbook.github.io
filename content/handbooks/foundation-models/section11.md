---
layout: default
title: "Instruction Tuning and RLHF"
description: "Explore fine-tuning techniques using instructions and reinforcement learning with human feedback."
---

<link rel="stylesheet" href="{{ '/assets/css/section-academic.css' | relative_url }}">

InstructGPT, RLHF pipeline, reward modeling, proximal policy optimization, supervised fine-tuning, alignment objectives

<script>
  // Navigation variables - no previous for index
  window.prevSection = "/content/handbooks/foundation-models/section10/";
  window.nextSection = "/content/handbooks/foundation-models/section12/";
</script>

<script src="{{ '/assets/js/section-academic.js' | relative_url }}"></script>
