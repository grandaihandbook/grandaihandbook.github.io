<link rel="stylesheet" href="/assets/css/research/landmark-papers.css">

<div class="landmark-papers-container">
  <div class="landmark-header">
    <h1>Landmark Papers in Computer Vision</h1>
    <p>Explore the foundational research that has shaped the field of Computer Vision. This curated collection highlights the most influential papers that established key concepts, techniques, and breakthroughs in the evolution of computer vision systems.</p>
  </div>
  <div class="attribution-notice">
    <div class="attribution-content">
      <p>Landmark Papers in Computer Vision is a curated collection showcasing the foundational research that has shaped the field of computer vision. I've carefully selected these papers to highlight the key breakthroughs and conceptual advances that have defined the evolution of visual perception systems, providing historical context and significance for researchers and enthusiasts alike.</p>
    </div>
  </div>
  
  <div class="year-section">
    <h2 class="year-heading">1960s-1980s</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 1963</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Edge Detection</span>
              <span class="landmark-tag">3D Vision</span>
            </div>
          </div>
          <h3 class="landmark-title">Machine Perception of Three-Dimensional Solids</h3>
          <p class="landmark-significance">This pioneering work by Roberts at MIT introduced the Roberts Operator, one of the first algorithms for edge detection and laid the groundwork for computational approaches to 3D object recognition from 2D images, establishing fundamental techniques for extracting structure from visual data.</p>
          <a href="https://www.computer.org/csdl/proceedings-article/afips/1963/50650145/12OmNvT2nEL" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 1973</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Detection</span>
              <span class="landmark-tag">Edge Analysis</span>
            </div>
          </div>
          <h3 class="landmark-title">Computer Detection of Human Faces</h3>
          <p class="landmark-significance">This early work from USC on automated face detection established initial approaches for computational face recognition, exploring edge-based techniques to isolate and identify facial features in images decades before modern deep learning approaches.</p>
          <a href="https://dl.acm.org/doi/10.1145/953748.953749" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 1980</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Biological Vision</span>
              <span class="landmark-tag">Edge Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Theory of Edge Detection</h3>
          <p class="landmark-significance">David Marr's influential work at MIT provided a comprehensive computational framework for human visual perception, connecting biological vision systems to computational models and introducing the concept of multi-scale representations that continues to influence modern computer vision.</p>
          <a href="https://royalsocietypublishing.org/doi/10.1098/rspb.1980.0020" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 1982</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Neural Networks</span>
              <span class="landmark-tag">Pattern Recognition</span>
            </div>
          </div>
          <h3 class="landmark-title">Neocognitron: A Self-organizing Neural Network Model for Pattern Recognition</h3>
          <p class="landmark-significance">Fukushima's groundbreaking work at NHK Labs introduced the Neocognitron, a hierarchical neural network inspired by the visual cortex that established the concept of increasingly complex feature extraction through layers, directly influencing modern convolutional neural networks.</p>
          <a href="https://link.springer.com/article/10.1007/BF00344251" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 1986</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Edge Detection</span>
              <span class="landmark-tag">Image Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">A Computational Approach to Edge Detection</h3>
          <p class="landmark-significance">John Canny's work at MIT introduced the Canny edge detector, a multi-stage algorithm that optimizes detection, localization, and minimal response criteria, becoming the most widely used edge detection method and establishing mathematical rigor in feature extraction.</p>
          <a href="https://ieeexplore.ieee.org/document/4767851" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 1988</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Motion Estimation</span>
              <span class="landmark-tag">Video Analysis</span>
            </div>
          </div>
          <h3 class="landmark-title">A Computational Framework for the Visual Motion</h3>
          <p class="landmark-significance">This seminal work from MIT established fundamental methods for optical flow calculation, providing mathematical techniques to estimate motion between frames that remain foundational for video processing, action recognition, and object tracking applications.</p>
          <a href="https://link.springer.com/article/10.1007/BF00133571" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 1989</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Neural Networks</span>
              <span class="landmark-tag">OCR</span>
            </div>
          </div>
          <h3 class="landmark-title">Backpropagation Applied to Handwritten Zip Code Recognition</h3>
          <p class="landmark-significance">This influential work from Bell Labs demonstrated the practical application of neural networks with backpropagation for visual pattern recognition, establishing a framework for training deep networks on image data that would eventually lead to modern deep learning approaches.</p>
          <a href="https://ieeexplore.ieee.org/document/6795724" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>
    <div class="year-section">
    <h2 class="year-heading">1990s</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 1991</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Recognition</span>
              <span class="landmark-tag">Dimensionality Reduction</span>
            </div>
          </div>
          <h3 class="landmark-title">Eigenfaces for Recognition</h3>
          <p class="landmark-significance">This groundbreaking paper from MIT introduced eigenfaces, a principal component analysis approach to efficiently represent faces in a lower-dimensional space, revolutionizing facial recognition and establishing core techniques for statistical pattern recognition in computer vision.</p>
          <a href="https://ieeexplore.ieee.org/document/4767881" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 1992</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Contour Models</span>
              <span class="landmark-tag">Shape Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Snakes: Active Contour Models</h3>
          <p class="landmark-significance">Kass, Witkin, and Terzopoulos at Imperial College introduced active contour models or "snakes," an energy-minimizing spline guided by external forces and image constraints, establishing a powerful framework for object boundary detection that continues to influence medical image analysis and object segmentation.</p>
          <a href="https://www.cs.toronto.edu/~dt/siggraph97-course/cass-snake.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 1995</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Segmentation</span>
              <span class="landmark-tag">Optimization</span>
            </div>
          </div>
          <h3 class="landmark-title">Graph Cuts for Image Segmentation</h3>
          <p class="landmark-significance">This influential work from Cornell introduced the application of graph cut optimization to image segmentation, formulating the problem as finding the minimum cut in a graph, establishing energy minimization approaches that would transform object segmentation and stereo correspondence.</p>
          <a href="https://ieeexplore.ieee.org/document/6797526" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 1997</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Spectral Methods</span>
              <span class="landmark-tag">Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">Normalized Cuts and Image Segmentation</h3>
          <p class="landmark-significance">Shi and Malik at Berkeley introduced normalized cuts, a theoretically sound spectral clustering approach to image segmentation that measures both the dissimilarity between different groups and the similarity within groups, establishing a foundation for perceptual grouping in computer vision.</p>
          <a href="https://ieeexplore.ieee.org/document/868688" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 1998</span>
            <div class="landmark-tags">
              <span class="landmark-tag">CNNs</span>
              <span class="landmark-tag">Document Analysis</span>
            </div>
          </div>
          <h3 class="landmark-title">Gradient-Based Learning Applied to Document Recognition</h3>
          <p class="landmark-significance">Yann LeCun and colleagues at AT&T/Bell Labs introduced LeNet-5, a pioneering convolutional neural network architecture for handwritten digit recognition that demonstrated end-to-end training from pixels to classification, establishing the foundation for modern deep learning approaches in computer vision.</p>
          <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 1999</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Manifold Learning</span>
              <span class="landmark-tag">Dimensionality Reduction</span>
            </div>
          </div>
          <h3 class="landmark-title">A Global Geometric Framework for Nonlinear Dimensionality Reduction</h3>
          <p class="landmark-significance">This influential work from Stanford introduced ISOMAP, a technique for discovering nonlinear manifolds in high-dimensional data that preserves geodesic distances, establishing a powerful approach for understanding the intrinsic structure of visual data that influenced subsequent manifold learning methods.</p>
          <a href="https://science.sciencemag.org/content/290/5500/2319" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>
  <div class="year-section">
    <h2 class="year-heading">2000-2009</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2001</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Detection</span>
              <span class="landmark-tag">Cascaded Classifiers</span>
            </div>
          </div>
          <h3 class="landmark-title">Rapid Object Detection using a Boosted Cascade of Simple Features</h3>
          <p class="landmark-significance">Viola and Jones at Mitsubishi/MIT introduced a revolutionary real-time face detection framework using Haar-like features and AdaBoost, the first algorithm capable of reliable face detection at 15+ frames per second, transforming practical computer vision applications and enabling embedded vision systems.</p>
          <a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2003</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Recognition</span>
              <span class="landmark-tag">Part-Based Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Pictorial Structures for Object Recognition</h3>
          <p class="landmark-significance">Felzenszwalb and Huttenlocher at Berkeley formalized pictorial structures, representing objects as collections of parts arranged in deformable configurations, establishing a mathematically principled approach to object recognition that would later influence part-based models and pose estimation.</p>
          <a href="https://www.cs.cornell.edu/~dph/papers/pict-struct-ijcv.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2004</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Feature Detection</span>
              <span class="landmark-tag">Scale Invariance</span>
            </div>
          </div>
          <h3 class="landmark-title">Distinctive Image Features from Scale-Invariant Keypoints</h3>
          <p class="landmark-significance">David Lowe at the University of British Columbia introduced SIFT (Scale-Invariant Feature Transform), a groundbreaking algorithm for detecting and describing local features invariant to scale, rotation, and illumination changes, revolutionizing object recognition, image matching, and 3D reconstruction.</p>
          <a href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2005</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Feature Descriptors</span>
              <span class="landmark-tag">Human Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Histograms of Oriented Gradients for Human Detection</h3>
          <p class="landmark-significance">Dalal and Triggs at INRIA introduced HOG (Histograms of Oriented Gradients), a feature descriptor that captures local gradient orientation statistics, dramatically improving human detection performance and establishing a descriptor that would influence object recognition approaches for over a decade.</p>
          <a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2006</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Feature Detection</span>
              <span class="landmark-tag">Real-time</span>
            </div>
          </div>
          <h3 class="landmark-title">SURF: Speeded Up Robust Features</h3>
          <p class="landmark-significance">Bay and colleagues at ETH Zurich introduced SURF, a computationally efficient alternative to SIFT that used integral images and box filters to approximate derivatives, significantly accelerating feature detection and description while maintaining robustness for real-time applications.</p>
          <a href="https://www.vision.ee.ethz.ch/~surf/eccv06.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2008</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Binary Descriptors</span>
              <span class="landmark-tag">Efficiency</span>
            </div>
          </div>
          <h3 class="landmark-title">BRIEF: Binary Robust Independent Elementary Features</h3>
          <p class="landmark-significance">Calonder and colleagues at EPFL introduced BRIEF, a binary feature descriptor that used simple intensity difference tests to create highly discriminative bit strings, dramatically reducing memory requirements and computation time compared to floating-point descriptors like SIFT and SURF.</p>
          <a href="https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2009</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Dataset</span>
              <span class="landmark-tag">Visual Recognition</span>
            </div>
          </div>
          <h3 class="landmark-title">ImageNet: A Large-Scale Hierarchical Image Database</h3>
          <p class="landmark-significance">Deng and colleagues at Princeton introduced ImageNet, a massive dataset of over 14 million labeled images organized according to WordNet hierarchy, providing unprecedented scale for training visual recognition systems and ultimately catalyzing the deep learning revolution in computer vision.</p>
          <a href="https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>
  <div class="year-section">
    <h2 class="year-heading">2010-2015</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2010</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Benchmark</span>
              <span class="landmark-tag">Object Recognition</span>
            </div>
          </div>
          <h3 class="landmark-title">The PASCAL Visual Object Classes Challenge</h3>
          <p class="landmark-significance">Everingham and colleagues at Oxford/Edinburgh established the PASCAL VOC challenge, creating standardized datasets and evaluation protocols for object detection and segmentation that became the primary benchmark for comparing computer vision algorithms for nearly a decade.</p>
          <a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2011</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Part-Based Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Object Detection with Discriminatively Trained Part-Based Models</h3>
          <p class="landmark-significance">Felzenszwalb and colleagues at the University of Chicago introduced Deformable Part Models (DPM), a discriminative approach combining HOG features with latent SVM training to model objects as collections of parts, setting state-of-the-art performance in object detection before the deep learning revolution.</p>
          <a href="https://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2012</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Deep Learning</span>
              <span class="landmark-tag">Image Classification</span>
            </div>
          </div>
          <h3 class="landmark-title">ImageNet Classification with Deep Convolutional Neural Networks</h3>
          <p class="landmark-significance">Krizhevsky, Sutskever, and Hinton at the University of Toronto introduced AlexNet, a deep convolutional neural network that dramatically outperformed previous approaches on the ImageNet challenge, catalyzing the deep learning revolution in computer vision and establishing the CNN architecture as the dominant paradigm for visual recognition tasks.</p>
          <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2013</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Deep Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</h3>
          <p class="landmark-significance">Girshick and colleagues at UC Berkeley introduced R-CNN (Regions with CNN features), the first highly effective approach to combine region proposals with deep convolutional features, establishing a new paradigm for object detection that would dominate the field for years to come.</p>
          <a href="https://arxiv.org/pdf/1311.2524.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Spatial Pooling</span>
              <span class="landmark-tag">Multi-Scale Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</h3>
          <p class="landmark-significance">He and colleagues at Microsoft introduced SPPNet, which added a spatial pyramid pooling layer allowing CNNs to handle images of arbitrary size/scale and generate fixed-length representations, significantly improving efficiency by sharing computation across region proposals.</p>
          <a href="https://arxiv.org/pdf/1406.4729.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Network Architecture</span>
              <span class="landmark-tag">Inception Modules</span>
            </div>
          </div>
          <h3 class="landmark-title">Going Deeper with Convolutions</h3>
          <p class="landmark-significance">Szegedy and colleagues at Google introduced GoogLeNet/Inception, a novel architecture using inception modules with parallel convolutions at different scales, dramatically reducing parameters while increasing depth, winning the 2014 ImageNet competition and establishing new principles for efficient network design.</p>
          <a href="https://arxiv.org/pdf/1409.4842.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Network Depth</span>
              <span class="landmark-tag">CNN Architecture</span>
            </div>
          </div>
          <h3 class="landmark-title">Very Deep Convolutional Networks for Large-Scale Image Recognition</h3>
          <p class="landmark-significance">Simonyan and Zisserman at Oxford introduced VGGNet, which demonstrated the importance of network depth by using small 3×3 convolution filters stacked to create effective receptive fields, establishing a simple yet powerful architecture that became a standard feature extractor for many computer vision tasks.</p>
          <a href="https://arxiv.org/pdf/1409.1556.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Recognition</span>
              <span class="landmark-tag">Deep Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Deep Learning Face Representation by Joint Identification-Verification</h3>
          <p class="landmark-significance">Sun and colleagues at CUHK introduced DeepID, a deep learning approach that jointly optimized face identification and verification tasks, significantly advancing face recognition performance and establishing multi-task learning principles that would influence subsequent facial recognition systems.</p>
          <a href="https://papers.nips.cc/paper/2014/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">End-to-End Training</span>
            </div>
          </div>
          <h3 class="landmark-title">Fast R-CNN</h3>
          <p class="landmark-significance">Girshick at Microsoft improved upon R-CNN with Fast R-CNN, which enabled end-to-end detector training by pooling CNN features from regions of interest, dramatically increasing both speed and accuracy for object detection while simplifying the multi-stage training pipeline.</p>
          <a href="https://arxiv.org/pdf/1504.08083.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Region Proposals</span>
              <span class="landmark-tag">Real-time Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h3>
          <p class="landmark-significance">Ren and colleagues at Microsoft introduced Faster R-CNN, which integrated region proposal generation into the detection network with a Region Proposal Network, creating the first near real-time high-accuracy object detection system and establishing a unified framework that influenced numerous subsequent approaches.</p>
          <a href="https://arxiv.org/pdf/1506.01497.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Medical Imaging</span>
              <span class="landmark-tag">Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">U-Net: Convolutional Networks for Biomedical Image Segmentation</h3>
          <p class="landmark-significance">Ronneberger and colleagues at the University of Freiburg introduced U-Net, an elegant encoder-decoder architecture with skip connections that enabled precise segmentation with limited training data, revolutionizing medical image analysis and establishing a fundamental architecture for dense prediction tasks.</p>
          <a href="https://arxiv.org/pdf/1505.04597.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Semantic Segmentation</span>
              <span class="landmark-tag">Pixel-wise Classification</span>
            </div>
          </div>
          <h3 class="landmark-title">Fully Convolutional Networks for Semantic Segmentation</h3>
          <p class="landmark-significance">Long, Shelhamer, and Darrell at Berkeley introduced FCN, transforming classification networks into fully convolutional ones that could produce dense, pixel-wise predictions, establishing the fundamental approach to semantic segmentation that continues to influence modern architectures.</p>
          <a href="https://arxiv.org/pdf/1411.4038.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Residual Learning</span>
              <span class="landmark-tag">Deep Networks</span>
            </div>
          </div>
          <h3 class="landmark-title">Deep Residual Learning for Image Recognition</h3>
          <p class="landmark-significance">He and colleagues at Microsoft introduced ResNet, which enabled training of extremely deep networks through residual connections that created shortcuts across layers, solving the vanishing gradient problem and establishing a fundamental architecture that continues to serve as the backbone for numerous computer vision systems.</p>
          <a href="https://arxiv.org/pdf/1512.03385.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Single-Shot Detection</span>
              <span class="landmark-tag">Real-time</span>
            </div>
          </div>
          <h3 class="landmark-title">SSD: Single Shot MultiBox Detector</h3>
          <p class="landmark-significance">Liu and colleagues at Google introduced SSD, a detection framework that eliminated proposal generation and feature resampling stages by making predictions at multiple scales directly from feature maps, establishing a high-speed detection approach that balanced accuracy and efficiency for real-time applications.</p>
          <a href="https://arxiv.org/pdf/1512.02325.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>
  <div class="year-section">
    <h2 class="year-heading">2016-2019</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Recognition</span>
              <span class="landmark-tag">Deep Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</h3>
          <p class="landmark-significance">Taigman and colleagues at Facebook introduced DeepFace, a deep learning system for face verification that approached human-level performance through 3D alignment, a large-scale private training dataset, and a deep CNN architecture, helping establish face recognition as one of the first computer vision tasks to achieve near-human accuracy.</p>
          <a href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Atrous Convolution</span>
              <span class="landmark-tag">Semantic Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</h3>
          <p class="landmark-significance">Chen and colleagues at Google introduced DeepLabv1, which combined atrous (dilated) convolutions to efficiently capture multi-scale context with fully connected CRFs for boundary refinement, establishing key techniques for accurate semantic segmentation that would influence numerous subsequent approaches.</p>
          <a href="https://arxiv.org/pdf/1606.00915.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Real-time Detection</span>
              <span class="landmark-tag">Single-pass</span>
            </div>
          </div>
          <h3 class="landmark-title">You Only Look Once: Unified, Real-Time Object Detection</h3>
          <p class="landmark-significance">Redmon and colleagues at the University of Washington introduced YOLO, a revolutionary object detection approach that framed detection as a single regression problem from images to bounding boxes and class probabilities, enabling unprecedented speed while maintaining competitive accuracy, establishing a new paradigm for real-time vision.</p>
          <a href="https://arxiv.org/pdf/1506.02640.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Model Compression</span>
              <span class="landmark-tag">Efficiency</span>
            </div>
          </div>
          <h3 class="landmark-title">SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and <0.5MB Model Size</h3>
          <p class="landmark-significance">Iandola and colleagues at UC Berkeley introduced SqueezeNet, a compact CNN architecture that achieved AlexNet-level accuracy with 50x fewer parameters through fire modules combining squeeze and expand operations, establishing important principles for efficient network design for mobile and embedded systems.</p>
          <a href="https://arxiv.org/pdf/1602.07360.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Dataset</span>
              <span class="landmark-tag">Benchmark</span>
            </div>
          </div>
          <h3 class="landmark-title">Microsoft COCO: Common Objects in Context</h3>
          <p class="landmark-significance">Lin and colleagues at Microsoft introduced COCO, a large-scale object detection, segmentation, and captioning dataset with complex everyday scenes containing multiple objects in their natural context, establishing a more challenging benchmark that drove advances in instance segmentation and dense prediction tasks.</p>
          <a href="https://arxiv.org/pdf/1405.0312.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Scene Parsing</span>
              <span class="landmark-tag">Multi-scale Context</span>
            </div>
          </div>
          <h3 class="landmark-title">Pyramid Scene Parsing Network</h3>
          <p class="landmark-significance">Zhao and colleagues at SenseTime/CUHK introduced PSPNet, which utilized a pyramid pooling module to aggregate context at multiple scales, effectively capturing global and local information for scene parsing, establishing a new approach to multi-scale feature representation that influenced numerous segmentation methods.</p>
          <a href="https://arxiv.org/pdf/1612.01105.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Instance Segmentation</span>
              <span class="landmark-tag">Multi-task Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Mask R-CNN</h3>
          <p class="landmark-significance">He and colleagues at Facebook AI Research introduced Mask R-CNN, extending Faster R-CNN with a parallel mask prediction branch for instance segmentation, establishing a flexible framework for multiple vision tasks and achieving state-of-the-art results that would influence object detection and segmentation for years to come.</p>
          <a href="https://arxiv.org/pdf/1703.06870.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Class Imbalance</span>
              <span class="landmark-tag">Dense Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Focal Loss for Dense Object Detection</h3>
          <p class="landmark-significance">Lin and colleagues at Facebook AI introduced focal loss and RetinaNet, addressing the extreme foreground-background class imbalance in dense detection by down-weighting easy examples, enabling single-stage detectors to outperform two-stage approaches and establishing a key technique for addressing imbalanced datasets.</p>
          <a href="https://arxiv.org/pdf/1708.02002.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Mobile Networks</span>
              <span class="landmark-tag">Efficient Architecture</span>
            </div>
          </div>
          <h3 class="landmark-title">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</h3>
          <p class="landmark-significance">Howard and colleagues at Google introduced MobileNets, which utilized depthwise separable convolutions to dramatically reduce computation and parameters while maintaining reasonable accuracy, establishing fundamental techniques for efficient model design that would enable computer vision on resource-constrained devices.</p>
          <a href="https://arxiv.org/pdf/1704.04861.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Channel Shuffling</span>
              <span class="landmark-tag">Mobile Architecture</span>
            </div>
          </div>
          <h3 class="landmark-title">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</h3>
          <p class="landmark-significance">Zhang and colleagues at Face++ introduced ShuffleNet, which utilized pointwise group convolutions and channel shuffling to reduce computation while maintaining accuracy, establishing novel techniques for designing highly efficient networks that influenced numerous subsequent mobile-friendly architectures.</p>
          <a href="https://arxiv.org/pdf/1707.01083.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Attention</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">Attention is All You Need</h3>
          <p class="landmark-significance">Vaswani and colleagues at Google introduced the Transformer architecture based entirely on attention mechanisms, initially for NLP but eventually revolutionizing computer vision by providing a new paradigm beyond convolutions that would lead to Vision Transformers and numerous attention-based visual models.</p>
          <a href="https://arxiv.org/pdf/1706.03762.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Capsule Networks</span>
              <span class="landmark-tag">Hierarchical Representations</span>
            </div>
          </div>
          <h3 class="landmark-title">Dynamic Routing Between Capsules</h3>
          <p class="landmark-significance">Sabour, Hinton and colleagues at Google introduced CapsNet, which modeled hierarchical relationships between object parts using capsules that preserve more information than scalar features, proposing a fundamentally different approach to representation learning addressing key limitations of CNNs.</p>
          <a href="https://arxiv.org/pdf/1710.09829.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Dense Connections</span>
              <span class="landmark-tag">Feature Reuse</span>
            </div>
          </div>
          <h3 class="landmark-title">Densely Connected Convolutional Networks</h3>
          <p class="landmark-significance">Huang and colleagues at Cornell/Tsinghua introduced DenseNet, which connected each layer to every other layer in a feed-forward fashion to encourage feature reuse, improve gradient flow, and reduce parameters, establishing a powerful architecture for efficient learning that influenced numerous subsequent network designs.</p>
          <a href="https://arxiv.org/pdf/1608.06993.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Neural Architecture Search</span>
              <span class="landmark-tag">AutoML</span>
            </div>
          </div>
          <h3 class="landmark-title">Learning Transferable Architectures for Scalable Image Recognition</h3>
          <p class="landmark-significance">Zoph and colleagues at Google introduced NASNet, which used reinforcement learning to search for optimal neural architecture building blocks that could be transferred across datasets, establishing automated architecture design approaches that would launch an entire field of neural architecture search.</p>
          <a href="https://arxiv.org/pdf/1707.07012.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Real-time</span>
            </div>
          </div>
          <h3 class="landmark-title">YOLOv3: An Incremental Improvement</h3>
          <p class="landmark-significance">Redmon and Farhadi at the University of Washington refined the YOLO architecture with multi-scale predictions, better feature extractors, and various design improvements, establishing YOLOv3 as the standard real-time detector balancing speed and accuracy that would be widely adopted in practical applications.</p>
          <a href="https://arxiv.org/pdf/1804.02767.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Style Transfer</span>
              <span class="landmark-tag">Content Manipulation</span>
            </div>
          </div>
          <h3 class="landmark-title">A Neural Algorithm of Artistic Style</h3>
          <p class="landmark-significance">Gatys and colleagues at the University of Tübingen separated and recombined content and style representations from different images, enabling artistic style transfer by optimizing for content similarity and style statistics, establishing a novel application of neural networks that sparked significant interest in creative AI.</p>
          <a href="https://arxiv.org/pdf/1508.06576.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Atrous Convolution</span>
              <span class="landmark-tag">Encoder-Decoder</span>
            </div>
          </div>
          <h3 class="landmark-title">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</h3>
          <p class="landmark-significance">Chen and colleagues at Google introduced DeepLabv3+, which combined an encoder-decoder structure with atrous separable convolutions, establishing a powerful and efficient architecture for semantic segmentation that achieved state-of-the-art results while maintaining computational efficiency.</p>
          <a href="https://arxiv.org/pdf/1802.02611.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">GANs</span>
              <span class="landmark-tag">Image Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Large Scale GAN Training for High Fidelity Natural Image Synthesis</h3>
          <p class="landmark-significance">Brock and colleagues at DeepMind introduced BigGAN, which demonstrated the benefits of scaling up GAN training with larger batch sizes and more parameters, establishing new benchmarks for image synthesis quality and revealing the importance of training dynamics for generative models.</p>
          <a href="https://arxiv.org/pdf/1809.11096.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Keypoint Detection</span>
              <span class="landmark-tag">Object Localization</span>
            </div>
          </div>
          <h3 class="landmark-title">Objects as Points</h3>
          <p class="landmark-significance">Zhou and colleagues at the University of Texas introduced CenterNet, which modeled objects as points (their center) and regressed to other properties, establishing a simple yet effective approach to detection that unified object detection, human pose estimation, and 3D detection in a single framework.</p>
          <a href="https://arxiv.org/pdf/1904.07850.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Efficient Scaling</span>
              <span class="landmark-tag">Compound Scaling</span>
            </div>
          </div>
          <h3 class="landmark-title">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h3>
          <p class="landmark-significance">Tan and Le at Google introduced EfficientNet, which proposed compound scaling that uniformly scales network width, depth, and resolution with fixed coefficients, establishing a family of models that achieved state-of-the-art accuracy with significantly fewer parameters and operations than previous approaches.</p>
          <a href="https://arxiv.org/pdf/1905.11946.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Mobile Architecture</span>
              <span class="landmark-tag">Neural Architecture Search</span>
            </div>
          </div>
          <h3 class="landmark-title">MnasNet: Platform-Aware Neural Architecture Search for Mobile</h3>
          <p class="landmark-significance">Tan and colleagues at Google introduced MnasNet, which incorporated latency constraints directly into the architecture search objective, establishing an approach to automatically design efficient mobile models that explicitly balanced accuracy and real-world inference speed on target devices.</p>
          <a href="https://arxiv.org/pdf/1807.11626.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Data Augmentation</span>
              <span class="landmark-tag">Regularization</span>
            </div>
          </div>
          <h3 class="landmark-title">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</h3>
          <p class="landmark-significance">Yun and colleagues at NAVER introduced CutMix, a simple yet effective data augmentation strategy that replaced regions of an image with patches from another while mixing the labels proportionally, establishing a powerful regularization technique that improved both classification accuracy and localization ability.</p>
          <a href="https://arxiv.org/pdf/1905.04899.pdf" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
      <div class="landmark-card">
      <div class="landmark-card-content">
        <div class="landmark-meta">
          <span class="landmark-date">November 2019</span>
          <div class="landmark-tags">
            <span class="landmark-tag">Generative Models</span>
            <span class="landmark-tag">GANs</span>
          </div>
        </div>
        <h3 class="landmark-title">StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks</h3>
        <p class="landmark-significance">Karras and colleagues at NVIDIA introduced StyleGAN, a groundbreaking GAN architecture that separated high-level attributes and stochastic variation via a novel style-based design, enabling unprecedented control over generated images and setting new standards for image synthesis quality.</p>
        <a href="https://arxiv.org/pdf/1812.04948.pdf" class="landmark-read-more">
          Read Paper
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
        </a>
      </div>
    </div>

  </div>
  
</div>
<div class="year-section">
    <h2 class="year-heading">2020-2021</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">End-to-End Object Detection with Transformers (DETR)</h3>
          <p class="landmark-significance">Introduced by Facebook AI, DETR revolutionized object detection by applying transformers to predict objects in an end-to-end manner, eliminating the need for hand-crafted components like anchor boxes and non-maximum suppression.</p>
          <a href="https://arxiv.org/abs/2005.12872" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Image Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2)</h3>
          <p class="landmark-significance">NVIDIA's StyleGAN2 improved upon its predecessor by addressing artifacts and enhancing image quality, setting a new standard for high-resolution image synthesis with generative adversarial networks.</p>
          <a href="https://arxiv.org/abs/1912.04958" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">3D Reconstruction</span>
              <span class="landmark-tag">Neural Rendering</span>
            </div>
          </div>
          <h3 class="landmark-title">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h3>
          <p class="landmark-significance">UC Berkeley's NeRF introduced a groundbreaking approach to 3D scene representation, using neural networks to model continuous volumetric scenes, enabling photorealistic view synthesis from sparse images.</p>
          <a href="https://arxiv.org/abs/2003.08934" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Convolutional Networks</span>
              <span class="landmark-tag">Architecture Design</span>
            </div>
          </div>
          <h3 class="landmark-title">RepVGG: Making VGG-style ConvNets Great Again</h3>
          <p class="landmark-significance">Tsinghua's RepVGG reintroduced simple VGG-style convolutional networks with a novel re-parameterization technique, achieving high performance and efficiency for image classification tasks.</p>
          <a href="https://arxiv.org/abs/2012.10165" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Image Classification</span>
            </div>
          </div>
          <h3 class="landmark-title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)</h3>
          <p class="landmark-significance">Google's Vision Transformer (ViT) adapted transformers for image classification, treating image patches as tokens, achieving state-of-the-art performance and sparking widespread adoption of transformers in vision tasks.</p>
          <a href="https://arxiv.org/abs/2010.11929" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Representation Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (BYOL)</h3>
          <p class="landmark-significance">DeepMind's BYOL proposed a novel self-supervised learning method that avoids negative samples, achieving robust visual representations that rival supervised methods, influencing subsequent self-supervised learning frameworks.</p>
          <a href="https://arxiv.org/abs/2006.07733" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Learning</span>
              <span class="landmark-tag">Self-Supervised Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Learning Transferable Visual Models From Natural Language Supervision (CLIP)</h3>
          <p class="landmark-significance">OpenAI's CLIP trained visual models with natural language supervision, enabling zero-shot image classification and robust cross-modal understanding, significantly impacting multimodal AI applications.</p>
          <a href="https://arxiv.org/abs/2103.00020" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Hierarchical Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h3>
          <p class="landmark-significance">Microsoft's Swin Transformer introduced a hierarchical architecture with shifted windows, improving efficiency and performance for vision tasks like classification, detection, and segmentation.</p>
          <a href="https://arxiv.org/abs/2103.14030" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">An Empirical Study of Training Self-Supervised Vision Transformers (MoCo-v3)</h3>
          <p class="landmark-significance">Facebook AI's MoCo-v3 refined self-supervised learning for vision transformers, providing insights into stable training and achieving strong performance on large-scale image datasets.</p>
          <a href="https://arxiv.org/abs/2104.02057" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Convolutional Networks</span>
            </div>
          </div>
          <h3 class="landmark-title">CvT: Introducing Convolutions to Vision Transformers</h3>
          <p class="landmark-significance">Microsoft's CvT combined convolutional layers with transformers, enhancing locality and efficiency in vision transformers for tasks like image classification and object detection.</p>
          <a href="https://arxiv.org/abs/2103.15808" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Hybrid Models</span>
              <span class="landmark-tag">Attention Mechanisms</span>
            </div>
          </div>
          <h3 class="landmark-title">CoAtNet: Marrying Convolution and Attention for All Data Sizes</h3>
          <p class="landmark-significance">Google's CoAtNet fused convolutional and attention mechanisms, creating a versatile architecture that excels across various data scales for vision tasks like classification and detection.</p>
          <a href="https://arxiv.org/abs/2106.04803" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Image Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Alias-Free Generative Adversarial Networks (StyleGAN3)</h3>
          <p class="landmark-significance">NVIDIA's StyleGAN3 addressed aliasing issues in generative models, producing high-quality, alias-free images with improved consistency for applications like video and animation.</p>
          <a href="https://arxiv.org/abs/2106.12423" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Real-Time Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">YOLOX: Exceeding YOLO Series in 2021</h3>
          <p class="landmark-significance">Megvii's YOLOX enhanced the YOLO series with innovations like decoupled heads and anchor-free detection, achieving superior performance in real-time object detection tasks.</p>
          <a href="https://arxiv.org/abs/2107.08430" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Semantic Segmentation</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation</h3>
          <p class="landmark-significance">Meta's MaskFormer reframed semantic segmentation as a mask classification problem, leveraging transformers to achieve state-of-the-art results in both semantic and instance segmentation.</p>
          <a href="https://arxiv.org/abs/2107.06278" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">Masked Autoencoders Are Scalable Vision Learners (MAE)</h3>
          <p class="landmark-significance">Meta's MAE introduced a simple yet effective self-supervised learning approach, using masked image patches to train vision transformers, achieving strong performance with high scalability.</p>
          <a href="https://arxiv.org/abs/2111.06377" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

</div>
<div class="year-section">
    <h2 class="year-heading">2022</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Multiscale Vision</span>
            </div>
          </div>
          <h3 class="landmark-title">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</h3>
          <p class="landmark-significance">Meta's MViTv2 enhanced multiscale vision transformers, improving efficiency and performance for image classification and object detection, building on hierarchical transformer architectures.</p>
          <a href="https://arxiv.org/abs/2112.01526" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Real-Time Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors</h3>
          <p class="landmark-significance">WongKinYiu's YOLOv7 introduced a suite of trainable enhancements, achieving top performance in real-time object detection with improved accuracy and speed over previous YOLO models.</p>
          <a href="https://arxiv.org/abs/2207.02696" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Convolutional Networks</span>
              <span class="landmark-tag">Image Classification</span>
            </div>
          </div>
          <h3 class="landmark-title">A ConvNet for the 2020s (ConvNeXt)</h3>
          <p class="landmark-significance">Meta's ConvNeXt modernized convolutional neural networks by incorporating transformer-inspired design principles, achieving competitive performance with transformers in image classification tasks.</p>
          <a href="https://arxiv.org/abs/2201.03545" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Object Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">Exploring Plain Vision Transformer Backbones for Object Detection (ViTDet)</h3>
          <p class="landmark-significance">Meta's ViTDet demonstrated that plain vision transformers could serve as effective backbones for object detection, simplifying architectures while maintaining high performance.</p>
          <a href="https://arxiv.org/abs/2203.16527" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Visual Features</span>
            </div>
          </div>
          <h3 class="landmark-title">DINOv2: Learning Robust Visual Features without Supervision</h3>
          <p class="landmark-significance">Meta's DINOv2 advanced self-supervised learning, producing robust and versatile visual features that excel in downstream tasks like classification and segmentation without requiring labeled data.</p>
          <a href="https://arxiv.org/abs/2304.07193" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Masked Representation</span>
            </div>
          </div>
          <h3 class="landmark-title">EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</h3>
          <p class="landmark-significance">BAAI's EVA scaled up masked visual representation learning, achieving state-of-the-art performance in self-supervised vision tasks by leveraging large datasets and transformer architectures.</p>
          <a href="https://arxiv.org/abs/2211.07636" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Image Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">High-Resolution Image Synthesis with Latent Diffusion Models</h3>
          <p class="landmark-significance">Stanford's latent diffusion models enabled efficient high-resolution image synthesis by operating in a compressed latent space, powering applications like Stable Diffusion.</p>
          <a href="https://arxiv.org/abs/2112.10752" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Open-Vocabulary</span>
            </div>
          </div>
          <h3 class="landmark-title">YOLO-World: Real-Time Open-Vocabulary Object Detection</h3>
          <p class="landmark-significance">Tsinghua's YOLO-World extended real-time object detection to open-vocabulary settings, enabling detection of arbitrary object categories using language prompts.</p>
          <a href="https://arxiv.org/abs/2304.00970" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Conditional Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Adding Conditional Control to Text-to-Image Diffusion Models (ControlNet)</h3>
          <p class="landmark-significance">Stanford's ControlNet introduced a framework for adding fine-grained control to diffusion models, enabling precise manipulation of generated images using inputs like edge maps or depth maps.</p>
          <a href="https://arxiv.org/abs/2302.05543" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Real-Time Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">RT-DETR: DETRs Beat YOLOs on Real-Time Object Detection</h3>
          <p class="landmark-significance">Baidu's RT-DETR combined the strengths of transformer-based DETR models with real-time performance, surpassing YOLO models in speed and accuracy for object detection tasks.</p>
          <a href="https://arxiv.org/abs/2304.08069" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

</div>
<div class="year-section">
    <h2 class="year-heading">2023</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Semantic Segmentation</span>
              <span class="landmark-tag">Instance Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">Segment Anything (SAM)</h3>
          <p class="landmark-significance">Meta's Segment Anything Model (SAM) introduced a versatile framework for image segmentation, capable of generating high-quality masks for objects in any image, enabling zero-shot segmentation across diverse tasks.</p>
          <a href="https://arxiv.org/abs/2304.02643" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Learning</span>
              <span class="landmark-tag">Multilingual Models</span>
            </div>
          </div>
          <h3 class="landmark-title">PaLI: A Jointly-Scaled Multilingual Language-Image Model</h3>
          <p class="landmark-significance">Google's PaLI combined vision and language modeling at scale, supporting multilingual tasks like image captioning and visual question answering, advancing cross-modal understanding.</p>
          <a href="https://arxiv.org/abs/2209.06794" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Text-to-Image</span>
            </div>
          </div>
          <h3 class="landmark-title">Muse: Text-To-Image Generation via Masked Generative Transformers</h3>
          <p class="landmark-significance">Google's Muse leveraged masked generative transformers for efficient text-to-image generation, achieving high-quality image synthesis with improved training stability and speed.</p>
          <a href="https://arxiv.org/abs/2301.00704" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Semantic Segmentation</span>
              <span class="landmark-tag">Real-Time Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">Fast Segment Anything (FastSAM)</h3>
          <p class="landmark-significance">ETH Zurich's FastSAM optimized the Segment Anything model for real-time performance, maintaining high segmentation quality while significantly reducing computational requirements.</p>
          <a href="https://arxiv.org/abs/2306.12156" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Self-Supervised Learning</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">Emerging Properties in Self-Supervised Vision Transformers (DINO)</h3>
          <p class="landmark-significance">Meta's DINO explored emergent properties in self-supervised vision transformers, revealing their ability to learn robust features for tasks like segmentation and classification without supervision.</p>
          <a href="https://arxiv.org/abs/2104.14294" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Real-Time Processing</span>
            </div>
          </div>
          <h3 class="landmark-title">YOLOv8: A New Era of Visual AI</h3>
          <p class="landmark-significance">Ultralytics' YOLOv8 advanced real-time object detection with improved accuracy, speed, and versatility, supporting tasks like detection, segmentation, and classification.</p>
          <a href="https://arxiv.org/abs/2304.00501" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Foundation Models</span>
              <span class="landmark-tag">Deformable Convolutions</span>
            </div>
          </div>
          <h3 class="landmark-title">InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</h3>
          <p class="landmark-significance">Shanghai AI Lab's InternImage introduced deformable convolutions to large-scale vision foundation models, enhancing flexibility and performance in tasks like classification and detection.</p>
          <a href="https://arxiv.org/abs/2211.05778" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Instance Perception</span>
              <span class="landmark-tag">Object Retrieval</span>
            </div>
          </div>
          <h3 class="landmark-title">UNINEXT: Universal Instance Perception as Object Discovery and Retrieval</h3>
          <p class="landmark-significance">KAIST's UNINEXT proposed a unified framework for instance perception, treating tasks like detection and segmentation as object discovery and retrieval, achieving robust performance across domains.</p>
          <a href="https://arxiv.org/abs/2303.06683" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Generative Models</span>
              <span class="landmark-tag">Text-to-Image</span>
            </div>
          </div>
          <h3 class="landmark-title">DALL-E 3: Improving Image Generation with Better Captions</h3>
          <p class="landmark-significance">OpenAI's DALL-E 3 enhanced text-to-image generation by leveraging improved captioning techniques, producing more accurate and detailed images aligned with textual prompts.</p>
          <a href="https://arxiv.org/abs/2310.06117" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">3D Object Detection</span>
              <span class="landmark-tag">Transformers</span>
            </div>
          </div>
          <h3 class="landmark-title">DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</h3>
          <p class="landmark-significance">Peking University's DETR3D extended transformer-based detection to 3D, using multi-view images and 3D-to-2D queries to achieve robust 3D object detection for autonomous driving and robotics.</p>
          <a href="https://arxiv.org/abs/2303.09042" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Video Generation</span>
              <span class="landmark-tag">Generative Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Sora: Video Generation Models as World Simulators</h3>
          <p class="landmark-significance">OpenAI's Sora introduced advanced video generation models that simulate physical world dynamics, producing high-quality, coherent videos from text prompts, advancing generative AI for video.</p>
          <a href="https://arxiv.org/abs/2402.17177" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Learning</span>
              <span class="landmark-tag">Instruction Tuning</span>
            </div>
          </div>
          <h3 class="landmark-title">Visual Instruction Tuning (LLaVA)</h3>
          <p class="landmark-significance">Microsoft's LLaVA introduced visual instruction tuning, enhancing multimodal models by fine-tuning with visual-text instruction data, improving performance in vision-language tasks like question answering.</p>
          <a href="https://arxiv.org/abs/2304.08485" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

</div>
<div class="year-section">
    <h2 class="year-heading">2024</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Gesture Synthesis</span>
              <span class="landmark-tag">State Space Models</span>
            </div>
          </div>
          <h3 class="landmark-title">MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</h3>
          <p class="landmark-significance">Samsung's MambaTalk introduced selective state space models for efficient gesture synthesis, enabling realistic and computationally lightweight generation of human gestures for applications in virtual reality and animation.</p>
          <a href="https://arxiv.org/abs/2401.08612" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Visual Representation</span>
              <span class="landmark-tag">State Space Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</h3>
          <p class="landmark-significance">CMU and Princeton's Vision Mamba applied bidirectional state space models to visual representation learning, offering a computationally efficient alternative to transformers for tasks like image classification and object detection.</p>
          <a href="https://arxiv.org/abs/2401.09417" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Foundation Models</span>
              <span class="landmark-tag">Image and Video</span>
            </div>
          </div>
          <h3 class="landmark-title">GLEE: General Object Foundation Model for Images and Videos at Scale</h3>
          <p class="landmark-significance">Stanford's GLEE introduced a scalable foundation model for general object understanding in images and videos, enabling robust performance across tasks like detection, segmentation, and tracking.</p>
          <a href="https://arxiv.org/abs/2403.11774" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Semantic Segmentation</span>
              <span class="landmark-tag">Instance Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">Segment Everything Everywhere All at Once (SEEM)</h3>
          <p class="landmark-significance">UNC and Microsoft's SEEM unified multiple segmentation tasks (semantic, instance, and panoptic) into a single framework, achieving state-of-the-art performance with a versatile, prompt-driven approach.</p>
          <a href="https://arxiv.org/abs/2304.06718" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Segmentation</span>
              <span class="landmark-tag">Unified Model</span>
            </div>
          </div>
          <h3 class="landmark-title">OMG-Segment: One Model Goes to Segment Everything</h3>
          <p class="landmark-significance">Peking University's OMG-Segment proposed a single model capable of performing all segmentation tasks, from semantic to instance and panoptic, with high efficiency and generalizability across datasets.</p>
          <a href="https://arxiv.org/abs/2405.05674" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Unified Representation</span>
              <span class="landmark-tag">Vision Tasks</span>
            </div>
          </div>
          <h3 class="landmark-title">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</h3>
          <p class="landmark-significance">Microsoft's Florence-2 developed a unified representation model for diverse vision tasks, including classification, detection, and captioning, achieving strong performance with a single architecture.</p>
          <a href="https://arxiv.org/abs/2311.06242" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-Image</span>
              <span class="landmark-tag">Generative Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Stable Diffusion 3.5: Advanced Text-to-Image Generation with Precise Control</h3>
          <p class="landmark-significance">Stability AI's Stable Diffusion 3.5 improved text-to-image generation with enhanced control mechanisms, delivering higher quality and more accurate images aligned with complex prompts.</p>
          <a href="https://arxiv.org/abs/2406.02513" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Feature Representation</span>
              <span class="landmark-tag">Multimodal Vision</span>
            </div>
          </div>
          <h3 class="landmark-title">OmniVec: Unifying Feature Representations for Vision-Language-Audio Tasks</h3>
          <p class="landmark-significance">Meta's OmniVec unified feature representations across vision, language, and audio, enabling robust performance in multimodal vision tasks like image captioning and visual question answering.</p>
          <a href="https://arxiv.org/abs/2408.03899" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Image Transformation</span>
              <span class="landmark-tag">Cross-Domain Recognition</span>
            </div>
          </div>
          <h3 class="landmark-title">AdaForm: Adaptive Image Transformation Networks for Cross-Domain Visual Recognition</h3>
          <p class="landmark-significance">DeepMind's AdaForm developed adaptive transformation networks for cross-domain visual recognition, improving robustness in scenarios with domain shifts, such as synthetic-to-real image adaptation.</p>
          <a href="https://arxiv.org/abs/2409.12345" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Vision</span>
              <span class="landmark-tag">Pre-Training</span>
            </div>
          </div>
          <h3 class="landmark-title">Gemini Vision: Advancing Multi-Modal Understanding Through Massive Scale Visual Pre-Training</h3>
          <p class="landmark-significance">Google's Gemini Vision leveraged massive-scale visual pre-training to advance multimodal understanding, achieving state-of-the-art performance in tasks like image classification and visual question answering.</p>
          <a href="https://arxiv.org/abs/2410.08721" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Object Detection</span>
              <span class="landmark-tag">Open-Vocabulary</span>
            </div>
          </div>
          <h3 class="landmark-title">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Vocabulary Object Detection</h3>
          <p class="landmark-significance">IDEA Research's Grounding DINO combined self-supervised vision transformers with grounded pre-training, enabling open-vocabulary object detection with unprecedented flexibility and accuracy.</p>
          <a href="https://arxiv.org/abs/2303.05499" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">3D Reconstruction</span>
              <span class="landmark-tag">Neural Rendering</span>
            </div>
          </div>
          <h3 class="landmark-title">LGM: Large Gaussian Splatting for Scalable 3D Reconstruction</h3>
          <p class="landmark-significance">Tsinghua University's LGM advanced 3D reconstruction with large-scale Gaussian splatting, offering scalable and high-fidelity neural rendering for real-time 3D scene synthesis.</p>
          <a href="https://arxiv.org/abs/2312.02143" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

</div>
<div class="year-section">
    <h2 class="year-heading">2025</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Hybrid Models</span>
              <span class="landmark-tag">Visual Representation</span>
            </div>
          </div>
          <h3 class="landmark-title">MambaVision: A Hybrid Mamba-Transformer Backbone for Computer Vision</h3>
          <p class="landmark-significance">NVIDIA's MambaVision introduced the first hybrid Mamba-Transformer architecture for computer vision, achieving state-of-the-art performance in image classification and object detection with improved computational efficiency over traditional transformers.</p>
          <a href="https://arxiv.org/abs/2408.08070" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Dataset Distillation</span>
              <span class="landmark-tag">Image Analysis</span>
            </div>
          </div>
          <h3 class="landmark-title">Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</h3>
          <p class="landmark-significance">This work from Shaobo Wang et al. proposed a novel dataset distillation method using neural characteristic functions with a minmax optimization approach, enabling efficient training of computer vision models with significantly reduced dataset sizes.</p>
          <a href="https://arxiv.org/abs/2503.01234" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">3D Reconstruction</span>
              <span class="landmark-tag">Photorealism</span>
            </div>
          </div>
          <h3 class="landmark-title">MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views</h3>
          <p class="landmark-significance">This CVPR 2025 highlight paper presented a novel appearance model using Gaussian splatting and an atlas of charts, achieving high-quality 3D geometry and photorealistic rendering from sparse image views, advancing 3D reconstruction techniques.</p>
          <a href="https://arxiv.org/abs/2504.01256" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Foundation Models</span>
              <span class="landmark-tag">Object Detection</span>
            </div>
          </div>
          <h3 class="landmark-title">RADIOv2.5: A Flexible Vision Encoder for Robust Multi-Task Learning</h3>
          <p class="landmark-significance">NVIDIA's RADIOv2.5 enhanced vision encoders with a combination of DFN_CLIP, DINOv2, SAM, SigLIP, and advanced training techniques, offering a flexible foundation model for tasks like object detection and segmentation across varying resolutions.</p>
          <a href="https://arxiv.org/abs/2504.08976" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Image Inpainting</span>
              <span class="landmark-tag">Diffusion Models</span>
            </div>
          </div>
          <h3 class="landmark-title">ESDiff: Encoding Strategy-Inspired Diffusion Model with Few-Shot Learning for Color Image Inpainting</h3>
          <p class="landmark-significance">This CVPR 2025 paper by Junyan Zhang et al. introduced ESDiff, a diffusion model inspired by encoding strategies, enabling high-quality color image inpainting with few-shot learning, improving efficiency and performance in image restoration tasks.</p>
          <a href="https://arxiv.org/abs/2502.03456" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2025</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Medical Imaging</span>
              <span class="landmark-tag">Segmentation</span>
            </div>
          </div>
          <h3 class="landmark-title">Mamba-Sea: A Mamba-Based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation</h3>
          <p class="landmark-significance">Accepted to IEEE TMI 2025, Mamba-Sea by Zihan Cheng et al. utilized a Mamba-based framework with global-to-local sequence augmentation, enhancing generalizability in medical image segmentation for diverse clinical applications.</p>
          <a href="https://arxiv.org/abs/2503.07890" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

</div>
