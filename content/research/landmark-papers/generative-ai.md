<link rel="stylesheet" href="/assets/css/research/landmark-papers.css">

<div class="landmark-papers-container">
  <div class="landmark-header">
    <h1>Landmark Papers in Generative AI</h1>
    <p>Explore the foundational research that has shaped the field of Generative AI. This curated collection highlights the most influential papers that established key concepts, techniques, and breakthroughs in the evolution of generative models.</p>
  </div>
  <div class="attribution-notice">
    <div class="attribution-content">
      <p>Landmark Papers in Generative AI is a curated collection showcasing the foundational research that has shaped the field of generative artificial intelligence. I've carefully selected these papers to highlight the key breakthroughs and conceptual advances that have defined the evolution of generative models, providing historical context and significance for researchers and enthusiasts alike.</p>
    </div>
  </div>
  
  <div class="year-section">
    <h2 class="year-heading">1994-2010</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 1994</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Mixture Models</span>
              <span class="landmark-tag">Probabilistic</span>
            </div>
          </div>
          <h3 class="landmark-title">Density Estimation by Mixture Models</h3>
          <p class="landmark-significance">This pioneering work by Hinton and colleagues at the University of Toronto advanced density estimation using mixture models, establishing foundational techniques for probabilistic generative approaches that would influence decades of subsequent research.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 1995</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Sequence Generation</span>
              <span class="landmark-tag">RNNs</span>
            </div>
          </div>
          <h3 class="landmark-title">Neural Network Models for Unconditional Generation of Sequences</h3>
          <p class="landmark-significance">Bengio and colleagues at the University of Montreal pioneered recurrent neural networks for sequence generation, establishing core approaches that would later evolve into modern language models and other sequential generative systems.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 1996</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Unsupervised Learning</span>
              <span class="landmark-tag">Generative Model</span>
            </div>
          </div>
          <h3 class="landmark-title">The Helmholtz Machine</h3>
          <p class="landmark-significance">Hinton, Dayan, Frey, and Neal at the University of Toronto introduced a groundbreaking generative model for unsupervised learning that established fundamental concepts for modern deep generative frameworks, including the interaction between bottom-up recognition and top-down generation processes.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2001</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Face Generation</span>
              <span class="landmark-tag">Image Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Generating Faces with Neural Networks</h3>
          <p class="landmark-significance">This pioneering work by Blanz and Vetter at MIT demonstrated the early potential of neural networks for realistic face synthesis, establishing a foundation for generative image models and inspiring later approaches to controllable image generation.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2003</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Topic Modeling</span>
              <span class="landmark-tag">Text Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Latent Dirichlet Allocation</h3>
          <p class="landmark-significance">Blei, Ng, and Jordan from UC Berkeley/Stanford introduced LDA, a groundbreaking generative probabilistic model for topic modeling that revolutionized text analysis and laid important groundwork for more sophisticated text-based generative AI approaches.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2006</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Deep Belief Networks</span>
              <span class="landmark-tag">Unsupervised Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">A Fast Learning Algorithm for Deep Belief Nets</h3>
          <p class="landmark-significance">This groundbreaking paper by Hinton, Osindero, and Teh at the University of Toronto proposed deep belief networks and efficient training methods, enabling unsupervised learning for generative tasks and helping spark the deep learning revolution.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2006</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Dimensionality Reduction</span>
              <span class="landmark-tag">RBMs</span>
            </div>
          </div>
          <h3 class="landmark-title">Reducing the Dimensionality of Data with Neural Networks</h3>
          <p class="landmark-significance">Hinton and Salakhutdinov at the University of Toronto developed restricted Boltzmann machines for image data, advancing generative dimensionality reduction techniques that would influence future deep generative models.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2010</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Deep Learning</span>
              <span class="landmark-tag">Generative Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Learning Deep Boltzmann Machines</h3>
          <p class="landmark-significance">Salakhutdinov and Hinton at the University of Toronto extended Boltzmann machines to deep architectures, significantly improving generative modeling capabilities and establishing techniques that would influence future generative architectures.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2012-2014</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2012</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Deep CNNs</span>
              <span class="landmark-tag">Computer Vision</span>
            </div>
          </div>
          <h3 class="landmark-title">ImageNet Classification with Deep Convolutional Neural Networks</h3>
          <p class="landmark-significance">This revolutionary paper by Krizhevsky, Sutskever, and Hinton established deep CNNs as the dominant approach for image classification, providing the critical infrastructure that would enable image-based generative models like GANs to flourish in subsequent years.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2013</span>
            <div class="landmark-tags">
              <span class="landmark-tag">VAEs</span>
              <span class="landmark-tag">Probabilistic Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Auto-Encoding Variational Bayes</h3>
          <p class="landmark-significance">Kingma and Welling at the University of Amsterdam introduced variational autoencoders (VAEs), a cornerstone for probabilistic generative modeling that combined deep learning with variational inference to create a powerful framework for learning complex data distributions.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">GANs</span>
              <span class="landmark-tag">Adversarial Training</span>
            </div>
          </div>
          <h3 class="landmark-title">Generative Adversarial Networks</h3>
          <p class="landmark-significance">Goodfellow and colleagues at the University of Montreal proposed GANs, revolutionizing image generation through adversarial training. This groundbreaking approach, where generator and discriminator networks compete in a minimax game, created a new paradigm for generative modeling with unprecedented realism.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2014</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Neural Visualization</span>
              <span class="landmark-tag">Generative Art</span>
            </div>
          </div>
          <h3 class="landmark-title">Inceptionism: Going Deeper into Neural Networks</h3>
          <p class="landmark-significance">Mordvintsev, Olah, and Tyka at Google introduced DeepDream, showcasing novel neural network visualization techniques that revealed the generative capabilities of CNNs and launched early applications of AI-generated art, sparking public interest in creative AI.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2015-2016</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">DCGANs</span>
              <span class="landmark-tag">Stable Training</span>
            </div>
          </div>
          <h3 class="landmark-title">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</h3>
          <p class="landmark-significance">Radford, Metz, and Chintala at Facebook AI developed DCGAN, addressing GAN training instability and enabling high-quality image synthesis through architectural constraints that made GANs practical for real-world applications.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2015</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Style Transfer</span>
              <span class="landmark-tag">Artistic Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">A Neural Algorithm of Artistic Style</h3>
          <p class="landmark-significance">Gatys, Ecker, and Bethge at the University of Tübingen pioneered neural style transfer, enabling artistic image generation by separating and recombining content and style representations in neural networks, establishing a new approach to creative AI.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Pixel-level Generation</span>
              <span class="landmark-tag">Autoregressive Models</span>
            </div>
          </div>
          <h3 class="landmark-title">Pixel Recurrent Neural Networks</h3>
          <p class="landmark-significance">van den Oord, Kalchbrenner, and Kavukcuoglu at Google introduced PixelRNN for pixel-level image generation, advancing autoregressive models that treated image generation as a sequence modeling problem and achieving impressive density estimation results.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Audio Generation</span>
              <span class="landmark-tag">Speech Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">WaveNet: A Generative Model for Raw Audio</h3>
          <p class="landmark-significance">van den Oord and colleagues at DeepMind developed WaveNet, transforming audio generation with dilated causal convolutions that enabled unprecedented quality in speech and music synthesis, establishing key techniques for modeling high-dimensional sequential data.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2016</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-Image</span>
              <span class="landmark-tag">Stacked GANs</span>
            </div>
          </div>
          <h3 class="landmark-title">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</h3>
          <p class="landmark-significance">Zhang and colleagues at Rutgers advanced text-to-image synthesis using stacked GANs, enabling the creation of higher-resolution and more realistic images from textual descriptions through a multi-stage refinement process.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2017-2018</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Unpaired Translation</span>
              <span class="landmark-tag">Cycle Consistency</span>
            </div>
          </div>
          <h3 class="landmark-title">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</h3>
          <p class="landmark-significance">Zhu and colleagues at UC Berkeley introduced CycleGAN, enabling unpaired image-to-image translation through cycle consistency loss, dramatically expanding the domains where generative translation could work by eliminating the need for paired training data.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformers</span>
              <span class="landmark-tag">Attention</span>
            </div>
          </div>
          <h3 class="landmark-title">Attention is All You Need</h3>
          <p class="landmark-significance">Vaswani and colleagues at Google proposed the Transformer architecture, establishing the foundation for text and multimodal generative models through self-attention mechanisms that enabled efficient modeling of long-range dependencies in sequential data.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Paired Translation</span>
              <span class="landmark-tag">Conditional GANs</span>
            </div>
          </div>
          <h3 class="landmark-title">Image-to-Image Translation with Conditional Adversarial Networks</h3>
          <p class="landmark-significance">Isola and colleagues at UC Berkeley developed Pix2Pix, enabling paired image-to-image translation with conditional GANs, establishing a general-purpose framework for supervised image translation that could be applied to diverse domains.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2017</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Progressive Training</span>
              <span class="landmark-tag">High Resolution</span>
            </div>
          </div>
          <h3 class="landmark-title">Progressive Growing of GANs for Improved Quality, Stability, and Variation</h3>
          <p class="landmark-significance">Karras and colleagues at NVIDIA introduced Progressive GANs, tackling the challenge of high-resolution image generation through gradual network growth, significantly improving stability and enabling the creation of higher-quality images.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Large Scale</span>
              <span class="landmark-tag">High Fidelity</span>
            </div>
          </div>
          <h3 class="landmark-title">Large Scale GAN Training for High Fidelity Natural Image Synthesis</h3>
          <p class="landmark-significance">Brock, Donahue, and Simonyan at DeepMind developed BigGAN, achieving unprecedented image synthesis quality through large-scale GAN training, demonstrating the benefits of scaling model capacity and batch size for generative models.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2018</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Style-Based</span>
              <span class="landmark-tag">Controllable Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">A Style-Based Generator Architecture for GANs</h3>
          <p class="landmark-significance">Karras, Laine, and Aila at NVIDIA introduced StyleGAN, revolutionizing controllable image generation by separating high-level attributes in a latent style space, enabling fine-grained control over generated image features and establishing a foundation for numerous subsequent advances in image synthesis.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2019-2020</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Language Generation</span>
              <span class="landmark-tag">Unsupervised Learning</span>
            </div>
          </div>
          <h3 class="landmark-title">Language Models are Unsupervised Multitask Learners</h3>
          <p class="landmark-significance">Radford and colleagues at OpenAI presented GPT-2, advancing large-scale language generation with unprecedented fluency and adaptability, demonstrating how scaling transformer models could produce remarkably capable text generation systems.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Vector Quantization</span>
              <span class="landmark-tag">Hierarchical Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Generating Diverse High-Fidelity Images with VQ-VAE-2</h3>
          <p class="landmark-significance">Razavi, van den Oord, and Vinyals at DeepMind improved high-resolution image generation with vector-quantized VAEs, presenting a hierarchical approach that combined the benefits of discrete latent spaces with autoregressive modeling to produce diverse, high-quality images.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Denoising</span>
              <span class="landmark-tag">Text Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation</h3>
          <p class="landmark-significance">Lewis and colleagues at Facebook AI introduced BART, enhancing text generation through denoising pre-training, establishing a flexible approach for language generation tasks by combining bidirectional encoding with autoregressive decoding.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2019</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Diffusion Models</span>
              <span class="landmark-tag">Probabilistic Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Denoising Diffusion Probabilistic Models</h3>
          <p class="landmark-significance">Ho, Jain, and Abbeel at UC Berkeley proposed DDPM, establishing diffusion models as a powerful generative framework that would eventually surpass GANs for image synthesis through a gradual denoising process inspired by non-equilibrium thermodynamics.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Image Quality</span>
              <span class="landmark-tag">GAN Improvement</span>
            </div>
          </div>
          <h3 class="landmark-title">Analyzing and Improving the Image Quality of StyleGAN</h3>
          <p class="landmark-significance">Karras and colleagues at NVIDIA presented StyleGAN2, refining the revolutionary StyleGAN architecture to address artifacts and improve image quality through redesigned normalization, progressive growing, and regularization techniques.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Music Generation</span>
              <span class="landmark-tag">Audio Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Jukebox: A Generative Model for Music</h3>
          <p class="landmark-significance">Dhariwal and colleagues at OpenAI introduced Jukebox, enabling high-quality music generation with lyrics, vocals, and complex instrumentation through a multi-scale VQ-VAE approach combined with transformer-based autoregressive modeling.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2020</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Few-Shot Learning</span>
              <span class="landmark-tag">Scaling</span>
            </div>
          </div>
          <h3 class="landmark-title">Language Models are Few-Shot Learners</h3>
          <p class="landmark-significance">Brown and colleagues at OpenAI presented GPT-3, scaling language models to unprecedented size and demonstrating emergent few-shot learning capabilities that transformed expectations for generative AI across a diverse range of tasks.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2021</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal</span>
              <span class="landmark-tag">Vision-Language</span>
            </div>
          </div>
          <h3 class="landmark-title">Learning Transferable Visual Models From Natural Language Supervision</h3>
          <p class="landmark-significance">Radford and colleagues at OpenAI introduced CLIP, enabling text-guided image generation and establishing a foundation for multimodal models by learning powerful visual representations from natural language supervision at scale.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">High-Resolution</span>
              <span class="landmark-tag">Transformer-Based</span>
            </div>
          </div>
          <h3 class="landmark-title">Taming Transformers for High-Resolution Image Synthesis</h3>
          <p class="landmark-significance">Esser, Rombach, and Ommer at Heidelberg University developed VQ-GAN with transformers, improving high-resolution image generation by combining the efficiency of discrete representations with the modeling power of transformer architectures.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-Image</span>
              <span class="landmark-tag">Zero-Shot</span>
            </div>
          </div>
          <h3 class="landmark-title">Zero-Shot Text-to-Image Generation</h3>
          <p class="landmark-significance">Ramesh and colleagues at OpenAI introduced DALL-E, pioneering text-to-image generation with transformers and demonstrating how autoregressive models could create remarkably diverse and creative images from natural language descriptions.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-Guided Diffusion</span>
              <span class="landmark-tag">Image Editing</span>
            </div>
          </div>
          <h3 class="landmark-title">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</h3>
          <p class="landmark-significance">Nichol and colleagues at OpenAI advanced text-guided image synthesis with diffusion models, providing stronger results than GANs while maintaining more diversity and establishing a foundation for text-conditional image generation and editing.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Code Generation</span>
              <span class="landmark-tag">Programming AI</span>
            </div>
          </div>
          <h3 class="landmark-title">Evaluating Large Language Models Trained on Code</h3>
          <p class="landmark-significance">Chen and colleagues at OpenAI presented Codex, enabling sophisticated code generation by fine-tuning language models on programming languages, influencing a new generation of AI programming tools and establishing the foundation for systems like GitHub Copilot.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Diffusion vs. GANs</span>
              <span class="landmark-tag">Image Quality</span>
            </div>
          </div>
          <h3 class="landmark-title">Diffusion Models Beat GANs on Image Synthesis</h3>
          <p class="landmark-significance">Dhariwal and Nichol at OpenAI demonstrated diffusion models' superiority over GANs for image generation, providing evidence that diffusion-based approaches could deliver higher quality results with fewer artifacts and greater diversity while remaining more stable during training.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2021</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Human Feedback</span>
              <span class="landmark-tag">Alignment</span>
            </div>
          </div>
          <h3 class="landmark-title">Training Language Models to Follow Instructions with Human Feedback</h3>
          <p class="landmark-significance">Ouyang and colleagues at OpenAI introduced RLHF for generative language models, establishing methods to align model outputs with human preferences and intentions, dramatically improving helpfulness and reducing harmful generations.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2022</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">CLIP Latents</span>
              <span class="landmark-tag">Text-to-Image</span>
            </div>
          </div>
          <h3 class="landmark-title">Hierarchical Text-Conditional Image Generation with CLIP Latents</h3>
          <p class="landmark-significance">Ramesh and colleagues at OpenAI presented DALL-E 2, enhancing text-to-image generation quality through a diffusion model conditioned on CLIP image embeddings, establishing a new paradigm for high-quality, controllable image synthesis from text.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Latent Diffusion</span>
              <span class="landmark-tag">Efficiency</span>
            </div>
          </div>
          <h3 class="landmark-title">High-Resolution Image Synthesis with Latent Diffusion Models</h3>
          <p class="landmark-significance">Rombach and colleagues at Stability AI introduced Stable Diffusion, democratizing high-quality image generation by moving diffusion to a compressed latent space, dramatically reducing computational requirements while maintaining quality and enabling widespread adoption.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Photorealism</span>
              <span class="landmark-tag">Language Understanding</span>
            </div>
          </div>
          <h3 class="landmark-title">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</h3>
          <p class="landmark-significance">Saharia and colleagues at Google presented Imagen, advancing diffusion-based text-to-image synthesis through a combination of powerful text encoders and cascaded diffusion models, achieving unprecedented photorealism and text alignment.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Autoregressive</span>
              <span class="landmark-tag">Text-to-Image</span>
            </div>
          </div>
          <h3 class="landmark-title">Pathways Autoregressive Text-to-Image Model</h3>
          <p class="landmark-significance">Yu and colleagues at Google introduced Parti, scaling autoregressive models for text-to-image generation to new heights, demonstrating that sequentially predicting tokens could rival diffusion approaches for high-quality, compositionally complex image creation.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-3D</span>
              <span class="landmark-tag">2D Diffusion</span>
            </div>
          </div>
          <h3 class="landmark-title">DreamFusion: Text-to-3D using 2D Diffusion</h3>
          <p class="landmark-significance">Poole and colleagues at Google enabled text-to-3D generation using diffusion models, introducing Score Distillation Sampling to optimize 3D representations through the lens of pretrained 2D diffusion models, unlocking a new dimension for generative AI.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2022</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Instruction Tuning</span>
              <span class="landmark-tag">Human Feedback</span>
            </div>
          </div>
          <h3 class="landmark-title">Training Language Models to Follow Instructions with Human Feedback</h3>
          <p class="landmark-significance">Ouyang and colleagues at OpenAI extended RLHF techniques for generative models, underpinning ChatGPT's conversational abilities and establishing methods for aligning large language models with human values, intentions, and conversational patterns.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2023</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Conditional Control</span>
              <span class="landmark-tag">Fine-grained Guidance</span>
            </div>
          </div>
          <h3 class="landmark-title">Adding Conditional Control to Text-to-Image Diffusion Models</h3>
          <p class="landmark-significance">Zhang and colleagues at Stanford introduced ControlNet, enhancing diffusion model controllability by enabling additional conditioning inputs like edges, poses, or depth maps while preserving the original model's capabilities, dramatically expanding creative control options.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Speech Recognition</span>
              <span class="landmark-tag">Weak Supervision</span>
            </div>
          </div>
          <h3 class="landmark-title">Robust Speech Recognition via Large-Scale Weak Supervision</h3>
          <p class="landmark-significance">Radford and colleagues at OpenAI presented Whisper, advancing generative audio transcription through massive weakly-supervised training, creating a highly robust multilingual speech recognition system with near-human level performance in diverse conditions.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal</span>
              <span class="landmark-tag">Scaling</span>
            </div>
          </div>
          <h3 class="landmark-title">GPT-4 Technical Report</h3>
          <p class="landmark-significance">OpenAI introduced GPT-4, a multimodal large language model with unprecedented capabilities in reasoning, specialized domains, and visual understanding, setting new benchmarks for generative AI and demonstrating emergent capabilities at scale.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Visual Instruction</span>
              <span class="landmark-tag">Multimodal</span>
            </div>
          </div>
          <h3 class="landmark-title">Visual Instruction Tuning</h3>
          <p class="landmark-significance">Liu and colleagues at the University of Wisconsin-Madison presented LLaVA, advancing vision-language multimodal generation through instruction tuning of visual models, enabling complex visual reasoning and comprehensive understanding of images with text.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Music Generation</span>
              <span class="landmark-tag">Text-to-Music</span>
            </div>
          </div>
          <h3 class="landmark-title">MusicLM: Generating Music From Text</h3>
          <p class="landmark-significance">Agostinelli and colleagues at Google introduced MusicLM, enabling high-quality text-guided music generation that could produce coherent compositions with unprecedented control over instrumentation, genre, and mood from natural language descriptions.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">May 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Audio Generation</span>
              <span class="landmark-tag">Language Modeling</span>
            </div>
          </div>
          <h3 class="landmark-title">AudioLM: a Language Modeling Approach to Audio Generation</h3>
          <p class="landmark-significance">Borsos and colleagues at Google advanced audio generation with language modeling techniques, demonstrating how hierarchical modeling of audio tokens could generate coherent long-form audio with unprecedented naturalness and contextual consistency.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Image Captions</span>
              <span class="landmark-tag">Quality Improvement</span>
            </div>
          </div>
          <h3 class="landmark-title">Improving Image Generation with Better Captions</h3>
          <p class="landmark-significance">Crowson and colleagues at OpenAI presented DALL-E 3, dramatically improving text-to-image consistency by integrating large language models to expand and enhance prompts, solving long-standing issues with text rendering and complex scene composition.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Fast Audio</span>
              <span class="landmark-tag">Latent Diffusion</span>
            </div>
          </div>
          <h3 class="landmark-title">Stable Audio: Fast Timing-Conditioned Latent Audio Diffusion</h3>
          <p class="landmark-significance">Stability AI introduced Stable Audio, enabling fast audio generation through latent diffusion techniques, bringing the efficiency and quality advances of latent space diffusion to audio synthesis for music and sound effects creation.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">September 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">High-Resolution</span>
              <span class="landmark-tag">Image Quality</span>
            </div>
          </div>
          <h3 class="landmark-title">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</h3>
          <p class="landmark-significance">Podell and colleagues at Stability AI enhanced Stable Diffusion with SDXL, dramatically improving image quality and resolution through architectural refinements, multi-aspect training, and specialized conditioning methods for more photorealistic generation.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-Video</span>
              <span class="landmark-tag">Without Video Data</span>
            </div>
          </div>
          <h3 class="landmark-title">Make-A-Video: Text-to-Video Generation without Text-Video Data</h3>
          <p class="landmark-significance">Singer and colleagues at Meta introduced Make-A-Video, advancing text-to-video generation by leveraging pretrained text-to-image models without requiring paired text-video training data, enabling high-quality video synthesis from text descriptions.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multiworld</span>
              <span class="landmark-tag">Visual Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Generative Multiworld Models for Visual Interaction</h3>
          <p class="landmark-significance">Yan and colleagues at Meta presented Emu, enabling multimodal visual generation with unprecedented flexibility, including image-to-image transformations, multi-turn visual conversations, and complex editing capabilities in a unified framework.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">December 2023</span>
            <div class="landmark-tags">
              <span class="landmark-tag">AI Feedback</span>
              <span class="landmark-tag">Safety</span>
            </div>
          </div>
          <h3 class="landmark-title">Constitutional AI: Harmlessness from AI Feedback</h3>
          <p class="landmark-significance">Anthropic introduced Claude 2, advancing safe generative AI systems through Constitutional AI methods that used AI-generated feedback to help align language models with human values and reduce harmful outputs without human labeling.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>

  <div class="year-section">
    <h2 class="year-heading">2024</h2>
    <div class="timeline">
      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">January 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Text-to-Video</span>
              <span class="landmark-tag">World Simulation</span>
            </div>
          </div>
          <h3 class="landmark-title">Video Generation Models as World Simulators</h3>
          <p class="landmark-significance">The Sora Team at OpenAI presented Sora, enabling high-quality text-to-video generation with unprecedented temporal consistency, physical realism, and compositional understanding, establishing video models as general-purpose world simulators.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">February 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal</span>
              <span class="landmark-tag">Generative Capabilities</span>
            </div>
          </div>
          <h3 class="landmark-title">Gemini: A Family of Highly Capable Multimodal Models</h3>
          <p class="landmark-significance">The Gemini Team at Google introduced a family of multimodal models with enhanced generative capabilities across text, images, audio, and video, establishing new benchmarks for multimodal understanding and generation in diverse contexts.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">March 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Models</span>
              <span class="landmark-tag">Safety Focus</span>
            </div>
          </div>
          <h3 class="landmark-title">Claude 3 Technical Report</h3>
          <p class="landmark-significance">Anthropic presented Claude 3, advancing multimodal generative AI with a strong safety focus, showcasing improvements in reasoning, accuracy, and multimodal processing while maintaining alignment with human values through constitutional methods.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">April 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Interactive Environments</span>
              <span class="landmark-tag">Game Generation</span>
            </div>
          </div>
          <h3 class="landmark-title">Generative Interactive Environments</h3>
          <p class="landmark-significance">The Genie Team at Google introduced a framework for generating interactive 3D environments from text descriptions, enabling the creation of playable games and simulations with emergent physics, interactions, and goal-directed behavior.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">June 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">3D Video</span>
              <span class="landmark-tag">Consistency</span>
            </div>
          </div>
          <h3 class="landmark-title">Stable Video 3D: Consistent Diffusion for End-to-End View-Consistent Video Generation</h3>
          <p class="landmark-significance">Stability AI advanced 3D video generation with diffusion models, introducing methods for creating temporally coherent videos with consistent camera movements around objects, enabling novel-view synthesis and interactive 3D experiences from text prompts.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">July 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Space-Time Diffusion</span>
              <span class="landmark-tag">Video Synthesis</span>
            </div>
          </div>
          <h3 class="landmark-title">Lumiere: A Space-Time Diffusion Model for Video Generation</h3>
          <p class="landmark-significance">Bar-Tal and colleagues at Google introduced Lumiere, improving space-time diffusion for video synthesis with novel architectures that jointly model spatial and temporal dimensions, enabling high-quality video generation with complex camera movements and reliable temporal consistency.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">August 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal Generation</span>
              <span class="landmark-tag">Unified Representations</span>
            </div>
          </div>
          <h3 class="landmark-title">Emu2: Advanced Multimodal Generation through Unified Representations</h3>
          <p class="landmark-significance">Yan and colleagues at Meta AI advanced multimodal generation with unified vision-language representations, enabling seamless generation and understanding across modalities with improved coherence, consistency, and instruction-following capabilities.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">October 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Transformer-Based</span>
              <span class="landmark-tag">Text-to-Video</span>
            </div>
          </div>
          <h3 class="landmark-title">VideoPoet: A Large-Scale Multimodal Model for Video Generation</h3>
          <p class="landmark-significance">Kondratyuk and colleagues at Google introduced VideoPoet, a transformer-based model for high-quality text-to-video generation, establishing new benchmarks for long-form video synthesis with temporal coherence, complex narratives, and controllable stylistic elements.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>

      <div class="landmark-card">
        <div class="landmark-card-content">
          <div class="landmark-meta">
            <span class="landmark-date">November 2024</span>
            <div class="landmark-tags">
              <span class="landmark-tag">Multimodal</span>
              <span class="landmark-tag">Cross-Modal Understanding</span>
            </div>
          </div>
          <h3 class="landmark-title">xAI Multimodal Grok: Generative Understanding Across Modalities</h3>
          <p class="landmark-significance">The xAI Team presented Grok 3, advancing multimodal generative AI for text and image tasks through novel cross-modal training techniques and architectural innovations that improved contextual understanding and generation capabilities.</p>
          <a href="#" class="landmark-read-more">
            Read Paper
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
          </a>
        </div>
      </div>
    </div>

  </div>
</div>
