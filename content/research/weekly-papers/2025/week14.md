<link rel="stylesheet" href="/assets/css/research/weekly-papers.css">
<link rel="stylesheet" href="/assets/css/research/weekly-paper-detail.css">
<div class="weekly-paper-container">
  <div class="weekly-paper-header">
    <div class="weekly-paper-meta">
      <span class="weekly-paper-date">April 1-7, 2025</span>
      <div class="weekly-paper-tags">
        <span class="weekly-paper-tag">Agents</span>
        <span class="weekly-paper-tag">Benchmarks</span>
        <span class="weekly-paper-tag">Models</span>
        <span class="weekly-paper-tag">Research</span>
      </div>
    </div>
    <h1 class="weekly-paper-title">Week 14: Autonomous Science, Enterprise LLMs, and Medical Simulations</h1>
    <p class="weekly-paper-description">This week features groundbreaking research in AI agent benchmarking, enterprise-ready LLMs, autonomous scientific experimentation, and advances in reasoning approaches. Key papers highlight capabilities in research replication, medical reasoning, and efficient token use.</p>
  </div>
  <div class="weekly-paper-content">
    <section class="weekly-paper-section">
      <h2>Research Highlights</h2>
      <div class="paper-card">
        <div class="paper-card-header">
          <h3>PaperBench: Benchmarking AI Agents on Research Replication</h3>
          <div class="paper-card-meta">
            <span class="paper-authors">OpenAI</span>
            <a href="#" class="paper-link">Paper Link</a>
          </div>
        </div>
        <div class="paper-card-content">
          <p>OpenAI introduces PaperBench to evaluate AI agents' ability to replicate cutting-edge machine learning research papers from scratch.</p>
          <ul class="paper-points">
            <li>Tests paper reproduction across 20 ICML 2024 papers with ~8,316 fine-grained tasks</li>
            <li>Uses LLM-based judges with high human agreement (F1 = 0.83) for scalable evaluation</li>
            <li>Best model (Claude 3.5 Sonnet) achieved only 21.0% score vs. human 41.4%</li>
          </ul>
          <div class="paper-quote">
            <p>"PaperBench reveals frontier models still struggle with long-horizon research tasks, highlighting limitations in planning and execution."</p>
          </div>
        </div>
      </div>

<div class="paper-card">
    <div class="paper-card-header">
      <h3>Command A: An Enterprise-Ready LLM</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Cohere</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>Cohere's 111B parameter open-weights model features modular expert merging and hybrid architecture for enterprise applications.</p>
      <ul class="paper-points">
        <li>Interleaves sliding window and full attention for efficient 256k context support</li>
        <li>Outperforms GPT-4o and Claude 3.5 on RAG, tool use, and enterprise tasks</li>
        <li>Excels in 23 languages with 94.2% pass rate on real-world generative tasks</li>
      </ul>
      <div class="paper-quote">
        <p>"Command A's decentralized training pipeline preserves expert domain performance with only ~1.8% average drop when merged."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>CodeScientist: Autonomous Scientific Experimentation</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">AI2</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>CodeScientist automatically generates and tests scientific hypotheses via code-based experimentation with minimal human input.</p>
      <ul class="paper-points">
        <li>Five-step pipeline: Ideation â†’ Planning â†’ Code Execution â†’ Reporting â†’ Meta-Analysis</li>
        <li>Produced 6 scientifically sound and novel findings from 50 AI research papers</li>
        <li>Discovered that LLM self-confidence often mismatches actual accuracy</li>
      </ul>
      <div class="paper-quote">
        <p>"CodeScientist demonstrates AI's capacity for autonomous discovery while highlighting the value of targeted human guidance."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>RARE: Retrieval-Augmented Reasoning Model</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>RARE shifts LLM training from memorizing knowledge to applying and evaluating it, separating domain knowledge from domain thinking.</p>
      <ul class="paper-points">
        <li>Uses retrieved knowledge during training to teach reasoning patterns</li>
        <li>Small RARE models outperform GPT-4+RAG on medical benchmarks</li>
        <li>Achieves up to +20% accuracy boosts on complex medical QA tasks</li>
      </ul>
      <div class="paper-quote">
        <p>"RARE's open-book training approach enables better performance under tight parameter budgets by focusing on reasoning over memorization."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>Why do LLMs Attend to First Token?</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>This paper explains why LLMs focus attention on the first token, showing it prevents representational collapse in deep Transformers.</p>
      <ul class="paper-points">
        <li>Attention sinks act as no-ops that preserve representation diversity</li>
        <li>Larger models (LLaMA 3.1 405B) show stronger sink behavior</li>
        <li>Sinks emerge naturally due to position, not the token itself</li>
      </ul>
      <div class="paper-quote">
        <p>"Attention sinks shield against over-mixing, allowing deeper models to maintain token differentiation through many layers."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>MedAgentSim: Automated Hospital Simulation</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>MedAgentSim creates a fully automated, open-source hospital simulation where LLM-powered agents simulate doctor-patient interactions.</p>
      <ul class="paper-points">
        <li>Features multi-turn consultations with lab requests in a 2D game environment</li>
        <li>Uses memory buffers and kNN retrieval for self-improvement</li>
        <li>Improves performance by 6-37% across medical benchmarks</li>
      </ul>
      <div class="paper-quote">
        <p>"MedAgentSim's dynamic simulation environment offers a more realistic test of medical AI than static QA benchmarks."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>Open Deep Search (ODS): Open-Source Search AI</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Sentient, UW, Princeton, UC Berkeley</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>ODS provides an open-source search AI framework that rivals proprietary systems like GPT-4o Search Preview and Perplexity Sonar.</p>
      <ul class="paper-points">
        <li>Combines search tools with reasoning agents in two variants: ReAct and CodeAct</li>
        <li>Outperforms GPT-4o Search Preview by +9.7% on FRAMES benchmark</li>
        <li>Adapts search queries dynamically for optimal balance of cost and accuracy</li>
      </ul>
      <div class="paper-quote">
        <p>"ODS demonstrates that open-source search AI can match or exceed proprietary alternatives through modular design."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>Z1: Efficient Test-time Scaling with Code</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>Z1 improves LLM compute efficiency by training models with varying reasoning trajectories and dynamically adjusting depth during inference.</p>
      <ul class="paper-points">
        <li>107K-sample dataset with short and long reasoning paths for coding problems</li>
        <li>Matches larger models while using ~30% of the reasoning tokens</li>
        <li>Code reasoning generalizes well to science and math tasks</li>
      </ul>
      <div class="paper-quote">
        <p>"Z1's Shifted Thinking Window allows models to adapt reasoning depth based on problem complexity, improving efficiency."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>A Survey of Efficient Reasoning for LLMs</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>This survey analyzes how to balance deep reasoning performance with computational cost in large language models.</p>
      <ul class="paper-points">
        <li>Reviews inefficiencies and behavioral patterns in LLM reasoning</li>
        <li>Explores solutions at both post-training and inference stages</li>
        <li>Provides framework for measuring reasoning economy</li>
      </ul>
      <div class="paper-quote">
        <p>"The survey highlights the growing importance of efficient reasoning approaches as models scale."</p>
      </div>
    </div>
  </div>

  <div class="paper-card">
    <div class="paper-card-header">
      <h3>Hidden Factual Knowledge in LLMs</h3>
      <div class="paper-card-meta">
        <span class="paper-authors">Anonymous</span>
        <a href="#" class="paper-link">Paper Link</a>
      </div>
    </div>
    <div class="paper-card-content">
      <p>This study introduces a framework to measure hidden knowledge in LLMs, revealing significant gaps between internal encoding and expressed outputs.</p>
      <ul class="paper-points">
        <li>Models encode up to 40% more factual information than they express</li>
        <li>Some answers, though known internally, are never generated</li>
        <li>Highlights limitations in test-time sampling for QA tasks</li>
      </ul>
      <div class="paper-quote">
        <p>"The study reveals that LLMs know more than they tell us, pointing to untapped potential in knowledge retrieval."</p>
      </div>
    </div>
  </div>
</section>

<section class="weekly-paper-section">
  <h2>Emerging Trends</h2>
  <div class="trends-grid">
    <div class="trend-card">
      <div class="trend-icon">ðŸ”¬</div>
      <h3>AI for Scientific Research</h3>
      <p>Tools like CodeScientist and PaperBench show AI's growing role in automating research workflows, from hypothesis generation to experiment reproduction.</p>
    </div>

    <div class="trend-card">
      <div class="trend-icon">ðŸ’Š</div>
      <h3>Medical AI Advances</h3>
      <p>RARE and MedAgentSim demonstrate specialized approaches for medical tasks, emphasizing dynamic simulation and reasoning over memorization.</p>
    </div>

    <div class="trend-card">
      <div class="trend-icon">âš¡</div>
      <h3>Compute Efficiency</h3>
      <p>Z1 and attention sink research highlight innovations in making models more efficient through adaptive reasoning and architectural insights.</p>
    </div>

    <div class="trend-card">
      <div class="trend-icon">ðŸ”Ž</div>
      <h3>Open-Source Competition</h3>
      <p>Command A and Open Deep Search demonstrate open-source alternatives challenging proprietary systems in enterprise and search domains.</p>
    </div>

  </div>
</section>

<section class="weekly-paper-section">
  <h2>Industry Implications</h2>
  <p class="section-intro">This week's research offers significant implications for AI applications:</p>

  <div class="implications-list">
    <div class="implication-item">
      <h3>Enterprise AI Accessibility</h3>
      <p>Command A's open-weights approach brings enterprise-grade capabilities to organizations with customization flexibility and multilingual support.</p>
    </div>

    <div class="implication-item">
      <h3>Scientific Research Acceleration</h3>
      <p>CodeScientist demonstrates AI's potential to accelerate discovery cycles by autonomously generating and testing hypotheses.</p>
    </div>

    <div class="implication-item">
      <h3>Healthcare Application Potential</h3>
      <p>RARE and MedAgentSim showcase specialized medical AI approaches that could improve clinical decision support tools and training simulations.</p>
    </div>

    <div class="implication-item">
      <h3>Resource Optimization</h3>
      <p>Z1 and attention sink research provide pathways to more efficient inference, potentially reducing costs for AI deployment at scale.</p>
    </div>

  </div>
</section>
</div>
  <div class="weekly-paper-nav">
    <a href="week15.html" class="weekly-paper-nav-next">

    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
      Next Week

    </a>
    <a href="week13.html" class="weekly-paper-nav-prev">

      Previous Week
      <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
    </a>

  </div>
</div>
