<link rel="stylesheet" href="/assets/css/research/year-review.css">

<div class="year-review-container">
    <div class="year-review-header">
        <h1>AI Research Year in Review 2024</h1>
        <p>A look back at the most significant papers, discoveries, and trends that shaped the field of Artificial Intelligence throughout 2024. Explore the breakthroughs that defined the year, month by month.</p>
    </div>
    <div class="year-review-attribution">
        <div class="year-review-attribution-content">
            <p>
                This 2024 Year in Review of AI Research is heavily inspired by and curates key highlights from Sebastian Raschka's insightful series published on his magazine:
                <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1" target="_blank">[AI Research Papers of 2024, Part 1]</a> and
                <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2" target="_blank">[AI Research Papers of 2024, Part 2]</a>.
                 My contribution involves providing expanded technical context, compiling essential publication details, adding historical comparisons, and analyzing the relevance of each advancement by the end of 2024. This effort aims to offer a more deeply structured and contextualized overview, supplementing the original curation with additional layers of information for researchers and enthusiasts.
            </p>
        </div>
    </div>

    <div class="year-review-year-section">
        <div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">January 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">MoE</span>
                            <span class="year-review-tag">Open Models</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Mixtral's Mixture of Experts Approach</h3>
                    <p class="year-review-description">The year kicked off with Mistral AI's release of Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) model. This influential open-weight model showcased impressive performance, challenging larger dense models across benchmarks, particularly noteworthy so early in the year.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The <a href="https://arxiv.org/abs/2401.02964" target="_blank">Mixtral of Experts paper (arXiv:2401.02964)</a> detailing the architecture and performance of the Mixtral 8x7B Sparse Mixture of Experts (SMoE) model.</li>
                                <li><strong>Publication Date:</strong> January 8, 2024</li>
                                <li><strong>Authors:</strong> The Mistral AI team</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Mixture of Experts (MoE), Sparse Models, Open-weight Models, Model Architecture, Benchmarking, Efficient Inference.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2401.02964" target="_blank">arXiv:2401.02964</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2401.02964" target="_blank">Read the Mixtral 8x7B paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> Mixtral's release was pivotal for the open-source LLM community in 2024. It proved that MoE architectures could deliver state-of-the-art performance comparable to much larger dense models, offering significant advantages in inference speed and cost, thereby validating an important alternative scaling strategy.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>Mixture of Experts (MoE) is an architectural paradigm where, instead of using one large feed-forward network (FFN) in each transformer block, you have several smaller FFNs ("experts"). A 'router' or gating network determines which subset of these experts (typically 1 or 2) processes the input token at each layer. Mixtral 8x7B utilizes 8 such experts per block, but only routes tokens to 2 of them during inference. This 'sparse' activation allows the model to have a high number of total parameters (making it highly capable) while requiring significantly less computation per token during inference compared to a dense model of similar parameter count, making it faster and cheaper to run.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Conceptual diagram showing a transformer block where the standard Feed-Forward Network (FFN) layer is replaced by an MoE layer (Router + multiple Experts). An arrow shows a token coming in, being processed by the router, and then sent to a selected subset of experts. (Reference figures like the one from the original "Attention Is All You Need" paper adapted to show the MoE layer replacement, or include a simple custom diagram with credit).</p>
                            <pre><code class="language-python">

# Simplified PyTorch-like pseudocode illustrating MoE block idea

class MoEBlock(nn.Module):
def **init**(self, embed*dim, num_experts, k):
super().**init**()
self.experts = nn.ModuleList([FeedForward(embed_dim) for * in range(num_experts)])
self.gate = nn.Linear(embed_dim, num_experts) # Gating network (router)
self.k = k # Number of experts to activate per token

    def forward(self, x):
        # x shape: (batch_size, sequence_length, embed_dim)
        batch_size, seq_len, embed_dim = x.shape
        x_reshaped = x.view(-1, embed_dim) # Flatten for gate

        gate_logits = self.gate(x_reshaped) # Get logits for each expert
        weights, selected_experts = torch.topk(gate_logits, self.k, dim=-1) # Select top k experts
        weights = torch.softmax(weights, dim=-1) # Normalize weights

        output = torch.zeros_like(x_reshaped)

        # Route tokens to selected experts and combine outputs
        # This is a simplified loop; actual implementations are highly optimized
        for i in range(batch_size * seq_len):
            token_output = torch.zeros(embed_dim, device=x.device)
            for j in range(self.k):
                expert_idx = selected_experts[i, j]
                expert_weight = weights[i, j]
                token_output += expert_weight * self.experts[expert_idx](x_reshaped[i].unsqueeze(0)).squeeze(0)
            output[i] = token_output

        return output.view(batch_size, seq_len, embed_dim) # Reshape back
                            </code></pre>
                            <h4>Historical Context/Comparison</h4>
                            <p>The Mixture of Experts concept has been around for decades, but its application in large-scale, high-performing transformer-based language models was primarily confined to proprietary models (like Google's early MoE models or Switch Transformer) before 2024. Mixtral 8x7B was arguably the first *openly available* MoE model to achieve performance competitive with or surpassing state-of-the-art dense models of the time (like Llama 2 70B and GPT-3.5) across a variety of benchmarks. Its release democratized access to experimenting with and understanding this architecture at scale, contrasting sharply with the dense-model focus of most prior open releases like Llama 1/2.</p>
                            <h4>Relevance at End of Year</h4>
                            <p>By December 2024, the MoE architecture remained a highly relevant topic, although dense models like Llama 3, Gemma 2, and Qwen 2.5 still dominated the very top of the open-source benchmarks for pure capability on some tasks. However, MoE solidified its niche as a powerful approach for achieving high capacity *and* high efficiency. The release of DeepSeek-V3 late in the year, which also used an MoE architecture and showed strong performance, further validated its continued importance. While not every leading model adopted MoE, it became an essential part of the toolkit for balancing model size, performance, and computational cost, particularly for inference.</p>
                        </div>
                    </details>
                </div>
            </div>

            </div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">February 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">PEFT</span>
                            <span class="year-review-tag">LoRA</span>
                            <span class="year-review-tag">DoRA</span>
                            <span class="year-review-tag">Finetuning</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Weight-decomposed LoRA (DoRA)</h3>
                    <p class="year-review-description">If you are finetuning open-weight LLMs, chances are high that you have been using low-rank adaptation (LoRA). This month's highlight is DoRA: Weight-Decomposed Low-Rank Adaptation (February 2024) by Liu and colleagues, a novel variant that builds upon the popular LoRA method for parameter-efficient LLM finetuning.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The paper <a href="https://arxiv.org/abs/2402.09353" target="_blank">"DoRA: Weight-Decomposed Low-Rank Adaptation" (arXiv:2402.09353)</a>, introducing Weight-Decomposed Low-Rank Adaptation (DoRA) as an improvement over standard LoRA for parameter-efficient finetuning.</li>
                                <li><strong>Publication Date:</strong> February 2024</li>
                                <li><strong>Authors:</strong> Liu and colleagues (including Qing Liu, Yuzhao Gu, Zhi Chen, Xu Han, Yi Hu, Maosong Sun, Zhiyuan Liu)</li>
                                <li><strong>Tags/Topics:</strong> Parameter-Efficient Finetuning (PEFT), Low-Rank Adaptation (LoRA), DoRA, Large Language Model (LLM) Finetuning, Model Adaptation, Deep Learning Optimization.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2402.09353" target="_blank">arXiv:2402.09353</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2402.09353" target="_blank">Read the DoRA paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> DoRA enhances LoRA by incorporating weight decomposition, leading to improved finetuning performance and robustness, sometimes even with fewer parameters. It offers a valuable, simple upgrade to one of the most widely used PEFT techniques.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p><strong>LoRA Recap:</strong> Full finetuning of large weight matrices (W) in an LLM involves computing a large update matrix (Delta W). LoRA drastically reduces this by approximating Delta W as the product of two much smaller matrices, A and B (so, W + Delta W becomes W + A.B). This significantly cuts down on computational and memory requirements.</p>
                            <p><strong>From LoRA to DoRA:</strong> DoRA extends LoRA by first decomposing a pretrained weight matrix into two components: a magnitude vector (m) and a directional matrix (V). Conceptually, this is like representing each column of the weight matrix by its length and direction. DoRA then applies the LoRA-style low-rank updates (A.B) *only* to the directional matrix (V), while the magnitude vector (m) is updated separately. This decomposition and selective updating provide DoRA with greater flexibility. Standard LoRA tends to scale magnitude and direction together, whereas DoRA can adjust direction more subtly without necessarily altering magnitude, leading to performance gains and improved robustness, especially at lower ranks.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Reference the illustration comparing regular finetuning and LoRA side-by-side. Also, reference the annotated illustration from the DoRA paper showing the weight decomposition into magnitude (m) and directional (V) components and how LoRA updates are applied to V.</p>
                            <pre><code class="language-python">

# Simplified PyTorch-like pseudocode illustrating MoE block idea

class MoEBlock(nn.Module):
def **init**(self, embed*dim, num_experts, k):
super().**init**()
self.experts = nn.ModuleList([FeedForward(embed_dim) for * in range(num_experts)])
self.gate = nn.Linear(embed_dim, num_experts) # Gating network (router)
self.k = k # Number of experts to activate per token

    def forward(self, x):
        # x shape: (batch_size, sequence_length, embed_dim)
        batch_size, seq_len, embed_dim = x.shape
        x_reshaped = x.view(-1, embed_dim) # Flatten for gate

        gate_logits = self.gate(x_reshaped) # Get logits for each expert
        weights, selected_experts = torch.topk(gate_logits, self.k, dim=-1) # Select top k experts
        weights = torch.softmax(weights, dim=-1) # Normalize weights

        output = torch.zeros_like(x_reshaped)

        # Route tokens to selected experts and combine outputs
        # This is a simplified loop; actual implementations are highly optimized
        for i in range(batch_size * seq_len):
            token_output = torch.zeros(embed_dim, device=x.device)
            for j in range(self.k):
                expert_idx = selected_experts[i, j]
                expert_weight = weights[i, j]
                token_output += expert_weight * self.experts[expert_idx](x_reshaped[i].unsqueeze(0)).squeeze(0)
            output[i] = token_output

        return output.view(batch_size, seq_len, embed_dim) # Reshape back
                            </code></pre>
                            <h4>Historical Context/Comparison</h4>
                            <p>DoRA is presented as a direct and logical extension of the highly popular and widely adopted LoRA method for parameter-efficient finetuning. While LoRA gained prominence for its efficiency, DoRA builds upon this by addressing how magnitude and direction updates are handled, offering a simple modification to potentially yield better results without significant added complexity to the LoRA framework itself.</p>
                            <h4>Relevance at End of Year</h4>
                            <p>By the end of 2024, while standard LoRA remained incredibly popular, DoRA was recognized as a promising improvement that adds minimal overhead. Its adoption wasn't universal, but it became a key consideration for researchers and practitioners looking to push the performance of parameter-efficient finetuning further. The continued relevance of LoRA-like methods was underscored by major players like Apple mentioning their use of LoRA for on-device model specialization, ensuring that research into LoRA variants like DoRA remains important for efficient AI deployment.</p>
                        </div>
                    </details>
                </div>
            </div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">March 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">Continual Pretraining</span>
                            <span class="year-review-tag">LLM Training</span>
                            <span class="year-review-tag">Optimization</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Simple Tips for Continually Pretraining LLMs</h3>
                    <p class="year-review-description">While instruction-finetuning is common, continually pretraining LLMs is essential for incorporating new knowledge. This section summarizes key findings from the paper "Simple and Scalable Strategies to Continually Pre-train Large Language Models" (March 2024) by Ibrahim and colleagues.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The paper <a href="https://arxiv.org/abs/2403.05928" target="_blank">"Simple and Scalable Strategies to Continually Pre-train Large Language Models" (arXiv:2403.05928)</a>, focusing on effective and straightforward techniques for continued pretraining of LLMs on new data.</li>
                                <li><strong>Publication Date:</strong> March 2024</li>
                                <li><strong>Authors:</strong> Ahmed Ibrahim and colleagues (including Amr Hendy, Muhammad Sharaf, Chien-Feng Liao, Muhammad Abdul-Mageed, Ganesh Kumar)</li>
                                <li><strong>Tags/Topics:</strong> Continual Learning, Continued Pretraining, Large Language Models (LLMs), LLM Training, Optimization, Learning Rate Schedules, Catastrophic Forgetting, Data Mixing.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2403.05928" target="_blank">arXiv:2403.05928</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2403.05928" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> The paper validates that surprisingly simple techniques, like appropriate learning rate scheduling and mixing a small fraction of original pretraining data, are highly effective for successful continued pretraining without significant forgetting, providing practical guidance for practitioners.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The paper explores simple yet effective strategies for continually pretraining LLMs on new data distributions. The two main takeaways validated through extensive experiments are:</p>
                            <ol>
                                <li><strong>Learning Rate Re-warming and Re-decaying:</strong> Using the exact same learning rate schedule (with warming up and decaying) that was used during the LLM's initial pretraining phase proves effective when starting continued pretraining on the new dataset. This helps stabilize training on the new data.</li>
                                <li><strong>Adding Original Pretraining Data:</strong> Including a small portion (even as low as 0.5% or 1%) of the original pretraining dataset alongside the new data is crucial for preventing catastrophic forgetting of the knowledge the model learned during its initial training. The paper found that mixing around 5% of the original data works well.</li>
                            </ol>
                            <p>While these strategies might seem intuitively correct or be considered "common knowledge" among some researchers, the paper's value lies in its rigorous empirical validation across numerous experiments, providing concrete evidence and detailed analysis in its 24 pages.</p>

                            <h4>Figures or Diagrams</h4>
                            <p>A key visual from the paper (or related work) illustrates the learning rate schedule used. This figure shows the initial warm-up phase followed by the decay, and how this same schedule is reapplied for continued pretraining.</p>
                            <div class="figure-placeholder" style="text-align: center; margin: 20px 0; padding: 20px; border: 1px dashed var(--border-light); background-color: var(--primary-lighter); border-radius: var(--radius-sm);">
                                <p style="color: var(--text-medium); font-style: italic; margin-bottom: 10px;">[Insert figure here]</p>
                                <p style="font-size: 14px; color: var(--text-light); margin-bottom: 10px;">You can embed the figure using an &lt;img&gt; tag within this div.</p>
                                <p style="font-size: 14px; color: var(--text-light); margin: 0;">Figure based on Build a Large Language Model From Scratch, <a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb" target="_blank">https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb</a></p>
                            </div>
                            <p>The core idea is to replicate the successful initial pretraining learning rate trajectory when continuing training on new data.</p>


                            <h4>Historical Context/Comparison</h4>
                            <p>Continued pretraining itself is a known method to update a model's knowledge base. This paper's significance in 2024 wasn't in proposing entirely novel techniques, but in providing a thorough, large-scale empirical study that confirms the effectiveness of simple, accessible strategies. At a time when LLM training seemed increasingly complex, this work reassured practitioners that foundational optimization and data management principles remain powerful tools for this specific task.</p>

                            <h4>Relevance at End of Year</h4>
                            <p>The principles outlined in this paper, particularly regarding learning rates and data mixing for continued pretraining, remained relevant throughout 2024. As pretraining pipelines evolved to include multiple stages (like short- and long-context phases), practitioners understood that while the exact application might require tweaking for specific pipelines, the core ideas of managing learning rate and combating forgetting via data replay are fundamental to successful continued pretraining in various complex settings.</p>
                        </div>
                    </details>
                </div>
            </div>
    </div>

    </div>
