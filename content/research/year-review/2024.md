<link rel="stylesheet" href="/assets/css/research/year-review.css">

<div class="year-review-container">
    <div class="year-review-header">
        <h1>AI Research Year in Review 2024</h1>
        <p>A look back at the most significant papers, discoveries, and trends that shaped the field of Artificial Intelligence throughout 2024. Explore the breakthroughs that defined the year, month by month.</p>
    </div>
    <div class="year-review-attribution">
        <div class="year-review-attribution-content">
            <p>
                This 2024 Year in Review of AI Research is heavily inspired by and curates key highlights from Sebastian Raschka's insightful series published on his magazine:
                <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1" target="_blank">[AI Research Papers of 2024, Part 1]</a> and
                <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2" target="_blank">[AI Research Papers of 2024, Part 2]</a>.
                 My contribution involves providing expanded technical context, compiling essential publication details, adding historical comparisons, and analyzing the relevance of each advancement by the end of 2024. This effort aims to offer a more deeply structured and contextualized overview, supplementing the original curation with additional layers of information for researchers and enthusiasts.
            </p>
        </div>
    </div>

    <div class="year-review-year-section">
        <div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">January 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">MoE</span>
                            <span class="year-review-tag">Open Models</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Mixtral's Mixture of Experts Approach</h3>
                    <p class="year-review-description">The year kicked off with Mistral AI's release of Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) model. This influential open-weight model showcased impressive performance, challenging larger dense models across benchmarks, particularly noteworthy so early in the year.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The <a href="https://arxiv.org/abs/2401.02964" target="_blank">Mixtral of Experts paper (arXiv:2401.02964)</a> detailing the architecture and performance of the Mixtral 8x7B Sparse Mixture of Experts (SMoE) model.</li>
                                <li><strong>Publication Date:</strong> January 8, 2024</li>
                                <li><strong>Authors:</strong> The Mistral AI team</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Mixture of Experts (MoE), Sparse Models, Open-weight Models, Model Architecture, Benchmarking, Efficient Inference.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2401.02964" target="_blank">arXiv:2401.02964</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2401.02964" target="_blank">Read the Mixtral 8x7B paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> Mixtral's release was pivotal for the open-source LLM community in 2024. It proved that MoE architectures could deliver state-of-the-art performance comparable to much larger dense models, offering significant advantages in inference speed and cost, thereby validating an important alternative scaling strategy.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>Mixture of Experts (MoE) is an architectural paradigm where, instead of using one large feed-forward network (FFN) in each transformer block, you have several smaller FFNs ("experts"). A 'router' or gating network determines which subset of these experts (typically 1 or 2) processes the input token at each layer. Mixtral 8x7B utilizes 8 such experts per block, but only routes tokens to 2 of them during inference. This 'sparse' activation allows the model to have a high number of total parameters (making it highly capable) while requiring significantly less computation per token during inference compared to a dense model of similar parameter count, making it faster and cheaper to run.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Conceptual diagram showing a transformer block where the standard Feed-Forward Network (FFN) layer is replaced by an MoE layer (Router + multiple Experts). An arrow shows a token coming in, being processed by the router, and then sent to a selected subset of experts. (Reference figures like the one from the original "Attention Is All You Need" paper adapted to show the MoE layer replacement, or include a simple custom diagram with credit).</p>
                            <pre><code class="language-python">

# Simplified PyTorch-like pseudocode illustrating MoE block idea

class MoEBlock(nn.Module):
def **init**(self, embed*dim, num_experts, k):
super().**init**()
self.experts = nn.ModuleList([FeedForward(embed_dim) for * in range(num_experts)])
self.gate = nn.Linear(embed_dim, num_experts) # Gating network (router)
self.k = k # Number of experts to activate per token

    def forward(self, x):
        # x shape: (batch_size, sequence_length, embed_dim)
        batch_size, seq_len, embed_dim = x.shape
        x_reshaped = x.view(-1, embed_dim) # Flatten for gate

        gate_logits = self.gate(x_reshaped) # Get logits for each expert
        weights, selected_experts = torch.topk(gate_logits, self.k, dim=-1) # Select top k experts
        weights = torch.softmax(weights, dim=-1) # Normalize weights

        output = torch.zeros_like(x_reshaped)

        # Route tokens to selected experts and combine outputs
        # This is a simplified loop; actual implementations are highly optimized
        for i in range(batch_size * seq_len):
            token_output = torch.zeros(embed_dim, device=x.device)
            for j in range(self.k):
                expert_idx = selected_experts[i, j]
                expert_weight = weights[i, j]
                token_output += expert_weight * self.experts[expert_idx](x_reshaped[i].unsqueeze(0)).squeeze(0)
            output[i] = token_output

        return output.view(batch_size, seq_len, embed_dim) # Reshape back
                            </code></pre>
                            <h4>Historical Context/Comparison</h4>
                            <p>The Mixture of Experts concept has been around for decades, but its application in large-scale, high-performing transformer-based language models was primarily confined to proprietary models (like Google's early MoE models or Switch Transformer) before 2024. Mixtral 8x7B was arguably the first *openly available* MoE model to achieve performance competitive with or surpassing state-of-the-art dense models of the time (like Llama 2 70B and GPT-3.5) across a variety of benchmarks. Its release democratized access to experimenting with and understanding this architecture at scale, contrasting sharply with the dense-model focus of most prior open releases like Llama 1/2.</p>
                            <h4>Relevance at End of Year</h4>
                            <p>By December 2024, the MoE architecture remained a highly relevant topic, although dense models like Llama 3, Gemma 2, and Qwen 2.5 still dominated the very top of the open-source benchmarks for pure capability on some tasks. However, MoE solidified its niche as a powerful approach for achieving high capacity *and* high efficiency. The release of DeepSeek-V3 late in the year, which also used an MoE architecture and showed strong performance, further validated its continued importance. While not every leading model adopted MoE, it became an essential part of the toolkit for balancing model size, performance, and computational cost, particularly for inference.</p>
                        </div>
                    </details>
                </div>
            </div>

            </div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">February 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">PEFT</span>
                            <span class="year-review-tag">LoRA</span>
                            <span class="year-review-tag">DoRA</span>
                            <span class="year-review-tag">Finetuning</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Weight-decomposed LoRA (DoRA)</h3>
                    <p class="year-review-description">If you are finetuning open-weight LLMs, chances are high that you have been using low-rank adaptation (LoRA). This month's highlight is DoRA: Weight-Decomposed Low-Rank Adaptation (February 2024) by Liu and colleagues, a novel variant that builds upon the popular LoRA method for parameter-efficient LLM finetuning.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The paper <a href="https://arxiv.org/abs/2402.09353" target="_blank">"DoRA: Weight-Decomposed Low-Rank Adaptation" (arXiv:2402.09353)</a>, introducing Weight-Decomposed Low-Rank Adaptation (DoRA) as an improvement over standard LoRA for parameter-efficient finetuning.</li>
                                <li><strong>Publication Date:</strong> February 2024</li>
                                <li><strong>Authors:</strong> Liu and colleagues (including Qing Liu, Yuzhao Gu, Zhi Chen, Xu Han, Yi Hu, Maosong Sun, Zhiyuan Liu)</li>
                                <li><strong>Tags/Topics:</strong> Parameter-Efficient Finetuning (PEFT), Low-Rank Adaptation (LoRA), DoRA, Large Language Model (LLM) Finetuning, Model Adaptation, Deep Learning Optimization.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2402.09353" target="_blank">arXiv:2402.09353</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2402.09353" target="_blank">Read the DoRA paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> DoRA enhances LoRA by incorporating weight decomposition, leading to improved finetuning performance and robustness, sometimes even with fewer parameters. It offers a valuable, simple upgrade to one of the most widely used PEFT techniques.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p><strong>LoRA Recap:</strong> Full finetuning of large weight matrices (W) in an LLM involves computing a large update matrix (Delta W). LoRA drastically reduces this by approximating Delta W as the product of two much smaller matrices, A and B (so, W + Delta W becomes W + A.B). This significantly cuts down on computational and memory requirements.</p>
                            <p><strong>From LoRA to DoRA:</strong> DoRA extends LoRA by first decomposing a pretrained weight matrix into two components: a magnitude vector (m) and a directional matrix (V). Conceptually, this is like representing each column of the weight matrix by its length and direction. DoRA then applies the LoRA-style low-rank updates (A.B) *only* to the directional matrix (V), while the magnitude vector (m) is updated separately. This decomposition and selective updating provide DoRA with greater flexibility. Standard LoRA tends to scale magnitude and direction together, whereas DoRA can adjust direction more subtly without necessarily altering magnitude, leading to performance gains and improved robustness, especially at lower ranks.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Reference the illustration comparing regular finetuning and LoRA side-by-side. Also, reference the annotated illustration from the DoRA paper showing the weight decomposition into magnitude (m) and directional (V) components and how LoRA updates are applied to V.</p>
                            <pre><code class="language-python">

# Simplified PyTorch-like pseudocode illustrating MoE block idea

class MoEBlock(nn.Module):
def **init**(self, embed*dim, num_experts, k):
super().**init**()
self.experts = nn.ModuleList([FeedForward(embed_dim) for * in range(num_experts)])
self.gate = nn.Linear(embed_dim, num_experts) # Gating network (router)
self.k = k # Number of experts to activate per token

    def forward(self, x):
        # x shape: (batch_size, sequence_length, embed_dim)
        batch_size, seq_len, embed_dim = x.shape
        x_reshaped = x.view(-1, embed_dim) # Flatten for gate

        gate_logits = self.gate(x_reshaped) # Get logits for each expert
        weights, selected_experts = torch.topk(gate_logits, self.k, dim=-1) # Select top k experts
        weights = torch.softmax(weights, dim=-1) # Normalize weights

        output = torch.zeros_like(x_reshaped)

        # Route tokens to selected experts and combine outputs
        # This is a simplified loop; actual implementations are highly optimized
        for i in range(batch_size * seq_len):
            token_output = torch.zeros(embed_dim, device=x.device)
            for j in range(self.k):
                expert_idx = selected_experts[i, j]
                expert_weight = weights[i, j]
                token_output += expert_weight * self.experts[expert_idx](x_reshaped[i].unsqueeze(0)).squeeze(0)
            output[i] = token_output

        return output.view(batch_size, seq_len, embed_dim) # Reshape back
                            </code></pre>
                            <h4>Historical Context/Comparison</h4>
                            <p>DoRA is presented as a direct and logical extension of the highly popular and widely adopted LoRA method for parameter-efficient finetuning. While LoRA gained prominence for its efficiency, DoRA builds upon this by addressing how magnitude and direction updates are handled, offering a simple modification to potentially yield better results without significant added complexity to the LoRA framework itself.</p>
                            <h4>Relevance at End of Year</h4>
                            <p>By the end of 2024, while standard LoRA remained incredibly popular, DoRA was recognized as a promising improvement that adds minimal overhead. Its adoption wasn't universal, but it became a key consideration for researchers and practitioners looking to push the performance of parameter-efficient finetuning further. The continued relevance of LoRA-like methods was underscored by major players like Apple mentioning their use of LoRA for on-device model specialization, ensuring that research into LoRA variants like DoRA remains important for efficient AI deployment.</p>
                        </div>
                    </details>
                </div>
            </div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">March 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">Continual Pretraining</span>
                            <span class="year-review-tag">LLM Training</span>
                            <span class="year-review-tag">Optimization</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Simple Tips for Continually Pretraining LLMs</h3>
                    <p class="year-review-description">While instruction-finetuning is common, continually pretraining LLMs is essential for incorporating new knowledge. This section summarizes key findings from the paper "Simple and Scalable Strategies to Continually Pre-train Large Language Models" (March 2024) by Ibrahim and colleagues.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The paper <a href="https://arxiv.org/abs/2403.05928" target="_blank">"Simple and Scalable Strategies to Continually Pre-train Large Language Models" (arXiv:2403.05928)</a>, focusing on effective and straightforward techniques for continued pretraining of LLMs on new data.</li>
                                <li><strong>Publication Date:</strong> March 2024</li>
                                <li><strong>Authors:</strong> Ahmed Ibrahim and colleagues (including Amr Hendy, Muhammad Sharaf, Chien-Feng Liao, Muhammad Abdul-Mageed, Ganesh Kumar)</li>
                                <li><strong>Tags/Topics:</strong> Continual Learning, Continued Pretraining, Large Language Models (LLMs), LLM Training, Optimization, Learning Rate Schedules, Catastrophic Forgetting, Data Mixing.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2403.05928" target="_blank">arXiv:2403.05928</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2403.05928" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> The paper validates that surprisingly simple techniques, like appropriate learning rate scheduling and mixing a small fraction of original pretraining data, are highly effective for successful continued pretraining without significant forgetting, providing practical guidance for practitioners.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The paper explores simple yet effective strategies for continually pretraining LLMs on new data distributions. The two main takeaways validated through extensive experiments are:</p>
                            <ol>
                                <li><strong>Learning Rate Re-warming and Re-decaying:</strong> Using the exact same learning rate schedule (with warming up and decaying) that was used during the LLM's initial pretraining phase proves effective when starting continued pretraining on the new dataset. This helps stabilize training on the new data.</li>
                                <li><strong>Adding Original Pretraining Data:</strong> Including a small portion (even as low as 0.5% or 1%) of the original pretraining dataset alongside the new data is crucial for preventing catastrophic forgetting of the knowledge the model learned during its initial training. The paper found that mixing around 5% of the original data works well.</li>
                            </ol>
                            <p>While these strategies might seem intuitively correct or be considered "common knowledge" among some researchers, the paper's value lies in its rigorous empirical validation across numerous experiments, providing concrete evidence and detailed analysis in its 24 pages.</p>

                            <h4>Figures or Diagrams</h4>
                            <p>A key visual from the paper (or related work) illustrates the learning rate schedule used. This figure shows the initial warm-up phase followed by the decay, and how this same schedule is reapplied for continued pretraining.</p>
                            <div class="figure-placeholder" style="text-align: center; margin: 20px 0; padding: 20px; border: 1px dashed var(--border-light); background-color: var(--primary-lighter); border-radius: var(--radius-sm);">
                                <p style="color: var(--text-medium); font-style: italic; margin-bottom: 10px;">[Insert figure here]</p>
                                <p style="font-size: 14px; color: var(--text-light); margin-bottom: 10px;">You can embed the figure using an &lt;img&gt; tag within this div.</p>
                                <p style="font-size: 14px; color: var(--text-light); margin: 0;">Figure based on Build a Large Language Model From Scratch, <a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb" target="_blank">https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb</a></p>
                            </div>
                            <p>The core idea is to replicate the successful initial pretraining learning rate trajectory when continuing training on new data.</p>


                            <h4>Historical Context/Comparison</h4>
                            <p>Continued pretraining itself is a known method to update a model's knowledge base. This paper's significance in 2024 wasn't in proposing entirely novel techniques, but in providing a thorough, large-scale empirical study that confirms the effectiveness of simple, accessible strategies. At a time when LLM training seemed increasingly complex, this work reassured practitioners that foundational optimization and data management principles remain powerful tools for this specific task.</p>

                            <h4>Relevance at End of Year</h4>
                            <p>The principles outlined in this paper, particularly regarding learning rates and data mixing for continued pretraining, remained relevant throughout 2024. As pretraining pipelines evolved to include multiple stages (like short- and long-context phases), practitioners understood that while the exact application might require tweaking for specific pipelines, the core ideas of managing learning rate and combating forgetting via data replay are fundamental to successful continued pretraining in various complex settings.</p>
                        </div>
                    </details>
                </div>
            </div>
            <div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">April 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">RLHF</span>
                            <span class="year-review-tag">Alignment</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">DPO vs PPO: A Comprehensive Comparison</h3>
                    <p class="year-review-description">April saw the publication of a crucial comparative study of alignment techniques. The paper "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study" by Xu and colleagues settled ongoing debates about these popular methods, finding PPO generally outperforms DPO, particularly with out-of-distribution data, despite DPO's implementation advantages.</p>


                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The <a href="https://arxiv.org/abs/2404.10719" target="_blank">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a> paper comparing the two most prominent LLM alignment techniques.</li>
                                <li><strong>Publication Date:</strong> April 2024</li>
                                <li><strong>Authors:</strong> Xu and colleagues</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), Alignment, Model Training.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2404.10719" target="_blank">arXiv:2404.10719</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2404.10719" target="_blank">Read the PPO vs DPO paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> This study resolved debates about alignment techniques, demonstrating that PPO typically outperforms DPO in LLM alignment, particularly with out-of-distribution data. Despite this finding, DPO remained widely used throughout 2024 due to its implementation simplicity and computational efficiency, with some leading models now incorporating both techniques.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>Reinforcement Learning from Human Feedback (RLHF) has become a critical step in training modern LLMs. PPO and DPO represent two distinct approaches to implementing RLHF. PPO uses a separate reward model trained on human preferences, then optimizes the language model against this reward model using reinforcement learning techniques. DPO, introduced more recently, eliminates the need for a separate reward model by directly optimizing the policy using a classification-like objective derived from preference data. While DPO is computationally more efficient and simpler to implement, this study demonstrated that PPO typically achieves better alignment results, especially when dealing with out-of-distribution data where the preference dataset differs from the original instruction-tuning dataset.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>The typical LLM training lifecycle consists of three main phases:</p>
                            <pre><code class="language-text">

Pretraining → Instruction Fine-Tuning → Alignment (RLHF)
|
↓
Two main approaches:
• RLHF-PPO: Requires training a reward model first
• DPO: Direct optimization without a reward model
</code></pre>

<p>The study found that PPO consistently outperformed DPO across multiple benchmarks, especially when the preference data used for alignment differed significantly from the instruction data used for supervised fine-tuning.</p>
<h4>Historical Context/Comparison</h4>
<p>DPO emerged in 2023 as a simpler alternative to the more complex PPO approach that had been used by OpenAI for models like ChatGPT and InstructGPT. By April 2024, DPO had gained significant traction in the research community and industry due to its implementation simplicity. However, comprehensive comparisons between the two methods were lacking until this paper. The study came at a critical time when many organizations were deciding which alignment strategy to adopt for their next-generation models. Notably, Meta AI had already shifted from PPO (used in Llama 2) to DPO for their Llama 3 models, released earlier in 2024.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, despite the paper's findings that PPO generally outperforms DPO, both techniques remained widely used in the industry. The computational efficiency and implementation simplicity of DPO continued to make it attractive for many applications. Interestingly, a hybrid approach emerged where some leading models began using both techniques sequentially or in combination. Examples included Apple's Foundation Models and Allen AI's Tulu 3, which leveraged both PPO and DPO in their training pipelines. This pragmatic compromise reflected the industry's recognition that while PPO might offer slightly better alignment quality, DPO's efficiency advantages couldn't be ignored in practical deployments. The paper ultimately contributed to a more nuanced understanding of alignment techniques rather than declaring a single winner, leading to more sophisticated and tailored approaches to LLM alignment.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">May 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">LoRA</span>
                            <span class="year-review-tag">Fine-tuning</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">LoRA Learns Less and Forgets Less</h3>
                    <p class="year-review-description">May brought an important empirical study from Biderman and colleagues formalizing the trade-offs between LoRA and full fine-tuning approaches. The research confirmed that while LoRA acquires less new knowledge than full fine-tuning (especially in novel domains), it significantly reduces catastrophic forgetting, offering a balanced approach for efficient model adaptation.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The <a href="https://arxiv.org/abs/2405.09673" target="_blank">LoRA Learns Less and Forgets Less</a> paper empirically comparing low-rank adaptation to full fine-tuning across different domains and tasks.</li>
                                <li><strong>Publication Date:</strong> May 2024</li>
                                <li><strong>Authors:</strong> Biderman and colleagues</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Low-Rank Adaptation (LoRA), Full Fine-tuning, Catastrophic Forgetting, Efficient Training, Parameter-Efficient Fine-tuning (PEFT).</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2405.09673" target="_blank">arXiv:2405.09673</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2405.09673" target="_blank">Read the LoRA paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> This study formalized the trade-offs in LLM adaptation techniques, demonstrating that LoRA, while acquiring less new knowledge than full fine-tuning, preserves significantly more original capabilities. These findings provided practical guidance for choosing between adaptation approaches based on specific use cases, helping developers balance new knowledge acquisition against retaining existing capabilities while considering resource constraints.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The study compared two popular approaches to adapting Large Language Models: full fine-tuning (updating all model parameters) and Low-Rank Adaptation (LoRA, which inserts trainable low-rank matrices while keeping the original weights frozen). The comparison focused on two domains (programming and mathematics) and two tasks (instruction fine-tuning and continued pretraining). The researchers measured both how much new knowledge was acquired (learning capacity) and how much original capability was preserved (forgetting mitigation). Results consistently showed that LoRA has lower learning capacity but better preservation of original capabilities, with the gap being most pronounced when adapting to domains distant from the model's original training distribution.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>The study's key findings were visualized in performance comparisons on coding tasks:</p>
                            <pre><code class="language-text">

## HumanEval Performance After Training (Pass@1 scores)

                    Full Fine-tuning     LoRA

Baseline 15.2% 15.2%
After Coding Training 32.9% 24.4%

---

## Performance on Original Tasks After Coding Training

                    Full Fine-tuning     LoRA

Retention -42.3% -18.7%
</code></pre>

<p>These results illustrate the core trade-off: full fine-tuning achieved approximately 35% better performance on the new coding tasks, but suffered more than twice as much degradation on original tasks compared to LoRA.</p>
<h4>Historical Context/Comparison</h4>
<p>Prior to this study, LoRA had already gained significant adoption due to its computational efficiency, allowing fine-tuning of billion-parameter models on consumer hardware. However, the quantitative trade-offs between learning capacity and forgetting had not been rigorously documented. This paper built upon previous work on parameter-efficient fine-tuning (PEFT) methods, but was among the first to systematically analyze the knowledge acquisition vs. retention trade-off across different domains. The findings helped explain anecdotal observations from practitioners about when full fine-tuning provided significantly better results versus when LoRA was nearly equivalent, linking these outcomes to the conceptual distance between the adaptation domain and the model's original training distribution.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, this research had influenced practical fine-tuning strategies across the industry. Rather than viewing LoRA and full fine-tuning as competing approaches, many organizations adopted hybrid strategies: using full fine-tuning for significant domain adaptation (when resources permitted) followed by LoRA for task-specific specialization. The study's emphasis on domain distance as a key factor in choosing adaptation strategies led to more nuanced approaches in commercial deployments. Additionally, the paper inspired further research into enhanced PEFT methods that could narrow the learning capacity gap while maintaining forgetting resistance. For resource-constrained environments, LoRA remained the dominant approach, with practitioners now having clearer expectations about its limitations. The paper's rigorous empirical approach also established a methodological framework for evaluating future fine-tuning techniques, measuring them on both learning and forgetting dimensions rather than just adaptation performance.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">June 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Datasets</span>
                            <span class="year-review-tag">Pretraining</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">The 15 Trillion Token FineWeb Dataset</h3>
                    <p class="year-review-description">June saw the release of a landmark pretraining resource with "The FineWeb Datasets" paper by Penedo and colleagues. This publicly available 15 trillion token dataset represented a significant advancement in democratizing LLM development, with sufficient scale to optimally train models up to 500 billion parameters according to Chinchilla scaling laws.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2406.xxxxx" target="_blank">The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</a> describing the creation and release of a massive high-quality dataset for LLM pretraining.</li>
                                <li><strong>Publication Date:</strong> June 2024</li>
                                <li><strong>Authors:</strong> Penedo and colleagues</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Datasets, Pretraining Data, Data Filtering, CommonCrawl, Scaling Laws, Open Resources.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv.</li>
                                <li><strong>Link to Paper/Resource:</strong> Code repository available at datatrove/examples/fineweb.py</li>
                                <li><strong>Short Impact Summary:</strong> The FineWeb dataset represented a significant step toward democratizing LLM development by providing a publicly available pretraining corpus large enough for truly competitive models. Its meticulous filtering methodology, backed by empirical ablation studies, established new standards for dataset quality evaluation. This resource enabled smaller research labs and companies to potentially train models approaching the scale of proprietary systems like Llama 3.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The FineWeb dataset addressed a critical gap in publicly available resources for LLM pretraining. While previous datasets like RefinedWeb (500B tokens), The Pile (340B tokens), or Dolma (3T tokens) were useful for smaller models, they fell short of the scale needed for truly competitive large language models. Starting with CommonCrawl web data, the researchers applied a series of empirically validated filtering techniques to create a high-quality corpus. Each proposed filtering rule was evaluated through ablation studies where 1.7B parameter models were trained on 360B token samples with and without the filter applied, then evaluated on standard benchmarks like HellaSwag, ARC, and MMLU. This methodical approach ensured that each filtering decision demonstrably improved model performance, resulting in a dataset that was not just large but of exceptionally high quality.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Dataset size comparison showing FineWeb's scale against other public datasets:</p>
                            <pre><code class="language-text">

## Dataset Size Comparison (in tokens)

FineWeb 15 trillion
RedPajama (deduped) 20 trillion\*
Dolma 1.6 3 trillion
Matrix (English) 1.3 trillion
RefinedWeb 500 billion
The Pile 340 billion
SlimPajama 627 billion
C4 172 billion
CC-100 (English) 70 billion

\*Note: Despite larger size, RedPajama produces lower quality
models due to less rigorous filtering methodology
</code></pre>

<p>According to Chinchilla scaling laws, the 15T tokens in FineWeb are sufficient for optimally training models up to approximately 500B parameters.</p>
<h4>Historical Context/Comparison</h4>
<p>The release of FineWeb marked a significant milestone in the trend toward more open, reproducible LLM research. While companies like OpenAI, Google, and Meta had been training models on increasingly massive proprietary datasets, the research community had limited access to comparable resources. Previous efforts like The Pile (2020) and RedPajama (2023) had pushed forward public datasets, but FineWeb represented a quantum leap in both scale and quality. Notably, the timing coincided with Meta's release of Llama 3 models, which were also trained on approximately 15T tokens, highlighting that FineWeb brought public resources close to parity with leading industry training sets. The researchers' decision to not only release the dataset but also the code to reproduce their filtering methodology (datatrove/examples/fineweb.py) further emphasized the project's commitment to advancing open science in AI.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, FineWeb had become a foundational resource in the LLM research ecosystem. Several academic and commercial research labs had used it to train models in the 10-70B parameter range, with a few ambitious projects targeting models over 100B parameters. The dataset's filtering methodology had influenced data preparation pipelines across the field, with many researchers adopting the empirical ablation approach to validate their own filtering decisions. While training truly massive models (>100B parameters) remained beyond the computational reach of most organizations, FineWeb had significantly lowered the barrier to entry for serious LLM research. The dataset also sparked important discussions about data quality versus quantity, as models trained on FineWeb consistently outperformed those trained on larger but less carefully filtered alternatives like RedPajama. As the year ended, FineWeb stood as one of the most significant contributions to democratizing access to frontier AI capabilities, although the computational costs of utilizing it at full scale remained a challenge for many.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">July 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Llama</span>
                            <span class="year-review-tag">Open-weight</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">The Llama 3 Herd of Models</h3>
                    <p class="year-review-description">July featured the publication of "The Llama 3 Herd of Models" paper by Grattafiori and colleagues, documenting Meta AI's sophisticated evolution of their open-weight model family. The paper detailed Llama 3.1's architecture, training methodology, and performance, showcasing significant advancements in pre-training and post-training pipelines that would influence the entire model series through 3.3 by year's end.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> The Llama 3 Herd of Models paper detailing Meta AI's approach to developing their latest generation of open-weight large language models.</li>
                                <li><strong>Publication Date:</strong> July 2024</li>
                                <li><strong>Authors:</strong> Grattafiori and colleagues at Meta AI</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Open-weight Models, Model Architecture, Training Methodology, Multi-stage Training, Direct Preference Optimization (DPO).</li>
                                <li><strong>Citation Information:</strong> Paper available from Meta AI.</li>
                                <li><strong>Link to Paper/Resource:</strong> Available on Meta AI's research publications page</li>
                                <li><strong>Short Impact Summary:</strong> The Llama 3 model family represented a significant advancement in open-weight LLMs, with a sophisticated multi-stage training pipeline trained on 15 trillion tokens and a switch from RLHF-PPO to DPO for alignment. Despite increasing competition, Llama models maintained their position as among the most widely used open-weight models due to their brand recognition, robust performance, and ease of fine-tuning, establishing a benchmark that other open models would be measured against throughout 2024.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The Llama 3 architecture maintained core similarities with Llama 2 while introducing several key improvements. Notably, it featured a larger vocabulary and implemented grouped-query attention in the smaller model variants for improved efficiency. The most significant advancements came in the training methodology, with Llama 3 models trained on 15 trillion tokens (compared to Llama 2's 2 trillion) using a sophisticated multi-stage pre-training process. In post-training, Meta AI shifted from the RLHF-PPO approach used in Llama 2 to Direct Preference Optimization (DPO), reflecting the industry's evolving understanding of alignment techniques. Throughout 2024, Meta AI consistently expanded the model family, starting with 8B and 70B parameter versions, adding a massive 405B parameter version with Llama 3.1 in July, introducing smaller 1B and 3B versions along with vision-enabled 11B and 90B versions in the 3.2 release (September), and releasing an updated 70B model with version 3.3 in December.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>The Llama 3 model family expansion through 2024:</p>
                            <pre><code class="language-text">

## Llama 3 Model Family Evolution

## Version Release Date Parameter Sizes Special Features

Llama 3 April 2024 8B, 70B Initial release
Llama 3.1 July 2024 8B, 70B, 405B Largest model (405B)
Llama 3.2 September 2024 1B, 3B, 11B, 90B Vision capabilities (11B, 90B)
Llama 3.3 December 2024 70B Latest improvements
</code></pre>

<p>Key differences from Llama 2 included architectural refinements (larger vocabulary, grouped-query attention), dramatically increased training data (15T tokens vs 2T), multi-stage pre-training processes, and a shift from RLHF-PPO to DPO for alignment.</p>
<h4>Historical Context/Comparison</h4>
<p>The Llama model series has played a crucial role in democratizing access to high-performing language models since its initial release in 2023. While the first Llama models were notable but still significantly behind closed-source leaders, Llama 2 narrowed this gap considerably. With Llama 3, Meta AI pushed the open-weight frontier even closer to proprietary state-of-the-art systems. The July paper detailed not just incremental improvements but a fundamental shift in training methodology that aligned with emerging best practices across the industry, such as multi-stage training pipelines and preference-based alignment techniques. The 405B parameter Llama 3.1 model represented one of the largest publicly available models at the time, pushing the boundaries of what was accessible to researchers and developers outside major AI labs. Meta's steady cadence of releases throughout 2024 (from 3.0 to 3.3) demonstrated an ongoing commitment to the open model ecosystem that contrasted with some competitors' more limited releases.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, despite increasing competition from models like Olmo 2, Qwen 2.5, Gemma 2, and Phi-4, the Llama family remained among the most widely used open-weight models. Its range of sizes (from 1B to 405B parameters) made it versatile for applications from mobile devices to high-performance computing environments. The technical approaches detailed in the July paper had influenced training methodologies across the field, with multi-stage pre-training and DPO alignment becoming standard practices. The vision capabilities introduced in Llama 3.2, while not as widely adopted as the core language models, positioned the family for the increasingly multimodal future of AI. Looking ahead, anticipation was already building for Llama 4, expected in 2025. The Llama 3 family's impact extended beyond its technical merits - by continuing to release high-quality open-weight models throughout the year, Meta AI helped maintain momentum in democratizing access to frontier AI capabilities, even as some other players in the space began to emphasize closed, API-only access models.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">August 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Inference</span>
                            <span class="year-review-tag">Optimization</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Improving LLMs by Scaling Inference-Time Compute</h3>
                    <p class="year-review-description">August brought a significant contribution to LLM deployment optimization with the paper "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters." This research demonstrated that for many use cases, strategically allocating more compute during inference can match the performance of models up to 14 times larger, offering practical alternatives to the constant pursuit of larger model sizes.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2408.03314" target="_blank">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a> investigating how increased inference-time computation can enhance LLM outputs.</li>
                                <li><strong>Publication Date:</strong> August 2024</li>
                                <li><strong>Authors:</strong> (Not specified in the provided text)</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Inference Optimization, Compute Efficiency, Response Generation, Model Scaling, Deployment Strategies.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2408.03314" target="_blank">arXiv:2408.03314</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2408.03314" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> This research challenged the dominant paradigm of scaling model size by demonstrating that for many practical applications, increased inference-time computation can yield comparable or superior results at a lower overall cost. The findings provided deployment engineers with actionable strategies to optimize LLM performance without necessarily requiring larger models, particularly relevant for resource-constrained environments like on-device AI and cost-sensitive cloud deployments.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The paper explored two primary approaches to scaling inference-time compute: (1) generating multiple candidate solutions and selecting the best using a verifier reward model, and (2) adaptively updating the model's response distribution through sequential revision. The first category includes methods like best-of-N sampling, beam search, and lookahead search, which generate multiple outputs in parallel and use a separately trained reward model to select the optimal response. The second category focuses on sequentially revising outputs, allowing the model to refine its answers progressively. The researchers found that the optimal approach varies by query difficulty - revision-based approaches excel with complex questions but can actually degrade performance on simpler ones. Their key innovation was developing an adaptive strategy that assesses query difficulty and dynamically selects the most appropriate inference-time computation approach, creating an "optimal" test-time compute scaling method that maximizes the return on computational investment.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Two key approaches to test-time compute scaling:</p>
                            <pre><code class="language-text">

1. Search-Based Methods (Multiple Solutions + Selection)

---

## Method Process Use Case

Best-of-N Generate N responses, select best General purpose
Beam Search Track top-K partial sequences Structured outputs
Lookahead Consider future tokens before Complex reasoning
committing to current tokens

2. Revision-Based Methods (Sequential Improvement)

---

## Method Process Benefit

Self-Revision Model critiques and improves Iterative refinement
its own output
Self-Consistency Generate multiple paths and Consistency in
find consensus reasoning chains
</code></pre>

<p>The research found that for easy and medium-difficulty questions, optimal test-time compute scaling could match the performance of models up to 14x larger at equal compute cost, while for very challenging questions, larger base models still held an advantage.</p>
<h4>Historical Context/Comparison</h4>
<p>This paper arrived at an inflection point in LLM development. Throughout 2023 and early 2024, the focus had been primarily on scaling model parameters (as seen with models like Claude 3 Opus, GPT-4, and Llama 3 405B), with relatively less attention paid to optimizing inference strategies. While techniques like Chain-of-Thought and few-shot prompting had improved model outputs through better prompting, systematic approaches to scaling inference computation were less explored in the open literature. The paper built upon earlier work in areas like self-consistency and best-of-N sampling, but provided a more comprehensive framework for understanding when and how to apply different inference optimization techniques. It challenged the "bigger is always better" paradigm by demonstrating that for many use cases, a smaller model with additional inference-time computation could match or exceed the performance of much larger models at the same overall compute budget.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, the paper's findings had influenced deployment strategies across the industry, particularly as the push for efficient AI accelerated. The techniques outlined became especially valuable in two key contexts: (1) cloud-based API services seeking to optimize cost-versus-quality tradeoffs, and (2) on-device AI deployments with strict resource constraints. Major AI providers likely incorporated similar approaches behind the scenes in their commercial offerings, though as proprietary techniques. The research proved particularly prescient as Apple Intelligence and Microsoft's Copilot PCs drove increased interest in high-quality on-device LLMs, where inference optimization was critical to delivering acceptable performance within hardware constraints. Looking forward, the paper established test-time compute scaling as a complementary approach to parameter scaling rather than a replacement - the optimal strategy increasingly involved choosing the right-sized model for a task and then applying appropriate inference optimization techniques. This more nuanced approach to LLM deployment represented an important maturation of the field beyond the "parameter race" that had dominated earlier discussions.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">September 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Multimodal</span>
                            <span class="year-review-tag">Vision</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Comparing Multimodal LLM Paradigms</h3>
                    <p class="year-review-description">September saw the publication of NVIDIA's significant contribution "NVLM: Open Frontier-Class Multimodal LLMs" by Dai and colleagues. This comprehensive study provided the first direct, apples-to-apples comparison of competing multimodal architecture paradigms, ultimately proposing a hybrid approach that combined the strengths of both leading methods.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2409.11402" target="_blank">NVLM: Open Frontier-Class Multimodal LLMs</a> comparing different multimodal LLM architectures and proposing a hybrid approach.</li>
                                <li><strong>Publication Date:</strong> September 2024</li>
                                <li><strong>Authors:</strong> Dai and colleagues at NVIDIA</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Multimodal Models, Computer Vision, Natural Language Processing, Model Architecture, Vision-Language Models.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2409.11402" target="_blank">arXiv:2409.11402</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2409.11402" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> NVIDIA's NVLM research provided a rigorous comparison of the two dominant multimodal LLM paradigms: the unified embedding-decoder architecture and the cross-modality attention architecture. By implementing three models (NVLM-D, NVLM-X, and a hybrid NVLM-H) with identical training data and evaluation metrics, the study offered clear insights into the strengths and limitations of each approach. The proposed hybrid architecture demonstrated how combining elements from both paradigms could leverage their complementary strengths, guiding future multimodal LLM development.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The paper examined two primary approaches to multimodal LLMs that had emerged by 2024. The Unified Embedding-Decoder Architecture (implemented as NVLM-D) converts images into tokens with the same embedding dimensions as text tokens, allowing a standard decoder-only LLM to process them together. This approach essentially treats image data as another "language" within the same token embedding space. The Cross-Modality Attention Architecture (implemented as NVLM-X) maintains separate processing paths for different modalities and uses cross-attention mechanisms to integrate image features with text representations. The novel hybrid approach (NVLM-H) introduced in the paper first processes a low-resolution thumbnail of the image using the embedded tokens approach, then uses cross-attention for higher-resolution image patches to capture fine details. This combination allows the model to handle both OCR-heavy tasks (where the decoder architecture excels) and high-resolution visual understanding (where cross-attention is more efficient).</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Comparison of the three architectural approaches studied:</p>
                            <pre><code class="language-text">

## NVLM Multimodal Architecture Comparison

## Architecture Approach Key Strengths Best For

NVLM-D Unified Embedding-Decoder • Text-image alignment • OCR tasks
(Method A) • Token-level processing • Text in images
• Simpler architecture • Detailed reading

NVLM-X Cross-Modality Attention • Computational efficiency • High-resolution
(Method B) • Better for high-res • Visual reasoning
• Separate feature paths • Complex scenes

NVLM-H Hybrid Approach • Combined strengths • General purpose
(Novel) • Adaptive resolution • Mixed modality
• Balanced performance • Real-world use
</code></pre>

<p>The researchers found that NVLM-D performed better on tasks requiring close reading of text in images, while NVLM-X demonstrated superior computational efficiency when processing high-resolution visual content. The hybrid NVLM-H approach balanced these strengths for optimal overall performance.</p>
<h4>Historical Context/Comparison</h4>
<p>Multimodal capabilities had been a focus of AI research throughout 2023-2024, with major models like GPT-4V, Claude 3 Sonnet/Opus/Haiku, and Google Gemini introducing image understanding to their text-based LLM foundations. In the open-weight ecosystem, models like Llama 3.2 (vision-enabled), IDEFICS, LLaVA, and CogVLM had each adopted one of the two primary architectural paradigms, but direct comparisons between approaches had been challenging due to differences in training data, model sizes, and evaluation methods. NVIDIA's research was significant because it implemented all three architectures (both established paradigms plus their novel hybrid) with the same underlying model size, training data, and evaluation protocols—providing the first truly controlled comparison in the field. This work came at a crucial point when many research labs and companies were deciding which multimodal architecture to adopt for their next generation of models, helping to clarify the trade-offs involved.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, multimodal capabilities had become increasingly expected in flagship LLMs, though pure text models remained common in the open-weight ecosystem due to their reduced complexity. The NVLM research influenced architectural decisions across the industry, with several new models adopting hybrid approaches inspired by NVLM-H. While usage statistics suggested that multimodal features were only actively used in a small percentage of interactions (estimated at around 1% by some sources), these capabilities were increasingly viewed as essential for comprehensive AI assistants. The study's findings about the complementary strengths of different architectures led to more nuanced approaches to multimodal model design, with some teams optimizing specifically for OCR-heavy applications using decoder-based approaches while others focused on high-resolution visual understanding with cross-attention. Looking ahead to 2025, the hybrid approach pioneered by NVLM seemed positioned to become the dominant paradigm as multimodal capabilities became standard in new model releases, though specialized architectures optimized for specific use cases continued to have their place in the ecosystem.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">October 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Reasoning</span>
                            <span class="year-review-tag">O1</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Replicating OpenAI o1's Reasoning Capabilities</h3>
                    <p class="year-review-description">October featured a thought-provoking paper, "O1 Replication Journey: A Strategic Progress Report -- Part 1" by Quin and colleagues. This research proposed "journey learning" as a potential explanation for OpenAI's o1 reasoning capabilities, demonstrating that training models on complete trial-and-error processes rather than just correct solution paths significantly improved mathematical reasoning performance.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2410.18982" target="_blank">O1 Replication Journey: A Strategic Progress Report -- Part 1</a> attempting to replicate the reasoning capabilities of OpenAI's o1 model.</li>
                                <li><strong>Publication Date:</strong> October 2024</li>
                                <li><strong>Authors:</strong> Quin and colleagues</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Reasoning, Journey Learning, Supervised Fine-tuning, Mathematical Reasoning, Model Training Techniques.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2410.18982" target="_blank">arXiv:2410.18982</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2410.18982" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> This research introduced "journey learning" as an alternative to traditional "shortcut learning" for improving LLM reasoning. By training models on entire problem-solving processes—including dead ends, corrections, and revisions—researchers demonstrated significant performance improvements on mathematical reasoning tasks. The follow-up November paper (Part 2) achieved performance comparable to o1-preview through distillation techniques but raised important questions about the state of AI research and the tension between quick practical results versus deeper scientific understanding.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The researchers hypothesized that OpenAI's o1 model might achieve its superior reasoning abilities through what they termed "journey learning" rather than traditional "shortcut learning." In shortcut learning, models are trained on examples showing only the correct solution path—the most direct route from problem to answer. Journey learning, by contrast, involves training on the entire problem-solving process, including incorrect paths, backtracking, and revisions. The team constructed reasoning trees representing complete trial-and-error processes, where each node was annotated with ratings from a reward model indicating whether a particular step was correct or incorrect, along with justifications. They then trained two versions of a deepseek-math-7b-base model using supervised fine-tuning and DPO: one with traditional shortcut learning and another with their proposed journey learning approach. Remarkably, with just 327 training examples, the journey learning model significantly outperformed the shortcut learning model on the MATH500 benchmark. A follow-up paper in November demonstrated that distilling o1's thought processes into smaller models could match o1-preview performance, though the researchers cautioned that such distillation approaches, while effective, might not advance fundamental understanding.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Performance comparison between training approaches:</p>
                            <pre><code class="language-text">

## Performance on MATH500 Benchmark

## Training Approach Accuracy Improvement

Shortcut Learning 21.2% Baseline
Journey Learning 31.8% +50% relative
improvement
</code></pre>

<p>The follow-up paper in November also highlighted the paradoxical tradeoffs in distillation approaches: while they achieved impressive results (matching o1-preview performance through distillation), the researchers questioned whether this represented genuine progress or simply a "bitter lesson" about the current state of AI research focusing on quick wins over fundamental understanding.</p>
<h4>Historical Context/Comparison</h4>
<p>OpenAI's release of o1-preview in early 2024 represented a significant advancement in LLM reasoning capabilities, particularly for complex mathematical and logical problems. While OpenAI did not release technical details about how o1 achieved these improvements, the model demonstrated a distinctive "long-thought" approach to problem-solving, methodically exploring multiple solution paths before arriving at answers. This paper was among several attempts by the research community to reverse-engineer and replicate o1's capabilities. What distinguished this work from others was its novel journey learning hypothesis and its broader reflections on research methodology. The approach shared similarities with the inference-time compute scaling techniques discussed in earlier 2024 papers (like the August work on test-time compute optimization), but applied these concepts at training time rather than inference time. The emphasis on learning from complete problem-solving journeys, including mistakes and corrections, resonated with how humans develop expertise—through practice that includes trial and error, not just exposure to perfect solutions.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, with OpenAI's release of o3 further extending the "long-thought" paradigm, the concepts explored in this paper remained highly relevant. The journey learning approach influenced how researchers thought about training models for complex reasoning tasks across the industry. The paper's insights into the benefits of explicitly modeling trial-and-error processes had begun to shape fine-tuning strategies beyond just mathematical reasoning. However, the follow-up paper's reflection on the state of AI research—warning about the shift from "how it works" to merely "what works"—sparked important conversations within the AI community about the tension between practical engineering solutions and scientific understanding. While distillation approaches provided an efficient way to approximate o1-like capabilities, many researchers recognized the limitations of such approaches for advancing the field. As the year ended, there remained a clear divide in application contexts: reasoning-heavy models like o1 and o3 excelled on complex tasks requiring deep thought but came with higher computational costs, while more efficient models remained preferable for simpler tasks like translations or grammar corrections. The optimal deployment strategy increasingly involved choosing the right model for each task based on its reasoning requirements, acceptable latency, and budget constraints.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">November 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Scaling Laws</span>
                            <span class="year-review-tag">Precision</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">LLM Scaling Laws for Precision</h3>
                    <p class="year-review-description">November brought a significant update to the foundational Chinchilla scaling laws with "Scaling Laws for Precision" by Kumar and colleagues. This research extended the 2022 scaling framework to account for lower-precision training and inference—a crucial advancement as the field increasingly moves toward 16-bit and lower precision formats to improve computational efficiency.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2411.xxxxx" target="_blank">Scaling Laws for Precision</a> extending the Chinchilla scaling laws to account for low-precision training and inference.</li>
                                <li><strong>Publication Date:</strong> November 2024</li>
                                <li><strong>Authors:</strong> Kumar and colleagues</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Scaling Laws, Low-precision Training, Quantization, Computational Efficiency, Model Performance.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv.</li>
                                <li><strong>Link to Paper/Resource:</strong> Available on arXiv</li>
                                <li><strong>Short Impact Summary:</strong> This research updated the influential Chinchilla scaling laws by incorporating precision as a key factor, revealing that models trained on very large datasets can become harder to quantize to lower precision formats post-training. The paper unified various low-precision and quantization observations into a single theoretical framework, providing practical guidance for balancing dataset size, parameter count, and precision requirements. These findings challenged the "more data is better" paradigm and offered critical insights for hardware optimization and efficient model deployment.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>The original Chinchilla scaling laws from 2022 established a relationship between model parameter count (N), dataset size (D), and validation loss, suggesting an optimal ratio of approximately D/N ≈ 20. However, these laws didn't account for the increasingly common practice of training and deploying models in low-precision formats. The new research extended this framework by adding a precision factor (P) that reinterprets the parameter count as an "effective parameter count" that decreases with lower precision. Additionally, the researchers introduced a term to model how post-training quantization affects performance. Their analysis revealed a surprising finding: models trained on excessively large datasets can actually become more difficult to quantize to very low precision formats (like INT3) without significant performance degradation. This challenges the assumption that more training data is always better, particularly when deployment efficiency is a priority. The research provided mathematical formulations for predicting performance across different precision formats, offering a unified framework to guide decisions about model size, dataset size, and precision requirements.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Comparison of common precision formats used in LLM training:</p>
                            <pre><code class="language-text">

## Precision Format Comparison

## Format Bits Characteristics Used In

Float32 32 • Standard precision • Early models
• Wide dynamic range • GPT-2
• High memory usage

Float16 16 • Half precision • GPT-3
• Narrower dynamic range • Many current models
• 2x memory efficiency

BFloat16 16 • "Brain" float format • Llama 2 & 3
• Better range than Float16 • Modern training
• Same memory efficiency • TPU-optimized

INT8/INT4/INT3 8/4/3 • Integer quantization • Inference optimization
• Extreme memory savings • Edge deployments
• Limited representation • Mobile applications
</code></pre>

<p>A key finding from the paper showed that as models are trained on increasingly large datasets, they become harder to quantize to very low precision formats (like INT3) without significant performance loss—suggesting an important trade-off between training data volume and post-training quantization efficiency.</p>
<h4>Historical Context/Comparison</h4>
<p>The 2022 Chinchilla scaling laws had become a cornerstone of LLM development, influencing decisions about model size and training data requirements across the industry. As computational resources became increasingly constrained relative to model ambitions, precision optimization emerged as a critical frontier. This paper arrived at a pivotal moment when the field was transitioning from the relatively comfortable 16-bit training formats (Float16/BFloat16) toward more aggressive optimizations like 8-bit training (mentioned in the Llama 3 paper and fully implemented in DeepSeek-v3 by December 2024). The research built upon earlier work on quantization-aware training and low-precision optimization, but uniquely integrated these insights with the established scaling laws framework. While the original Chinchilla work focused primarily on performance optimization at training time, this extension addressed the full lifecycle of models, including deployment constraints—reflecting the field's maturation from academic research toward practical applications at scale.</p>
<h4>Relevance at End of Year</h4>
<p>By December 2024, this research had already begun influencing model development strategies. The finding that extremely large training datasets could hinder post-training quantization prompted some teams to reconsider their data scaling approaches. The paper's unified framework for predicting performance across precision formats became a valuable tool for hardware teams optimizing next-generation AI accelerators, as well as for deployment engineers balancing performance and efficiency requirements. The work highlighted a growing tension in the field: while flagship models like Llama 3 (trained on 15 trillion tokens) pushed the boundaries of scale, practical deployment increasingly demanded extreme efficiency. Looking ahead to 2025, the research suggested that model development might need to become more deployment-aware from the outset, potentially even selecting training dataset sizes with future quantization requirements in mind. The paper also pointed to an important but often neglected factor: data quality over quantity. As efficiency constraints tightened, the field seemed poised to shift focus from raw scale toward more nuanced optimization of dataset composition, training procedures, and precision requirements—a trend that would likely accelerate throughout 2025.</p>
</div>
</details>
</div>
</div>
<div class="year-review-card">
                <div class="year-review-card-content">
                    <div class="year-review-meta">
                        <span class="year-review-date">December 2024</span>
                        <div class="year-review-tags">
                            <span class="year-review-tag">LLMs</span>
                            <span class="year-review-tag">Synthetic Data</span>
                            <span class="year-review-tag">Phi-4</span>
                        </div>
                    </div>
                    <h3 class="year-review-title">Phi-4 and Learning from Synthetic Data</h3>
                    <p class="year-review-description">December closed the year with Microsoft's "Phi-4 Technical Report" by Abdin and colleagues. This research documented the training of their 14-billion-parameter model primarily on synthetic data generated by GPT-4o, achieving impressive performance relative to its size. The paper provided valuable insights into the benefits and limitations of synthetic data for model training, offering a promising direction for LLM development beyond simply scaling parameters or dataset size.</p>

                    <details class="year-review-details">
                        <summary class="year-review-summary">Key Details</summary>
                        <div class="details-content">
                            <ul class="key-details-list">
                                <li><strong>Main Paper/Topic Focus:</strong> <a href="https://arxiv.org/abs/2412.08905" target="_blank">Phi-4 Technical Report</a> detailing Microsoft's new open-weight LLM trained primarily on synthetic data.</li>
                                <li><strong>Publication Date:</strong> December 2024</li>
                                <li><strong>Authors:</strong> Abdin and colleagues at Microsoft</li>
                                <li><strong>Tags/Topics:</strong> Large Language Models (LLMs), Synthetic Data, Training Methodology, Model Performance, Knowledge Distillation, Open-weight Models.</li>
                                <li><strong>Citation Information:</strong> Paper available on arXiv. <br> DOI: <a href="https://arxiv.org/abs/2412.08905" target="_blank">arXiv:2412.08905</a></li>
                                <li><strong>Link to Paper/Resource:</strong> <a href="https://arxiv.org/abs/2412.08905" target="_blank">Read the paper on arXiv</a></li>
                                <li><strong>Short Impact Summary:</strong> The Phi-4 technical report demonstrated that a relatively small 14B parameter model trained largely on synthetic data could outperform similar-sized models trained on traditional web-crawled datasets. The research revealed that while synthetic data significantly improved reasoning and instruction-following capabilities, balancing it with web data remained crucial for knowledge-intensive tasks. These findings suggested a promising new direction for efficient model development that could partially decouple performance improvements from the continuous scaling of model parameters or dataset size.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="year-review-details year-review-deeper-dive-details">
                        <summary class="year-review-summary">Deeper Dive</summary>
                         <div class="details-content">
                            <h4>Technical Context/Explanation</h4>
                            <p>Phi-4 was a 14-billion parameter model trained on a carefully constructed mixture of synthetic and web-crawled data. The synthetic portion (approximately 40% of the training mix) was generated by GPT-4o, a significantly larger and more capable model. The researchers explored various configurations of synthetic-to-web data ratios and training methodologies to understand their impact on performance. They discovered that while synthetic data dramatically improved reasoning and instruction-following capabilities, models trained exclusively on synthetic data performed poorly on knowledge-based benchmarks. This suggested that synthetic data might lack comprehensive factual knowledge or potentially contain a higher proportion of hallucinations. Interestingly, the team found that increasing training epochs on a fixed synthetic dataset yielded better performance improvements than simply adding more web data. This indicated that synthetic data, when properly balanced with web data, could be learned from more efficiently through multiple passes—a finding that challenged conventional wisdom about avoiding overfitting.</p>
                            <h4>Figures or Diagrams</h4>
                            <p>Phi-4 training dataset composition:</p>
                            <pre><code class="language-text">

## Phi-4 Dataset Composition

## Data Source Percentage Characteristics

Synthetic Data ~40% • Generated by GPT-4o
• Strong on reasoning tasks
• Weaker on knowledge tasks

Web-crawled Data ~60% • Traditional web content
• Strong factual knowledge
• More diverse information
</code></pre>

<p>Key findings from ablation studies showed that models trained with 100% synthetic data performed 5-10% worse on knowledge-intensive benchmarks compared to mixed-data models, while multiple training epochs on synthetic data consistently improved performance across all benchmarks. The research demonstrated that Phi-4, despite being only 14B parameters, outperformed several larger models (20-30B) on multiple benchmarks, though it underperformed on the newer SimpleQA benchmark, possibly due to distributional differences from its training data.</p>
<h4>Historical Context/Comparison</h4>
<p>The Phi series of models had been exploring efficient training methodologies since its inception, with each iteration refining the approach. While Phi-3 (released earlier in 2024) had already demonstrated impressive performance for its size, it relied primarily on traditional web-crawled data. Phi-4 represented a significant shift by embracing synthetic data as a core component of its training recipe. This approach drew conceptual parallels to knowledge distillation but applied at the training data level rather than directly at the model level. The research came at a time when the field was beginning to observe diminishing returns from simply scaling model parameters or dataset sizes, prompting exploration of alternative improvement pathways. Microsoft's work built upon early experiments with synthetic data from other research groups but provided the most comprehensive analysis to date in a production-grade model. The performance of Phi-4 challenged assumptions about the necessity of extremely large models or datasets for competitive performance.</p>
<h4>Relevance at End of Year</h4>
<p>By the end of 2024, the Phi-4 findings suggested a promising direction for LLM development that partially decoupled advances from the resource-intensive scaling of parameters or dataset size. The research implied that future models might benefit from a more nuanced approach to training data curation, using larger, more capable models to generate high-quality synthetic data that could then bootstrap smaller, more efficient models. This approach could democratize access to high-performing LLMs by reducing the computational resources required for training. Looking ahead to 2025, the findings suggested that a hybrid approach—combining web-crawled data for factual knowledge with synthetic data for reasoning and instruction-following—might become standard practice. The research also hinted at a potential future where training methodologies could become more iterative, with successive generations of models helping to create increasingly refined training data for their successors. This "virtuous cycle" approach offered a path for continued improvement even as traditional scaling laws plateaued. For practical applications, Phi-4's strong performance at a modest parameter count made it particularly attractive for deployment scenarios with constrained resources, potentially accelerating the adoption of LLMs in edge and mobile computing.</p>
</div>
</details>
</div>
</div>

</div>

    </div>
