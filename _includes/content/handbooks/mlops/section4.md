---
layout: default
title: "Model Deployment"
description: "Techniques for deploying ML models into production."
---

<link rel="stylesheet" href="{{ '/assets/css/section-academic.css' | relative_url }}">

Chapter 15: Deployment Strategies
Batch inference, real-time inference, hybrid approaches
REST APIs, gRPC
Deployment patterns: Blue-Green, Canary
Chapter 16: Model Serving
Tools: TensorFlow Serving, TorchServe, ONNX Runtime
Serverless inference with AWS Lambda
Chapter 17: Containerization for ML
Docker, Kubernetes
Building lightweight containers with Buildpacks
Chapter 18: Cloud-Based MLOps
AWS SageMaker, Google Vertex AI, Azure ML
Managed services vs. custom setups
Cloud spend optimization for ML workloads (New subtopic)
Chapter 19: Multi-Cloud and Hybrid Cloud Strategies (New)
Cross-cloud ML pipelines
Vendor-agnostic MLOps frameworks
[Tools: Kubeflow, Flyte; Multi-cloud orchestration, portability]

<script>
  // Navigation variables - no previous for index
  window.prevSection = "/content/handbooks/foundation-models/section3/";
  window.nextSection = "/content/handbooks/foundation-models/section5/";
</script>

<script src="{{ '/assets/js/section-academic.js' | relative_url }}"></script>
