---
layout: default
title: "Word Embeddings and Representations"
description: "Techniques for representing words and contexts in vector spaces."
---

<link rel="stylesheet" href="{{ '/assets/css/section-academic.css' | relative_url }}">

Chapter 14: Static and Contextual Embeddings
Static: Word2Vec (CBOW, Skip-gram), GloVe, FastText
Contextual: ELMo, ULMFiT
[Negative sampling, hierarchical softmax, transfer learning]
References
Chapter 15: Embedding Evaluation
Intrinsic: Word similarity, analogy tasks
Extrinsic: Downstream NLP performance
[Cosine similarity, Spearman correlation, task-specific benchmarks]
References

<script>
  // Navigation variables
  var prevSection = "/content/handbooks/generative-ai/index.md";
  var nextSection = "/content/handbooks/generative-ai/section2.md";
</script>

<script src="{{ '/assets/js/section-academic.js' | relative_url }}"></script>
