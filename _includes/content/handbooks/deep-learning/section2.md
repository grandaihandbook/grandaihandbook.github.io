Chapter 3: Building Blocks: Neurons and Layers
The Artificial Neuron Model (Perceptron Revisited).

Activation Functions: Purpose, Properties, Common Choices (Sigmoid, Tanh, ReLU, Leaky ReLU, ELU, Swish, GELU, Softmax).

Neural Network Architecture: Layers (Input, Hidden, Output), Depth vs. Width, Fully Connected (Dense) Layers.

Forward Propagation: Calculating Network Output, Matrix Operations perspective.

Chapter 4: Training Neural Networks: The Core Loop
Loss Functions: Measuring Prediction Error (MSE for Regression, Cross-Entropy for Classification - Binary/Categorical, Hinge Loss, etc.).

Gradient Descent: The Optimization Workhorse (Concept, Learning Rate).

Backpropagation: Algorithm Explained (Chain Rule for efficient gradient computation).

Optimization Algorithms: Improving Gradient Descent (SGD with Momentum, Nesterov Momentum, AdaGrad, RMSprop, Adam, AdamW, Lookahead).

Weight Initialization Strategies: Importance, Common Methods (Random, Xavier/Glorot, He).
