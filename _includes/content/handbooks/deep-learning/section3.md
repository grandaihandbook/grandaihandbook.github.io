Chapter 5: Regularization for Deep Learning
Overfitting in Deep Networks.

L1 and L2 Weight Regularization (Weight Decay).

Dropout: Technique and Intuition (Ensemble interpretation).

Early Stopping: Monitoring Validation Performance.

Data Augmentation as Implicit Regularization (Overview, details later).

Chapter 6: Normalization Techniques
Motivation: Stabilizing Training, Faster Convergence.

Batch Normalization: How it Works (Normalizing layer inputs per mini-batch), Benefits and Considerations (Train vs. Inference).

Layer Normalization, Instance Normalization, Group Normalization: Alternatives to Batch Norm and Use Cases.

Chapter 7: Hyperparameter Tuning and Best Practices
Key Hyperparameters in DL (Learning Rate, Batch Size, Network Architecture, Optimizer Choice, Regularization Strength).

Tuning Strategies (Manual, Grid Search, Random Search, Bayesian Optimization revisited for DL).

Learning Rate Schedules (Step Decay, Exponential Decay, Cosine Annealing, Warmup).

Gradient Checking and Debugging Techniques.
