Chapter 14: The Attention Mechanism
Limitations of Basic Encoder-Decoder RNNs for Long Sequences.

Attention Intuition: Allowing Decoder to Focus on Relevant Input Parts.

Different Attention Types (Bahdanau vs. Luong Attention in Seq2Seq).

Self-Attention: Attention within a Single Sequence (Query, Key, Value Formulation).

Chapter 15: The Transformer Architecture
Motivation: Overcoming RNN sequential computation limits, Parallelization.

Scaled Dot-Product Attention.

Multi-Head Self-Attention: Capturing Different Relationship Subspaces.

Positional Encodings: Injecting Sequence Order Information.

Encoder Block: Multi-Head Attention, Add & Norm, Feed-Forward Network.

Decoder Block: Masked Multi-Head Attention, Encoder-Decoder Attention, Add & Norm, Feed-Forward.

Putting it Together: The Full Encoder-Decoder Architecture.

Chapter 16: Transformers in Practice: BERT, GPT, and Beyond
BERT (Bidirectional Encoder Representations from Transformers): Masked Language Model (MLM) Pre-training, Next Sentence Prediction (NSP), Fine-tuning for Downstream Tasks.

GPT (Generative Pre-trained Transformer): Autoregressive Language Modeling, Decoder-only Architecture, Zero-shot and Few-shot Learning via Prompting.

Other Variants and Developments (T5, BART, Vision Transformer - ViT concept).

Applications Revisited: Advanced NLP Tasks, Code Generation, etc.
